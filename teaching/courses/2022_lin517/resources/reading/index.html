<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.113">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-08-22">

<title>Lin517: Natural Language Processing - Reading a Technical Paper</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: 1;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../resources/notation/index.html" rel="next">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible">


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Reading a Technical Paper</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Lin517: Natural Language Processing</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Resources</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../resources/reading/index.html" class="sidebar-item-text sidebar-link active">Reading a technical paper</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../resources/notation/index.html" class="sidebar-item-text sidebar-link">Mathematical notation</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">Lecture Notes</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/what_is_nlp/index.html" class="sidebar-item-text sidebar-link">What is NLP?</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/data_sparsity/data_sparsity.html" class="sidebar-item-text sidebar-link">Data Sparsity</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/data_processing/index.html" class="sidebar-item-text sidebar-link">Data processing</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/data_processing/addendum.html" class="sidebar-item-text sidebar-link">Data processing – addendum</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/lem_stem/index.html" class="sidebar-item-text sidebar-link">Lemmatizing and stemming</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/ngram/index.html" class="sidebar-item-text sidebar-link">ngram Language Models</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lectures/evaluation/index.html" class="sidebar-item-text sidebar-link">Evaluating models</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">python sessions</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../python_sessions/00_session1.html" class="sidebar-item-text sidebar-link">Starting Python</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../python_sessions/01_session2.html" class="sidebar-item-text sidebar-link">Lists and Dictionaries</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../python_sessions/02_session3.html" class="sidebar-item-text sidebar-link">Loops Etc.</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../python_sessions/03_session4.html" class="sidebar-item-text sidebar-link">Comprehensions and Useful Things</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../python_sessions/04_session5.html" class="sidebar-item-text sidebar-link">Making and Counting Bigrams</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#reading-papers-is-always-hard" id="toc-reading-papers-is-always-hard" class="nav-link active" data-scroll-target="#reading-papers-is-always-hard">Reading papers is always hard</a></li>
  <li><a href="#a-tricky-paragraph" id="toc-a-tricky-paragraph" class="nav-link" data-scroll-target="#a-tricky-paragraph">A Tricky Paragraph</a></li>
  <li><a href="#can-we-figure-out-the-upshot" id="toc-can-we-figure-out-the-upshot" class="nav-link" data-scroll-target="#can-we-figure-out-the-upshot">Can we figure out the upshot?</a>
  <ul class="collapse">
  <li><a href="#the-upshot" id="toc-the-upshot" class="nav-link" data-scroll-target="#the-upshot">The upshot</a></li>
  </ul></li>
  <li><a href="#can-we-figure-out-themes-from-repetition" id="toc-can-we-figure-out-themes-from-repetition" class="nav-link" data-scroll-target="#can-we-figure-out-themes-from-repetition">Can we figure out themes from repetition?</a></li>
  <li><a href="#any-search-terms-for-background-research" id="toc-any-search-terms-for-background-research" class="nav-link" data-scroll-target="#any-search-terms-for-background-research">Any search terms for background research?</a>
  <ul class="collapse">
  <li><a href="#models" id="toc-models" class="nav-link" data-scroll-target="#models"><span style="color: #F9C80E">“Models”</span></a></li>
  <li><a href="#nlp-tasks" id="toc-nlp-tasks" class="nav-link" data-scroll-target="#nlp-tasks"><span style="color: #F86624">“NLP Tasks”</span></a></li>
  <li><a href="#scores" id="toc-scores" class="nav-link" data-scroll-target="#scores"><span style="color: #662E9B">Scores</span></a></li>
  </ul></li>
  <li><a href="#wrapping-up" id="toc-wrapping-up" class="nav-link" data-scroll-target="#wrapping-up">Wrapping up</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Reading a Technical Paper</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Josef Fruehwald </p>
             <p> </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 22, 2022</p>
    </div>
  </div>
    
  </div>
  

</header>

<section id="reading-papers-is-always-hard" class="level2">
<h2 class="anchored" data-anchor-id="reading-papers-is-always-hard">Reading papers is always hard</h2>
<p>There may never be a point in your academic life where you will read a paper and understand everything in it (or if there is, I haven’t gotten there). Instead, you have to develop methods for getting whatever information you <em>can</em> out of a paper itself, and then draw up a list of terms and concepts to do further background research on.</p>
<p>Let’s work through a sample paragraph from <span class="citation" data-cites="bender2021">Bender et al. (<a href="#ref-bender2021" role="doc-biblioref">2021</a>)</span> to see some of these strategies in action.</p>
</section>
<section id="a-tricky-paragraph" class="level2">
<h2 class="anchored" data-anchor-id="a-tricky-paragraph">A Tricky Paragraph</h2>
<p><span class="citation" data-cites="bender2021">Bender et al. (<a href="#ref-bender2021" role="doc-biblioref">2021</a>)</span> is an important paper about ethics and safety concerns in natural language processing. However, it could be hard to follow some of the discussion without a background in the NLP literature. Here’s a sample paragraph where they discuss specific advancements made in NLP models.</p>
<blockquote class="blockquote">
<p>The next big step was the move towards using pretrained representations of the distribution of words (called <em>word embeddings</em>) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g.&nbsp;82].</p>
</blockquote>
<p>My own approach to trying to understand paragraphs (and whole papers) like this, where I might not be familiar with all of the concepts involved, is ask the following questions:</p>
<ul>
<li><p>Can I figure out the upshot? What message is this paragraph trying to communicate?</p></li>
<li><p>Can I pick out any themes? What keeps getting repeated?</p></li>
<li><p>What search terms can I pull out of the paper to do more background research.</p></li>
</ul>
</section>
<section id="can-we-figure-out-the-upshot" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="can-we-figure-out-the-upshot">Can we figure out the upshot?</h2>
<p>This paragraph is trying to tell us <em>something</em>. I’ve color coded the pieces of the paragraph which I think are most useful for figuring out the point of this paragraph even if you don’t know what all the specifics mean.</p>
<blockquote class="blockquote page-columns page-full">
<div class="page-columns page-full"><p><span style="color: #F86624"><strong>The next big step</strong></span><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> was the move towards using pretrained representations of the distribution of words (called <em>word embeddings</em>) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and <span style="color: #F86624"><strong>supported state of the art</strong></span><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> performance on question answering, textual entailment, semaantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, <span style="color: #F86624"><strong>at first in English and later for other languages as well</strong></span><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. While training the word embeddings required a (relatively) large amount of data, <span style="color: #662E9B"><strong>it reduced the amount of labeled data necessary</strong></span><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo <span style="color: #662E9B"><strong>reduced the necessary amount of training data needed</strong></span><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo <span style="color: #F86624"><strong>reached the maximum development F1 score</strong></span><a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> <span style="color: #662E9B"><strong>in 10 epochs as opposed to 486</strong></span><a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> without ELMo. This model furthermore <span style="color: #662E9B"><strong>achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data</strong></span><a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g.&nbsp;82].</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;<span style="color: #F86624">“Big steps”, improvement</span></p></li><li id="fn2"><p><sup>2</sup>&nbsp;<span style="color: #F86624">“state of the art”, (SOTA) means the best it can be</span></p></li><li id="fn3"><p><sup>3</sup>&nbsp;<span style="color: #F86624">“at first … and later” from limited applications, it expanded</span></p></li><li id="fn4"><p><sup>4</sup>&nbsp;<span style="color:#662E9B">less “labeled data” needed.</span></p></li><li id="fn5"><p><sup>5</sup>&nbsp;<span style="color:#662E9B">Less “training data” needed.</span></p></li><li id="fn6"><p><sup>6</sup>&nbsp;<span style="color: #F86624">Whatever the “F1 score” is, this got the best one</span></p></li><li id="fn7"><p><sup>7</sup>&nbsp;<span style="color:#662E9B">Whatever an “epoch” is, this needed less of them</span></p></li><li id="fn8"><p><sup>8</sup>&nbsp;<span style="color: #662E9B">The same score, but less data.</span></p></li></div></div>
</blockquote>
<section id="the-upshot" class="level3">
<h3 class="anchored" data-anchor-id="the-upshot">The upshot</h3>
<p><span style="color: #F86624"><strong>~Things~ got better</strong></span> <span style="color: #662E9B"><strong>with less</strong></span>.</p>
</section>
</section>
<section id="can-we-figure-out-themes-from-repetition" class="level2">
<h2 class="anchored" data-anchor-id="can-we-figure-out-themes-from-repetition">Can we figure out themes from repetition?</h2>
<p>I’ve highlighted the words whose repetition struck me as being important to the theme of this paragraph.</p>
<blockquote class="blockquote">
<p>The next big step was the move towards using <span style="color: #EA3546"><strong>pretrained</strong></span> representations of the distribution of words (called <em>word embeddings</em>) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While <span style="color: #EA3546"><strong>training</strong></span> the word embeddings required a (relatively) large amount of <span style="color: #43BCCD"><strong>data</strong></span>, it reduced the amount of <span style="color: #43BCCD"><strong>labeled data</strong></span> necessary for <span style="color: #EA3546"><strong>training</strong></span> on the various supervised tasks. For example, [99] showed that a model <span style="color: #EA3546"><strong>trained</strong></span> with ELMo reduced the necessary amount of <span style="color: #EA3546"><strong>training</strong></span> <span style="color: #43BCCD"><strong>data</strong></span> needed to achieve similar results on SRL compared to models without, as shown in one instance where a model <span style="color: #EA3546"><strong>trained</strong></span> with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the <span style="color: #43BCCD"><strong>data</strong></span> as the baseline model achieved with 10% of the <span style="color: #EA3546"><strong>training</strong></span> <span style="color: #43BCCD"><strong>data</strong></span>. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g.&nbsp;82].</p>
</blockquote>
<p>It seems clear “<span style="color: #EA3546">training</span>” is a big deal, and it involves <span style="color: #43BCCD">data</span>.</p>
</section>
<section id="any-search-terms-for-background-research" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="any-search-terms-for-background-research">Any search terms for background research?</h2>
<p>There are a lot of technical terms in this paragraph, but it seems like they can be classified under a few main categories.</p>
<blockquote class="blockquote">
<p>The next big step was the move towards using pretrained representations of the distribution of words (called <span style="color: #F9C80E"><strong><em>word embeddings</em></strong></span>) in other (supervised) <span style="color: #F86624"><strong>NLP tasks</strong></span>. These <span style="color: #F9C80E"><strong>word vectors</strong></span> came from systems such as <span style="color: #F9C80E"><strong>word2vec</strong></span> [85] and <span style="color: #F9C80E"><strong>GloVe</strong></span> [98] and later <span style="color: #F9C80E"><strong>LSTM models</strong></span> such as <span style="color: #F9C80E"><strong>context2vec</strong></span> [82] and <span style="color: #F9C80E"><strong>ELMo</strong></span> [99] and supported state of the art performance on <span style="color: #F86624"><strong>question answering</strong></span>, <span style="color: #F86624"><strong>textual entailment</strong></span>, <span style="color: #F86624"><strong>semantic role labeling (SRL)</strong></span>, <span style="color: #F86624"><strong>coreference resolution</strong></span>, <span style="color: #F86624"><strong>named entity recognition (NER)</strong></span>, and <span style="color: #F86624"><strong>sentiment analysis</strong></span>, at first in English and later for other languages as well. While training the <span style="color: #F9C80E"><strong>word embeddings</strong></span>required a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with <span style="color: #F9C80E"><strong>ELMo</strong></span> reduced the necessary amount of training data needed to achieve similar results on <span style="color: #F86624"><strong>SRL</strong></span> compared to models without, as shown in one instance where a model trained with <span style="color: #F9C80E"><strong>ELMo</strong></span> reached the maximum development <span style="color: #662E9B"><strong>F1 score</strong></span> in 10 epochs as opposed to 486 without <span style="color: #F9C80E"><strong>ELMo</strong></span>. This model furthermore achieved the same <span style="color: #662E9B"><strong>F1 score</strong></span> with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for <span style="color: #F9C80E"><strong>LSTMs</strong></span> [e.g.&nbsp;82].</p>
</blockquote>
<p>There’s at least three kinds of things we could try doing some background research on here:</p>
<section id="models" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="models"><span style="color: #F9C80E">“Models”</span></h3>
<p>There are a few models named here. Some seem like generic names, and others seem more specific. The most generic “word embeddings” is actually defined in the paragraph</p>
<blockquote class="blockquote">
<p>representations of the distribution of words</p>
</blockquote>
<p>There’s probably a lot more to investigate here.</p>
<p>After defining “word embeddings,” they say “these word vectors”, which seems to suggest that these are synonymous or at least highly interchangeable concepts.</p>
<p>Specific “systems” that generate “word vectors” are</p>
<ul>
<li><p>word2vec</p></li>
<li><p>GloVe</p></li>
</ul>
<p>Some good search terms to find information about these would probably be “word2vec word vectors” or “GloVe word vectors”.</p>
<div class="page-columns page-full"><p>Next they name another class of model, “LSTM models,” that generate “word vectors”, and name some specific LSTM models</p><div class="no-row-height column-margin column-container"><span class="aside">You can also get an idea of naming conventions in this field. The name formats seem to either be <em>thing2thing</em> or an acronym that is a pronounceable word… and hard to search for all on its own.</span></div></div>
<ul>
<li><p>context2vec</p></li>
<li><p>ELMo</p></li>
</ul>
<p>Some good search terms here would probably be “LSTM context2vec” or “LSTM ELMo”.</p>
</section>
<section id="nlp-tasks" class="level3">
<h3 class="anchored" data-anchor-id="nlp-tasks"><span style="color: #F86624">“NLP Tasks”</span></h3>
<p>After saying that these models are used on “NLP tasks”, they name a few specific ones. Some of them just have names, while others also have an acronym associated with them.</p>
<ul>
<li><p>question answering</p></li>
<li><p>textual entailment</p></li>
<li><p>semantic role labeling (SRL)</p></li>
<li><p>coreference resolution</p></li>
<li><p>named entity recognition (NER)</p></li>
<li><p>sentiment analysis</p></li>
</ul>
<p>Some of these tasks wouldn’t make for great search terms on their own, like “question answering,” but appending “NLP” to the beginning for “NLP question answering” would probably work.</p>
</section>
<section id="scores" class="level3">
<h3 class="anchored" data-anchor-id="scores"><span style="color: #662E9B">Scores</span></h3>
<p>In this paragraph, only one kind of score, the “F1 score” is mentioned. But in the context of the rest of the paper, there are a number of other “Scores” that could be important to investigate.</p>
</section>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping up</h2>
<p>Normally, the process isn’t as elaborate as it appears in this demo. I don’t usually color code all of the words in a paragraph, much less a whole paper, like this. But I <em>do</em> often try to mentally summarize paragraphs and sections with what the upshot is. Some authors are better than others in getting across their point in among the technical aspects, but ideally they are always trying to communicate some message that you can at least approximate even if you don’t understand everything in detail.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-bender2021" class="csl-entry" role="doc-biblioentry">
Bender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. <span>“FAccT ’21: 2021 ACM Conference on Fairness, Accountability, and Transparency.”</span> In, 610–23. Virtual Event Canada: ACM. <a href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a>.
</div>
</div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><div>CC BY 4.0</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../resources/notation/index.html" class="pagination-link">
        <span class="nav-page-text">Mathematical notation</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>
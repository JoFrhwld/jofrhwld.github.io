[
  {
    "objectID": "resources/reading/index.html",
    "href": "resources/reading/index.html",
    "title": "Reading a Technical Paper",
    "section": "",
    "text": "There may never be a point in your academic life where you will read a paper and understand everything in it (or if there is, I haven’t gotten there). Instead, you have to develop methods for getting whatever information you can out of a paper itself, and then draw up a list of terms and concepts to do further background research on.\nLet’s work through a sample paragraph from Bender et al. (2021) to see some of these strategies in action."
  },
  {
    "objectID": "resources/reading/index.html#a-tricky-paragraph",
    "href": "resources/reading/index.html#a-tricky-paragraph",
    "title": "Reading a Technical Paper",
    "section": "A Tricky Paragraph",
    "text": "A Tricky Paragraph\nBender et al. (2021) is an important paper about ethics and safety concerns in natural language processing. However, it could be hard to follow some of the discussion without a background in the NLP literature. Here’s a sample paragraph where they discuss specific advancements made in NLP models.\n\nThe next big step was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g. 82].\n\nMy own approach to trying to understand paragraphs (and whole papers) like this, where I might not be familiar with all of the concepts involved, is ask the following questions:\n\nCan I figure out the upshot? What message is this paragraph trying to communicate?\nCan I pick out any themes? What keeps getting repeated?\nWhat search terms can I pull out of the paper to do more background research."
  },
  {
    "objectID": "resources/reading/index.html#can-we-figure-out-the-upshot",
    "href": "resources/reading/index.html#can-we-figure-out-the-upshot",
    "title": "Reading a Technical Paper",
    "section": "Can we figure out the upshot?",
    "text": "Can we figure out the upshot?\nThis paragraph is trying to tell us something. I’ve color coded the pieces of the paragraph which I think are most useful for figuring out the point of this paragraph even if you don’t know what all the specifics mean.\n\nThe next big step1 was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art2 performance on question answering, textual entailment, semaantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well3. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary4 for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed5 to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score6 in 10 epochs as opposed to 4867 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data8. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g. 82].1 “Big steps”, improvement2 “state of the art”, (SOTA) means the best it can be3 “at first … and later” from limited applications, it expanded4 less “labeled data” needed.5 Less “training data” needed.6 Whatever the “F1 score” is, this got the best one7 Whatever an “epoch” is, this needed less of them8 The same score, but less data.\n\n\nThe upshot\n~Things~ got better with less."
  },
  {
    "objectID": "resources/reading/index.html#can-we-figure-out-themes-from-repetition",
    "href": "resources/reading/index.html#can-we-figure-out-themes-from-repetition",
    "title": "Reading a Technical Paper",
    "section": "Can we figure out themes from repetition?",
    "text": "Can we figure out themes from repetition?\nI’ve highlighted the words whose repetition struck me as being important to the theme of this paragraph.\n\nThe next big step was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g. 82].\n\nIt seems clear “training” is a big deal, and it involves data."
  },
  {
    "objectID": "resources/reading/index.html#any-search-terms-for-background-research",
    "href": "resources/reading/index.html#any-search-terms-for-background-research",
    "title": "Reading a Technical Paper",
    "section": "Any search terms for background research?",
    "text": "Any search terms for background research?\nThere are a lot of technical terms in this paragraph, but it seems like they can be classified under a few main categories.\n\nThe next big step was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While training the word embeddingsrequired a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g. 82].\n\nThere’s at least three kinds of things we could try doing some background research on here:\n\n“Models”\nThere are a few models named here. Some seem like generic names, and others seem more specific. The most generic “word embeddings” is actually defined in the paragraph\n\nrepresentations of the distribution of words\n\nThere’s probably a lot more to investigate here.\nAfter defining “word embeddings,” they say “these word vectors”, which seems to suggest that these are synonymous or at least highly interchangeable concepts.\nSpecific “systems” that generate “word vectors” are\n\nword2vec\nGloVe\n\nSome good search terms to find information about these would probably be “word2vec word vectors” or “GloVe word vectors”.\nNext they name another class of model, “LSTM models,” that generate “word vectors”, and name some specific LSTM modelsYou can also get an idea of naming conventions in this field. The name formats seem to either be thing2thing or an acronym that is a pronounceable word… and hard to search for all on its own.\n\ncontext2vec\nELMo\n\nSome good search terms here would probably be “LSTM context2vec” or “LSTM ELMo”.\n\n\n“NLP Tasks”\nAfter saying that these models are used on “NLP tasks”, they name a few specific ones. Some of them just have names, while others also have an acronym associated with them.\n\nquestion answering\ntextual entailment\nsemantic role labeling (SRL)\ncoreference resolution\nnamed entity recognition (NER)\nsentiment analysis\n\nSome of these tasks wouldn’t make for great search terms on their own, like “question answering,” but appending “NLP” to the beginning for “NLP question answering” would probably work.\n\n\nScores\nIn this paragraph, only one kind of score, the “F1 score” is mentioned. But in the context of the rest of the paper, there are a number of other “Scores” that could be important to investigate."
  },
  {
    "objectID": "resources/reading/index.html#wrapping-up",
    "href": "resources/reading/index.html#wrapping-up",
    "title": "Reading a Technical Paper",
    "section": "Wrapping up",
    "text": "Wrapping up\nNormally, the process isn’t as elaborate as it appears in this demo. I don’t usually color code all of the words in a paragraph, much less a whole paper, like this. But I do often try to mentally summarize paragraphs and sections with what the upshot is. Some authors are better than others in getting across their point in among the technical aspects, but ideally they are always trying to communicate some message that you can at least approximate even if you don’t understand everything in detail."
  },
  {
    "objectID": "resources/notation/index.html",
    "href": "resources/notation/index.html",
    "title": "Mathematical Notation",
    "section": "",
    "text": "The most common variables we’re going to be seeing:\n\n\\(x, y\\)\n\nStand-ins for numbers, usually a list, or vector, of numbers\n\\(y\\) is often some kind of “outcome” variable\n\\(x\\) is often some kind of input, or predictor variables\ne.g. “We want to predict how many ice cream cones, \\(y\\), we’ll sell if it’s a specific temperature, \\(x\\).\n\n\\(X, Y\\)\n\nStand ins for categorical variables\n\n\\(w\\)\n\nA special variable for a word\n\n\\(c\\)\n\nA “count” of something\n\n\\(p\\)\n\nA probability\n\n\\(N\\)\n\nUsually, the total number of something\n\n\\(n\\)\n\nA contextual number. E.g. “In \\(n+1\\) days (that is, tomorrow)…”\n\n\\(n,m\\)\n\nWhen \\(n\\) and \\(m\\) are used together, it’s often to describe the number of rows and columns of a matrix.\n\n\\(A, B\\)\n\nThese are almost always used for matrices\nCapital roman letters are often a clue we’re looking at a matrix (but not always)\n\n\\(\\lambda\\)\n\n“lambda”\nOften used for an arbitrary value you multiply things by.\n\n\\(k\\)\n\nOften used for an arbitrary value you add things to.\n\n\\(\\delta\\)\n\n“delta”\nOften used to describe a difference of some kind\n\n\\(\\alpha, \\beta, \\gamma\\)\n\n“alpha”, “beta”, “gamma”\nUsed for model parameters\n\n\\(\\theta\\)\n\n“theta”\nAlso used as a model parameter.\nOr, to describe an angle (in radians)\n\n\n\n\n\n\\(\\hat{y}\\)\n\n“y hat”\nA predicted value for \\(y\\)\ne.g. “I predicted \\(\\hat{y}\\) ice cream cones to be sold, but they actually sold \\(y\\).\n\n\\(\\bar{y}\\)\n\n“y bar”\nThe average value of \\(y\\)\n\n\\(y^*\\)\n\n“y star”\nA modified value of \\(y\\)"
  },
  {
    "objectID": "resources/notation/index.html#one-dimensional",
    "href": "resources/notation/index.html#one-dimensional",
    "title": "Mathematical Notation",
    "section": "One Dimensional",
    "text": "One Dimensional\nWhen we have a variable that contains a list of values, each individual value will be described with an “index”. For example, if we had a variable \\(X\\) that contained the names of the week.\n\\[\nX = (\\text{Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday})\n\\]\nThen the first value in the list would be \\(X_1\\), and we could say \\(X_1 = \\text{Monday}\\). You’d usually pronounce \\(X_1\\) as “X sub 1”.\nSometimes we want to be able to refer to a generic value in the list \\(X\\) and for that we’d use an index variable like \\(X_i\\) (pronounced “X sub i”). The most common letters used to indicate generic indices are \\(i, j, k\\).\nWe can do math on the indices also. So, if the name of today is \\(X_i\\), then the name of tomorrow is \\(X_{i+1}\\). The name of yesterday is \\(X_{i-1}\\). The day after tomorrow would be \\(X_{i+2}\\).\nWe can also include a range of numbers in the indices. So, the names of the first three days of the week are \\(X_{1:3} = (\\text{Monday, Tuesday, Wednesday})\\). The names of yesterday, today, and tomorrow are \\(X_{i-1:i+1}\\)."
  },
  {
    "objectID": "resources/notation/index.html#two-dimensional",
    "href": "resources/notation/index.html#two-dimensional",
    "title": "Mathematical Notation",
    "section": "Two Dimensional",
    "text": "Two Dimensional\nWe could imagine a Month as being a matrix \\(M\\) made up of 4 weeks, with each week being 7 days.\n\\[\nM = \\left[ \\begin{array}{lllllll}\n\\text{Monday} & \\text{Tuesday} & \\text{Wednesday} & \\text{Thursday} &  \\text{Friday} &     \\text{Saturday} & \\text{Sunday} \\\\\n\\text{Monday} & & & \\dots & & & \\text{Sunday}\\\\\n\\vdots & & & \\ddots\\\\\n\\text{Monday} & & & \\dots & & &\\text{Sunday}\\\\\n\\end{array} \\right]\n\\]\nIf we wanted to get the name of the third day during the first week, it would be \\(M_{1, 3} = \\text{Wednesday}\\). When giving numeric indices of a matrix, the rows (the parts going across) come first, and the columns (the parts going up and down) come second. So, we’d describe the matrix \\(M\\) as being a \\(4\\times7\\) “four by seven” matrix.\nTo refer to a day of the month generically, we’d use \\(M_{i,j}\\) (pronounced “M sub i, j”). We can also use ranges in these indices. So, to refer to the second week of the month, we’d use \\(M_{2,1:7}\\). Or, to refer to all of the Saturdays in the month, we’d use \\(M_{1:4,6}\\).\nAnd, we can also use math in these indices. So, we could refer to “a week from today” with \\(M_{i+1,j}\\)."
  },
  {
    "objectID": "resources/notation/index.html#matrix-transposition",
    "href": "resources/notation/index.html#matrix-transposition",
    "title": "Mathematical Notation",
    "section": "Matrix Transposition",
    "text": "Matrix Transposition\nThe one special notation related to matrices is “transposition”, which basically takes a matrix and flips it.\n\\[\nA = \\left[\\begin{array}{ccc}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{array}\n\\right]\n\\]\n\\[\nA^\\intercal = \\left[\\begin{array}{cc}1 & 4\\\\\n2 & 5 \\\\\n3 & 6 \\end{array}\\right]\n\\]\nThe little \\(^\\intercal\\) indicates that we’re transposing the matrix. You’d pronounce \\(A^\\intercal\\) as “A transpose”.\nThe transpose operation can apply to single lists too. This will eventually be important.\n\\[\nx = [1, 2, 3, 4]\n\\]\n\\[\nx^\\intercal = \\left[\\begin{array}{c} 1\\\\2\\\\3\\\\4 \\end{array}\\right]\n\\]\nAgain, \\(x^\\intercal\\) is pronounced “x transpose.”"
  },
  {
    "objectID": "resources/notation/index.html#generic",
    "href": "resources/notation/index.html#generic",
    "title": "Mathematical Notation",
    "section": "Generic",
    "text": "Generic\n\n\\(f(), g()\\)\n\nThese are the most common “functions” you’ll see. Usually in a whole formula like \\(y = f(x)\\) (pronounced “y equals f of x”).\nIt means “we stick the value \\(x\\) in and \\(y\\) comes out.\nWhat \\(f()\\) or \\(g()\\) (or whatever name we give the function) is needs to be defined. They don’t have a fixed meaning."
  },
  {
    "objectID": "resources/notation/index.html#specialized",
    "href": "resources/notation/index.html#specialized",
    "title": "Mathematical Notation",
    "section": "Specialized",
    "text": "Specialized\n\n\\(P()\\)\n\nThe function \\(P()\\) refers to the probability of whatever we put in.\n\\(P(X_i)\\) returns the probability of a specific \\(X_i\\) value.\nWe’d refer to the specific value the \\(P()\\) returns with the variable \\(p\\), usually with the same index. So \\(p_i = P(X_i)\\).\n\n\\(C()\\)\n\nThe function \\(C()\\) returns the count of whatever we put in.\nIn the matrix above, \\(C(\\text{Monday}) = 4\\)\nWe’d refer to the specific count value of something with the variable \\(c\\), usually with the same index. So \\(c_{i} = C(X_i)\\)."
  },
  {
    "objectID": "resources/notation/index.html#summation",
    "href": "resources/notation/index.html#summation",
    "title": "Mathematical Notation",
    "section": "Summation",
    "text": "Summation\n\\(\\sum\\)\nThis operator indicates that we are adding together numbers in a list. Let’s look at the table of the height of actors, in cm who played Spider-Man in Spider-Man: No Way Home.\n\n\n\n\nActor\nHeight (cm)\n\n\n\n\nTobey Maguire\n172\n\n\nAndrew Garfield\n179\n\n\nTom Holland\n169\n\n\n\n\nWe would say that there are \\(N=3\\) actors who played Spider-Man in the movie. And we could represent their heights in a variable \\(y\\), and say\n\\[\ny = (172, 179, 169)\n\\]\nThe height of the first actor would be \\(y_1\\), which equals \\(172\\), and the way to refer to any given height on the list would be \\(y_i\\).\nTo get the total height of the actors (like if one stood on the head of the other), we would have to sum it up, which we could represent like this:\n\\[\nh = y_1 + y_2 + y_3\n\\]\nOr, we could use summation notation\n\\[\nh = \\sum_{i=1}^Ny_i\n\\]\nThe way to read this out loud is “h equals the sum of y sub i from i equals 1 to N”. The \\(i=1\\) part underneath the \\(\\sum\\) means “start getting values out of \\(y\\) starting with 1”. The \\(N\\) at the top means “keep adding 1 to \\(i\\) until \\(i = N\\).”\nWhat the whole notation is going to do is pull out every value of \\(y\\) and add them together.\nIf you know how to do some coding, sometimes it’s easier to understand the mathematical notation to see it in code.\n\n```{python}\ny = [172, 179, 169]\nN = 3 # = len(y)\nh = 0 \n\nfor i in range(N):\n  h = h + y[i]\n\nprint(h)\n```\n\n520"
  },
  {
    "objectID": "resources/notation/index.html#product",
    "href": "resources/notation/index.html#product",
    "title": "Mathematical Notation",
    "section": "Product",
    "text": "Product\n\\(\\prod\\)\nThe product operator, \\(\\prod\\), works a lot like the the summation operator, except instead of adding numbers together, it multiplies them. For example, let’s say we’re keeping track of the day-to-day changes in the number of visitors to a website.\n\n\n\nDay\nPercent Change\nMultiplier\n\n\n\n\n1\nup 1%\n1.01\n\n\n2\nno change\n1.00\n\n\n3\ndown 5%\n0.95\n\n\n\nWe can get the total proportional change over these three days by multiplying the proportions together. Writing it out the long way, it’s\n\\[\nN = 3\n\\]\n\\[\ny = (1.01, 1, 0.95)\\\\\n\\]\n\\[\nt = y_1\\cdot y_2\\cdot y_3\n\\]\nIn product notation, though, it looks like this.\n\\[\nt = \\prod_{i=1}^Ny_i\n\\]\nAgain, if you’re more comfortable with programming code, it’s equivalent to this.\n\n```{python}\ny = [1.01, 1, 0.95]\nN = 3\nt = 1\n\nfor i in range(N):\n  t = t * y[i]\n\nprint(f\"{t:.4}\")\n```\n\n0.9595"
  },
  {
    "objectID": "resources/notation/index.html#conditional-probability",
    "href": "resources/notation/index.html#conditional-probability",
    "title": "Mathematical Notation",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nThe “conditional probability” is the probability of some value or event \\(Y\\), holding constant some other value or event \\(X\\) . For example, maybe\n\\[\nY = \\text{I am teaching}\n\\]\nand\n\\[\nX_1 = \\text{It is a Monday}\n\\]\nand\n\\[\nX_6=\\text{It is a Saturday}\n\\]\nThe probability that I am teaching a class is is a lot lower on a Saturday than on a Monday. We can express these like so.\n\\[\np_1 = P(Y | X_1)\n\\]\na.k.a “p sub 1 equals the probability I am teaching, given that it is a Monday”.\n\\[\np_6 = P(Y|X_6)\n\\]\na.k.a. “p sub 6 equals the probability I am teaching given that it is a Saturday”\nand\n\\[\np_6 < p_1\n\\]\nThe key piece of notation in the expressions above is the \\(|\\) (the “pipe”) inside of the \\(P()\\) function."
  },
  {
    "objectID": "resources/notation/index.html#joint-probability",
    "href": "resources/notation/index.html#joint-probability",
    "title": "Mathematical Notation",
    "section": "Joint Probability",
    "text": "Joint Probability\nThe joint probability is the probability of two values or events happening together. It’s not the same as a conditional probability of one event given the other, but explaining why requires more time and space.\nIf we stick with the same events as above (“I am teaching” and “It is a Monday”) the joint probability of “I am teaching and it is a Monday” would be notated as\n\\[\nq_1 = P(Y,X_1)\n\\]\nThe comma inside the \\(P()\\) function means “and.”\n\n\npdf\n\n\n\n\nCC BY-SA 4.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lin517: Natural Language Processing",
    "section": "",
    "text": "Course materials for Fall 2022’s Natural Language Processing Course at University of Kentucky.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n2022-08-20\n\n\nMathematical Notation\n\n\n\n\n\n\n2022-08-22\n\n\nReading a Technical Paper\n\n\n\n\n\n\n2022-08-24\n\n\nWhat is NLP?(for this course)\n\n\nintro\n\n\n\n\n2022-08-31\n\n\nData Sparsity\n\n\ndata\n\n\n\n\n2022-09-02\n\n\nStarting Python\n\n\npython\n\n\n\n\n2022-09-06\n\n\nData Processing\n\n\ndata\n\n\n\n\n2022-09-09\n\n\nLists and Dictionaries\n\n\npython\n\n\n\n\n2022-09-13\n\n\nAddendum\n\n\ndata\n\n\n\n\n2022-09-13\n\n\nLemmatizing and Stemming\n\n\ndata\n\n\n\n\n2022-09-16\n\n\nLoops Etc.\n\n\npython\n\n\n\n\n2022-09-23\n\n\nComprehensions and Useful Things\n\n\npython\n\n\n\n\n2022-09-28\n\n\nngram Language Models\n\n\nngram\n\n\n\n\n2022-09-30\n\n\nMaking and Counting Bigrams\n\n\npython\n\n\n\n\n2022-10-03\n\n\nEvaluating models\n\n\nmodels\n\n\n\n\n2022-10-10\n\n\nngrams - Perplexity\n\n\nngram,models\n\n\n\n\n2022-10-11\n\n\nngram - Smoothing\n\n\nngram\n\n\n\n\n2022-10-14\n\n\nFunctions\n\n\n\n\n\n\n2022-10-26\n\n\nWord Vectors - Concepts\n\n\nword vectors\n\n\n\n\n2022-11-01\n\n\nTerm-Document and Term-Context matrices\n\n\nword vectors\n\n\n\n\n2022-11-07\n\n\nword2vec\n\n\nword vectors\n\n\n\n\n2022-11-14\n\n\nGradient Descent\n\n\nNeural Networks\n\n\n\n\n2022-11-16\n\n\nMatrix Multiplication\n\n\nNeural Networks\n\n\n\n\n2022-11-21\n\n\nNeural Nets\n\n\nNeural Networks\n\n\n\n\n\n\nWord Vectors\n\n\nword vectors\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "python_sessions/04_session5.html",
    "href": "python_sessions/04_session5.html",
    "title": "Making and Counting Bigrams",
    "section": "",
    "text": "The goal is, for a given book, find\n\nThe token most likely to follow “the”. What is its conditional probability?\nWhat is that token’s overall probability in the book?\nHow much does knowing the preceding word is “the” boost that token’s probability vs not know what the preceding word is?\n\n\n\nI’ve written the getbook.py script for you to be able to quickly download a book from project gutenberg with the header and license info stripped out. You can use it like this, in the shell:\n# bash\npython getbook.py 84 frankenstein.txt\n\n\n\n\n\nAfter reading in a book (and potentially .strip()ing off leading and trailing whitespace), you’ll need to glue all of the lines together into one big megastring for tokenizing. You can do that like so:\n```{python}\nmegastring = \" \".join(book_lines)\n```\n\n\n\nThere’s a convenient function called collections.Counter() that counts how many things are in a list, and returns a dictionary keyed by the values it counted, with its values as the dictionary values.\n\n#python\nfrom collections import Counter\n\nletters = [\"a\", \"a\", \"b\", \"b\", \"b\"]\nletters_c = Counter(letters)\n\nprint(letters_c)\n\nprint(letters_c[\"b\"])\n\nCounter({'b': 3, 'a': 2})\n3\n\n\nYou can also get the most common value from the counting dictionary with .most_common(1). This returns a list of “tuples”\n\n# python\nprint(letters_c.most_common(1))\n\n[('b', 3)]\n\n\n\n\n\n\nnltk has a few functions that will make this go easier.\n\n\nYou might need to run nltk.download('punkt')\n\n\n\nIn a long paragraph or a “megastring”, if we want bigram counts that are sensitive to sentence boundaries, that means we need to first split it up into sentences. We can do that with ntlk.sent_tokenize()\n\nimport pprint\npp = pprint.PrettyPrinter(indent = 2)\n\n\n# python\nfrom nltk import sent_tokenize\n\n\npara = \"This is a sentence. This is a sentence too. Is this?\"\nsentences = sent_tokenize(para)\npp.pprint(sentences)\n\n['This is a sentence.', 'This is a sentence too.', 'Is this?']\n\n\n\n\n\nDon’t forget to tokenize sentences into words\n\n# python\nfrom nltk import word_tokenize\n\nsentence_words = [word_tokenize(sent) for sent in sentences]\npp.pprint(sentence_words)\n\n[ ['This', 'is', 'a', 'sentence', '.'],\n  ['This', 'is', 'a', 'sentence', 'too', '.'],\n  ['Is', 'this', '?']]\n\n\n\n\n\nWe’ll also want to put start-of-sentence and end-of-sentence padding on each sentence, which we can do with nltk.lm.preprocessing.pad_both_ends()\n\n# python\n\nfrom nltk.lm.preprocessing import pad_both_ends\n\n# n = 2 because we're *going* to do bigrams\n# pad_both_ends returns a special object we're\n# converting to a list, just to see what's happening\nsentence_padded = [list(pad_both_ends(sent, n = 2)) \n                     for sent in sentence_words]\npp.pprint(sentence_padded)\n\n[ ['<s>', 'This', 'is', 'a', 'sentence', '.', '</s>'],\n  ['<s>', 'This', 'is', 'a', 'sentence', 'too', '.', '</s>'],\n  ['<s>', 'Is', 'this', '?', '</s>']]\n\n\n\n\n\nWe (finally!) get the bigrams in each sentence nltk.bigrams().\n\n# python\nfrom nltk import bigrams\n\n# Again, bigrams() returns a special object we're\n# converting to a list\nsent_bg = [list(bigrams(sent)) \n             for sent in sentence_padded]\npp.pprint(sent_bg)\n\n[ [ ('<s>', 'This'),\n    ('This', 'is'),\n    ('is', 'a'),\n    ('a', 'sentence'),\n    ('sentence', '.'),\n    ('.', '</s>')],\n  [ ('<s>', 'This'),\n    ('This', 'is'),\n    ('is', 'a'),\n    ('a', 'sentence'),\n    ('sentence', 'too'),\n    ('too', '.'),\n    ('.', '</s>')],\n  [('<s>', 'Is'), ('Is', 'this'), ('this', '?'), ('?', '</s>')]]\n\n\n\n\n\nBefore you try counting anything, you’re going to need to “flatten” this list of lists into just one flat list of all of the bigrams.\nleft as an exercise to the reader.\n\n\n\n\nWhen I find the “conditional probability” of the most common word following “the”, what I mean is “What is the probability of the word w, given that we just had ‘the’?”. Or, to put it in math terms \\(P(w | \\text{the})\\).\nThe conditional probability \\(P(w | \\text{the})\\) is equal to the joint probability of P(the, w) (a.k.a. the probability of that bigram out of all bigrams) divided by the probability of just “the”, \\(P(\\text{the})\\).\n\\[\nP(w|\\text{the}) = \\frac{P(\\text{the}~w)}{P(\\text{the})}\n\\]\nTo get the probablity of \\(P(\\text{the}~w)\\), you’ll need to divide the count of “the w” by the count of all bigram tokens (hint: this is just how long the list of bigrams is.)\nTo get the probability of just “the”, you’ll actually have to get a separate count of just all individual tokens, count how frequent “the” is, and divide that by the number of total tokens.\n\n\n\nTake a moment or two to list out each piece of code or information you’re going to need to get to do this project, at a high level. It doesn’t need to be complete, and you’ll probably come back to this list and revise it. But having a list like this will help guide you to what the next step in the process is."
  },
  {
    "objectID": "python_sessions/02_session3.html",
    "href": "python_sessions/02_session3.html",
    "title": "Loops Etc.",
    "section": "",
    "text": "Python gets really useful when we stop typing our data into the script directly, and start reading it in programmatically. There are lots of kinds of files you might want to read into python, and sometimes specialized libraries are involved.\n\n\n\n\n\n\nNote\n\n\n\nWe’re not going to be dealing with many of these file formats, but just to keep your imaginations open about the kind of data you can read into python:\n\n\n# comes with python\nimport json\nwith open(\"file.json\", \"r\") as f:\n  data = json.load(f)\n\n\n\n# requires pip install pyyaml\nimport yaml\nwith open(\"file.yml\", \"r\") as f:\n  data = yaml.safe_load(f)\n\n\n\n# requires pip install pandas\nimport pandas as pd\ndata = pd.read_csv(\"file.csv\")\n\n\n\n# requires pip install librosa\nwav, sr = librosa.load(\"file.wav\")\n\n\n\n# requires pip install imageio\nimport imageio\ndata = imageio.imread(\"file.png\")\n\n\n\nTo read in text data into python, we need to\n\nTell python where the file is (i.e. give it a path).\nTell python to open the file, and how to open the file.\nRead in the file.\n\n\n\nAdd this to your main.py script (the indentation matters!)\nfile_path = \"frankenstein\"\nwith open(file_path, \"r\") as f:\n   lines = f.readlines()\nThe object lines is now a list of all of the lines in Frankenstein.\nAssign the 55th line to a variable called line55 and print it."
  },
  {
    "objectID": "python_sessions/02_session3.html#scripting-next-steps",
    "href": "python_sessions/02_session3.html#scripting-next-steps",
    "title": "Loops Etc.",
    "section": "Scripting : next steps",
    "text": "Scripting : next steps\n\nImports\nThere is a lot of great functionality including in “base” python. However, we will usually want to access some functionality written by other people that is not included in the in basic python, in which case we’ll need to import it using import followed by the name of the module we’re importing.\nIt is usually best practice to put all imports at the TOP of your scripts\n\n💡 TASK 2\nImport both nltk and re in your main.py script nltk also requires a (one time) download, so add that as well\nimport nltk\nimport re\nnltk.download('punkt')\n\n\nAccessing functionality from the imports\nTo access the functions of the modules we’ve just imported, we type out the name of the module, a dot, and the function we want to access. For example, the re module has functions for doing things with regular expressions, including one called findall(). To access this function, we would type re.findall().\n\n💡 TASK 3\nuse the word_tokenize() function from nltk to tokenize the 55th line from Frankenstein. Assign the result to a variable called tokens55.\n\n\n\n\n\n\n\nOther import possibilities\n\n\n\nIn order to save yourself some typing, you can tell python to import a module, and to call it by a different name with import <> as <>. For example, by social convention, people tend to import the pandas package as pd.\nimport pandas as pd\ndata = pd.read_csv(\"file.csv\")\nYou can also import one off functions from a module with from <> import <>. We could replace the pandas example with\nfrom pandas import read_csv\ndata = read_csv(\"file.csv\")\nNote, this only gives you access to the read_csv() function and nothing else from the pandas module."
  },
  {
    "objectID": "python_sessions/02_session3.html#functions-vs-methods",
    "href": "python_sessions/02_session3.html#functions-vs-methods",
    "title": "Loops Etc.",
    "section": "Functions vs Methods",
    "text": "Functions vs Methods\nThere are (at least) two ways to interact with an object in python. The first is by passing the object to a function. For example, if we pass a string or a list to the len() function, it will tell us how long it is.\n\n💡 TASK 4\nGet the length of tokens55 with len() and assign it to the variable len55. Then add this print statement.\nprint(f\"There are {len55} tokens in line 55\")\n\nMost objects in python also have associated methods. Methods are like functions that are bundled into objects, and get applied to those methods. We’ve already worked with methods like .append() to add a value to a list, or .sort() to sort a list.\n\n💡 TASK 5\nWe can all of the methods associated with an object with dir(). Print out the the results of\ndir(tokens55)\n\nThe methods you’ll most often want to use don’t have __ before and after their names.\nYou can get help on how to use any method or function with help(). For example, to get help on nltk.word_tokenize() we would just add this to our script\nhelp(nltk.word_tokenize)\n\n💡 TASK 6\nUsing a mixture of dir() and help(), figure out how to use a method associated with tokens55 to get the index of \"regarded\". Assign this index to the variable regard_idx and add this print statement to your script.\nprint(f\"The index of 'regarded' is {regard_idx}\")"
  },
  {
    "objectID": "python_sessions/02_session3.html#loops",
    "href": "python_sessions/02_session3.html#loops",
    "title": "Loops Etc.",
    "section": "Loops",
    "text": "Loops\nSo far we’ve printed out an individual line from lines. If we wante to print out every line from Frankenstein, it would be a bad use of a compter to start listing\nprint(lines[0])\nprint(lines[1])\nprint(lines[2])\nprint(lines[3])\n...\nInstead, we can leverage computers’ ability to do repetitive and boring tasks very fast with a for loop.\n\n💡 TASK 7\nPut the following for loop in your script\nfor character in line55:\n    print(character)\nWhat happens?\n\nLet’s break down what’s happening here, step-by-step.\n\nFirst, python knows it’s going to be “looping over” the object line55.\nIt starts by taking the first value in line55, which is \"c\".\nIt assigns this value to the variable character.\nIt runs whatever code is inside of the loop, in this case, print(character).\nIt goes back to step 2), and gets the next value in line55, which is now \"o\".\nIt assigns this value to the variable character …\n\nAnd it will continue doing this until there are no more values to get out of line55.\n\nCollecting results\nWe can “collect” values from for loops by declaring a collector variable before the for loop, and then modifying it inside the loop.\nFor example, if we wanted to get the total length, in characters, in the book, we wouldn’t want to write it out this way:\ntotal_len = 0\ntotal_len += len(lines[0])\ntotal_len += len(lines[1])\ntotal_len += len(lines[2])\n...\nInstead, we’d want to write a for loop\n\n💡 TASK 8\nUsing a for loop, and looping over lines, tally up how many total characters there are in a variable called total_len.\n\n\n💡 TASK 9\nUsing a for loop, and looping over lines, collect all of the tokens you get from nltk.word_tokenize() in a single, flat list called all_tok.\n(hint: you’ll have to initialize an empty list like this: all_tok = [])\n\n\n\nConditionals\nWe can control the behavior of for loops a with if statements. Outside of the for loop context, we can see how if statements work.\ndoctor = \"Frankenstein\"\nmonster = \"Frankenstein's monster\"\nif monster == \"Frankenstein\":\n  print(\"The monster is named Frankenstein\")\nelse:\n  print(\"Actually, it was the *doctor* who was named Frankenstein\")\nThe comparison statement monster == \"Frankenstein” returns a True or False value. When an if statement gets a True value, it runs the code inside its block. Otherwise, it just passes onto the rest of the script, or it runs code in an else block.\n\n💡 TASK 10\nInitialize an empty list called five_character. Loop over the list of all tokens in all_tok. If a token has five characters, append it to five_character."
  },
  {
    "objectID": "python_sessions/03_session4.html",
    "href": "python_sessions/03_session4.html",
    "title": "Comprehensions and Useful Things",
    "section": "",
    "text": "We’re going to be exploring the way the spaCy package does tokenization.\nIf you get an error at the very beginning when hitting Run, run this code to download the spaCy model in the shell.\n# bash\npython -m spacy download en_core_web_sm\nCurrently, the code in main.py\n\nloads the spaCy English model\nreads in Frankenstein\nStrips leading whitespace from the beginning and end of each line\nConcatenates all of the lines into one megastring\nUses the spaCy analyzer to (among other things) tokenize the book.\n\n\nimport spacy\nfrom collections import Counter\nfrom collections import defaultdict\n\n# Load the spaCy english model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# open and read in Frankenstein\nwith open(\"gen/texts/frank.txt\", 'r') as f:\n  lines = f.readlines()\n\n# Remove leading and trailing whitespaces\nlines = [line.strip() for line in lines]\n\n# concatenate frankenstein into one huge string\nfrank_one_string = \" \".join(lines)\n\n# Tokenize all of frankenstein\nfrank_doc = nlp(frank_one_string)\n\nprint(frank_doc[500:600])\n\nriver. But supposing all these conjectures to be false, you cannot contest the inestimable benefit which I shall confer on all mankind, to the last generation, by discovering a passage near the pole to those countries, to reach which at present so many months are requisite; or by ascertaining the secret of the magnet, which, if at all possible, can only be effected by an undertaking such as mine.  These reflections have dispelled the agitation with which I began my letter, and I feel my heart glow\n\n\n\n\n\nWe can treat frank_doc like a list, but it’s actually a special data structure. The same goes for each token inside frank_doc. If you just say\n\nprint(frank_doc[506])\n\nconjectures\n\n\nIt will print conjectures. But if you say\n\nprint(\n  dir(frank_doc[506])\n)\n\n['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', 'ancestors', 'check_flag', 'children', 'cluster', 'conjuncts', 'dep', 'dep_', 'doc', 'ent_id', 'ent_id_', 'ent_iob', 'ent_iob_', 'ent_kb_id', 'ent_kb_id_', 'ent_type', 'ent_type_', 'get_extension', 'has_dep', 'has_extension', 'has_head', 'has_morph', 'has_vector', 'head', 'i', 'idx', 'iob_strings', 'is_alpha', 'is_ancestor', 'is_ascii', 'is_bracket', 'is_currency', 'is_digit', 'is_left_punct', 'is_lower', 'is_oov', 'is_punct', 'is_quote', 'is_right_punct', 'is_sent_end', 'is_sent_start', 'is_space', 'is_stop', 'is_title', 'is_upper', 'lang', 'lang_', 'left_edge', 'lefts', 'lemma', 'lemma_', 'lex', 'lex_id', 'like_email', 'like_num', 'like_url', 'lower', 'lower_', 'morph', 'n_lefts', 'n_rights', 'nbor', 'norm', 'norm_', 'orth', 'orth_', 'pos', 'pos_', 'prefix', 'prefix_', 'prob', 'rank', 'remove_extension', 'right_edge', 'rights', 'sent', 'sent_start', 'sentiment', 'set_extension', 'set_morph', 'shape', 'shape_', 'similarity', 'subtree', 'suffix', 'suffix_', 'tag', 'tag_', 'tensor', 'text', 'text_with_ws', 'vector', 'vector_norm', 'vocab', 'whitespace_']\n\n\nYou’ll see a lot more values and methods associated with the token than you normally would for a string. For example, frank_doc[506].text will give us the text of the token, and frank_doc[506].lemma_ will give us the lemma.\n\nprint(\n  f\"The word '{frank_doc[506].text}' is lemmatized as '{frank_doc[506].lemma_}'\"\n)\n\nThe word 'conjectures' is lemmatized as 'conjecture'\n\n\nOr we can get the guessed part of speech with frank_doc[506].pos_\n\nprint(\n  f\"The word '{frank_doc[506].text}' is given the part of speech '{frank_doc[506].pos_}'\"\n)\n\nThe word 'conjectures' is given the part of speech 'VERB'\n\n\nOr we can pull out the guessed morphological information:\n\nprint(\n  f\"spacy guesses '{frank_doc[506].text}' is '{frank_doc[506].morph}'\"\n)\n\nspacy guesses 'conjectures' is 'Number=Sing|Person=3|Tense=Pres|VerbForm=Fin'\n\n\n\n\n\nWe can use if statements to control how our code runs. An if statement checks to see if its logical comparison is true, and if it is, it executes its code.\n\n## This is not true, so it dosn't print\nif frank_doc[506].pos_ == \"NOUN\":\n  print(\"it's a verb!\")\n\n## This is true, so it prints\nif frank_doc[506].pos_ == \"VERB\":\n  print(\"it's a verb!\")\n\nit's a verb!\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nPrint the .text of every word whose .lemma_ is \"monster\"\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWith a for loop, create a list called five_letter which contains every 5 letter word from the book (a.k.a. .text is 5 characters long.)\n\n\n\n\n\n\n“Comprehensions” are a great shortcut around writing out a whole for loop. Let’s take the following list:\n\nrain_list = \"The rain in Spain stays mainly on the plain\".split(\" \")\nprint(rain_list)\n\n['The', 'rain', 'in', 'Spain', 'stays', 'mainly', 'on', 'the', 'plain']\n\n\nIf I wanted to capitalize all of those words, one way I could do it is with a for loop\n\nupper_rain = []\nfor word in rain_list:\n  upper_rain.append(word.upper())\n\nprint(upper_rain)\n\n['THE', 'RAIN', 'IN', 'SPAIN', 'STAYS', 'MAINLY', 'ON', 'THE', 'PLAIN']\n\n\nAlternatively, I could do it with a “list comprehension”:\n\nupper_rain2 = [word.upper() for word in rain_list]\n\nprint(upper_rain2)\n\n['THE', 'RAIN', 'IN', 'SPAIN', 'STAYS', 'MAINLY', 'ON', 'THE', 'PLAIN']\n\n\nList comprehensions keep the for word in rain_list part the same, but instead of needing to initialize a whole empty list, we wrap the whole thing inside [ ], which tells python we’re going to capture the results inside a list. The variable (& whatever we do to it) at the beginning of the command is what gets captured.\nWe can use if statements too.\n\nai_words = [word for word in rain_list if \"ai\" in word]\n\nprint(ai_words)\n\n['rain', 'Spain', 'mainly', 'plain']\n\n\nWe can even have nested for statements\n\nrain_list = \"The rain in Spain stays mainly on the plain\".split(\" \")\nletters = [letter\n            for word in rain_list\n              for letter in word]\nprint(letters)\n\n['T', 'h', 'e', 'r', 'a', 'i', 'n', 'i', 'n', 'S', 'p', 'a', 'i', 'n', 's', 't', 'a', 'y', 's', 'm', 'a', 'i', 'n', 'l', 'y', 'o', 'n', 't', 'h', 'e', 'p', 'l', 'a', 'i', 'n']\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWith a list comprehension, create a list called five_letter2 which contains every 5 letter word from the book (a.k.a. .text is 5 characters long.)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nBy whatever means necessary (but I recommend using a list comprehension), create a list containing all of the words with a VERB as .pos\n\n\n\n\n\n\nA set is another special python data structure that, among other things, will “uniquify” a list.\n\nbman_list = \"na na na na na na na na na na na na na na na na Batman\".split(\" \")\nbman_set = set(bman_list)\nprint(bman_set)\n\n{'na', 'Batman'}\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFind out how many total words there are in Frankenstein, excluding tokens with .pos of PUNCT and SPACE\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFind out how many total unique words (.text) there are in Frankenstein, excluding tokens with .pos of PUNCT and SPACE\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFind out how many total unique lemmas (.lemma_) there are in Frankenstein, excluding tokens with .pos of PUNCT and SPACE\n\n\n\n\n\n\nThere is a handy dandy function called Counter that we can import from the collections module like so\n\nfrom collections import Counter\n\nWhen we pass Counter() a list, it will return a dictionary of counts of items in that list.\n\nbman_list = \"na na na na na na na na na na na na na na na na Batman\".split(\" \")\nbman_count = Counter(bman_list)\nprint(bman_count)\n\nCounter({'na': 16, 'Batman': 1})\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nCreate a counter dictionary of all of the forms of “be” (.lemma == \"be\") in Frankenstein"
  },
  {
    "objectID": "python_sessions/01_session2.html",
    "href": "python_sessions/01_session2.html",
    "title": "Lists and Dictionaries",
    "section": "",
    "text": "In the previous module, we learned about assignment and the kinds of values we can have in python. Just a quick recap:"
  },
  {
    "objectID": "python_sessions/01_session2.html#lists",
    "href": "python_sessions/01_session2.html#lists",
    "title": "Lists and Dictionaries",
    "section": "Lists",
    "text": "Lists\nWe’ll rarely want to work with just one of string values. More often we’ll want to work with a collection of values. For this we’ll use lists. For example, here is a list of Mary Shelly’s published novels.\nnovels = [\"Frankenstein\",\n          \"Valperga\",\n          \"The Last Man\", \n          \"The Fortunes of Perkin Warbeck, A Romance\", \n          \"Lodore\", \n          \"Falkner. A Novel\"]\nAssignment works the same way as before. We can use any text, without commas, as a variable name, then assign a value to it with the = operator.\n\n💡 TASK 1\nAssign the names of people in your group (first and last) to a list called our_group.\n\nLists don’t just have to contain strings, but can actually contain any mixture of data types.\npublished_dates = [1818, 1823, 1826, 1830, 1835, 1837]\n\nLists can get complex\nYou can have lists that contain other lists.\nmy_signs = [\"Joe\", [\"Capricorn\", \"Gemini\", \"Capricorn\"]]\n\n💡 TASK 2\nEach person in the group create a list of lists, with your name as the first value, and your big three in a list as the second value. Assign this value to a variable named with your initials. (If you don’t know your big three or don’t want to share, just say “Capricorn” across the board.)\n\n\n\nMaking changes to lists\n\nAdding items\nThere are a few ways you can add additional values to lists. One way is with the + operator. For example, one of Mary Shelley’s books, Mathida was published posthumously.\nposthumous = [\"Mathilda\"]\nWe could add this to the list of novels like so.\nprint(novels + posthumous)\nOne thing you should note is that this doen’t change the list novels. If you run print(novels) now, it will have just the original books in it.\nTo change the actual list stored in novels, we need to use the .append() “method”. We’ll talk more about “methods” vs functions in later lessons\nnovels.append(\"Mathilda\")\nprint(novels)\nA thing to remember about this is .append() changes the variable novels “in place.” That is, without doing any additional assignment, or anything, you’ve changed its value.\n\n💡 TASK 3\nCreate a variable called group_sign which is a list of each person’s star sign list from Task 2.\n\n\n\nSorting\nTo sort a list, alphabetically for strings or numerically for numbers, you can use the .sort() method. Again, this changes the list in place.\n\n💡 TASK 4\nAlphabetically sort the titles of Mary Shelly’s novels.\n\n\n\n\nIndexing Lists (and other “iterables”)\nIn order to pull values out of a list, we need to “index” it. Here’s a really important thing to remember:\nIndexing Starts at 0!\nTo get the fist value out of a list we use the index value 0, and pass it in-between brackets, like so:\nprint(novels[0])\nIn the alphabetically ordered list, this should return Falkner, A Novel\n\n💡 TASK 5\nPrint the name of the second member in our_group.\n\nNumeric indexing works with any “iterable” in python. For example, if we assigned the first novel to a variable, and started indexing that, it would start printing individual letters.\nfirst_novel = novels[0]\nprint(first_novel[2])\nThis will have printed out the third letter of the first novel’s title.\n🚨We can do this better by stacking up indices.\nInstead of assigning the first valye to a variable, we can get the same result by just placing these indexing brackets one after another.\nprint(novels[0][2])\n\n💡 TASK 6\nPrint the second letter from the first value in the list our_group.\n\n\n💡 TASK 7\nPrint the moon sign of the third member of your group, as stored in our_signs from task 3.\n\n\n\nSlicing\nIf we want to get a range of value out of the list, we can use a “slice”. For example, to get the first three books out of the list novels, we can do\nprint(novels[0:3])\nNow… this should strike you as weird, because to get the third value from the list, you use novels[2]. Why does the slice end in 3?\nWe should think about the relationship between values and indices like this:\n\nRather than being set one on top of eachother, the indices come between the values. When you give just one numeric index, python returns the first value to the right. When you pass python a slice with a starting index and and ending index, it returns everything between those indices.\n\n\nReverse indexing.\nThere’s a quick way to get the final value from a list or iterable as well: index with -1.\n\n💡 TASK 8\nPrint the last letter from the last value in the list our_group\n\n\n💡 TASK 9\nPrint the rising sign of the first person as stored in our_signs"
  },
  {
    "objectID": "python_sessions/01_session2.html#dictionaries",
    "href": "python_sessions/01_session2.html#dictionaries",
    "title": "Lists and Dictionaries",
    "section": "Dictionaries",
    "text": "Dictionaries\nWhile lists can be highly complex, and can even capture the relationships between data, they can be a bit limiting. For example, here’s a list representing the relationship between the title and the year of publication of Mary Shelley’s first and last novels.\nnovel_dates = [[\"Frankenstein\", 1818],\n               [\"Mathilda\",     1959]]\nWhile this does the job, if I came along later, and all I knew was the title “Frankenstein” and wanted to quickly get the date, it would take a bit of work with these nested lists.\nWhat would speed up the job are “dictionaries”, which store key:value pairs.\nnovel_dict = {\"Frankenstein\"     : 1818,\n              \"Valperga\"         : 1823,\n              \"The Last Man\"     : 1826, \n              \"The Fortunes of Perkin Warbeck, A Romance\" : 1830,\n              \"Lodore\"           : 1835,\n              \"Falkner. A Novel\" : 1837}\nThese dictionaries are created with opening and closing curly brackets { }, and have a sequence of key : value pairs. The “key” is called the “key”, because instead of indexing dictionaries with numeric values, we index them with whatever the keys are.\nSo to get the publication date of Frankenstein, we do\nnovel_dict[\"Frankenstein\"]\nTo add a previously absent key : value pair to a dictionary, we pass the new key we want to use to [ ], and then assign the new value.\nnovel_dict[\"Mathilda\"] = 1959\n\n💡 TASK 10\nCreate a dictionary where the keys are the names (first and last) of your group members, and the values are your heights, in inches (as a numeric value).\n(Hint, if you’re 5’10, the math would be (5*12) + 10)\n\n\n💡 TASK 11\nPrint the height of the alphebetically first group member. (Don’t just type in their name, get it from python).\n\n\nChecking for keys\nTo check if a key is already in the dictionary, we can use the in operator.\n`python \"Mary: A Fiction\" in novel_dict # False"
  },
  {
    "objectID": "python_sessions/00_session1.html",
    "href": "python_sessions/00_session1.html",
    "title": "Starting Python",
    "section": "",
    "text": "Welcome to Python!\nIf you have never done programming before, all python is is a program that reads a text file, and executes commands contained in the text file."
  },
  {
    "objectID": "python_sessions/00_session1.html#its-a-text-file",
    "href": "python_sessions/00_session1.html#its-a-text-file",
    "title": "Starting Python",
    "section": "It’s a text file",
    "text": "It’s a text file\nGo ahead and open the text file fake.txt. It contains text that is all valid python code. If you pop over to the shell and run\npython3 fake.txt\nPython will happily run the code in fake.txt. However, I recommend we always save our python scripts with the .py file extension for a few reasons.\n\nEveryone does it, so it’ll match human expectations for people looking at your code.\nAny worthwhile code editor (replit included) will decide what the text in the file is supposed to be based on the file extension, and trys to help you accordingly with things like\n\nSyntax highlighting, making your code easier to read.\nAutocomplete assistance (e.g. if you type an open (, it’ll automatically insert the closing ).\nCode suggestions. Some text editors will try to clue you with the possible names of functions your trying to type if you just type in the first few letters, and then may even try to clue you with the names of arguments to the functions."
  },
  {
    "objectID": "python_sessions/00_session1.html#interacting-python",
    "href": "python_sessions/00_session1.html#interacting-python",
    "title": "Starting Python",
    "section": "Interacting Python",
    "text": "Interacting Python\nThere are are a bunch of different ways you can interact with python to get it to interpret your code.\n\nInteractive Session\nIf you go to the Shell and just run python3 without any other arguments, it will launch the python shell, which inteprets the text you type in as python code. If you copy-paste this code into the python shell, it should print hello world back at you.\nprint(\"hello world\")\nTo quit the python shell, run\nquit()\n\n\n“Notebooks” (not in replit)\nThere are a few “notebook” options out there which allow you to interleave text, like notes to yourself, or descriptions of the code, with python code chunks.\nYou can experiment with free online notebooks by setting up an account at Kaggle or Google Colab.\nFor your local set up, I’d recommend either configuring VS Code to run Jupyter notebooks, or use a Quarto document in RStudio (it’s not just for R!)\n\n\nScripts\nThe primary way we’ll be interacting with python inside of replit is with python “scripts,” which are just text documents of code python should run, line by line."
  },
  {
    "objectID": "python_sessions/00_session1.html#getting-started",
    "href": "python_sessions/00_session1.html#getting-started",
    "title": "Starting Python",
    "section": "Getting Started",
    "text": "Getting Started\nThe python script main.py already has some code in it.\nimport numpy as np\n\n# This is a comment\n# Python won't interpret or run a line starting with #\n\nages = np.array([40, 42, 25])\nyear = np.array([2016, 2001, 2019])\n\n💡 TASK 1\nBelow this line (it doesn’t matter how many new lines you add) enter this line of code crucially without any spaces or tabs preceding it.\nprint(\"hello world\")\nOnce you’ve done that, hit the big green run button, and hello world should print out in the console.\n\n\nHow to see what python is doing\nWe’re going to be using the print() function a lot. The primary way we’ll be interacting with python is via python scripts, and the only way to see what our code has done is to explicitly tell python to print the output."
  },
  {
    "objectID": "python_sessions/00_session1.html#values-variables-and-assignment",
    "href": "python_sessions/00_session1.html#values-variables-and-assignment",
    "title": "Starting Python",
    "section": "Values, Variables, and Assignment",
    "text": "Values, Variables, and Assignment\nBefore we get into what the stuff at the very top of the script means, we’re going to first cover the basics of values, variable, and assignment.\n\nValues\nThe main python value types are\n\nNumeric\nCharacter\nBoolean (True/False)\n\n\nNumeric Practice\n\n💡 TASK 2\nCalculate how many seconds and print the output by adding this line to your script.\n print(f\"There are {365 * 24 * 60} minutes in a year.\")\n\nWhat python has done is multiplied 365 (for the number of days) by 24 (for the number of hours in a day) by 60 (for the number of minutes in an hour) to produce the number of minutes in a day.\nPython can do any kind of arithmetic you ask of it. For example, we can caclute what percent of a whole day one minute is by adding this line of code to our script.\n\n💡 TASK 3\nCalculate how many seconds and print the output by adding this line to your script.\n print(f\"One minute is {(1/(24 * 60)) * 100}% of a day.\")\n\n\n💡 TASK 4\nChapter 1 of Frankenstein has 1,780 words, and 75 of them were the word “of”. Calculate what percent of words were “of” by adding this line of code to your script.\nprint(f\"{}% of words in Chapter 1 of Franenstein were 'of'\")\nFill in the correct mathematical formula inside the {}\n\n\n\n\nAssignment\nWe don’t usually want to just do some calculations and then just let the values disappear when we print them, though. We’ll usually want to save some values for future use. We can do that by “assigning” values to “variables.”\nThe assignment operator in python is =. For example we can assign the name of this class to a variable called this_class like so:\nthis_class = \"Lin517\"\n\n💡 TASK 5\nAssign the value \"Lin517\" to this_class\n\n\n💡 TASK 6\nPrint the variable this_class\n\n\nImportant Things to Note!\n\nThe variable this_class did not exist before we did assignment! If we had asked python to print this_class before we did the assignment, it would have given us an error.\nVariable names are case sensitive! If we tried to print This_class or this_Class or This_Class they would all return an error.\nYou can start variable names with any letter or underscore, but that’s all (no numbers at the start).\nAfter the first character, you can use any letter, number, or underscore.\nNo &, ., * or ? are allowed.\nAny text that isn’t enclosed inside \" \" will be interpreted as a variable name."
  },
  {
    "objectID": "python_sessions/00_session1.html#doing-things-with-variables.",
    "href": "python_sessions/00_session1.html#doing-things-with-variables.",
    "title": "Starting Python",
    "section": "Doing things with variables.",
    "text": "Doing things with variables.\nOnce you assign a value to a variable, it can stand in as if it was that variable. For example.\ndays_in_year    = 365\nhours_in_day    = 24\nminutes_in_hour = 60\n\nprint(f\"There are {days_in_year * hours_in_day * minutes_in_hour} minutes in a year\")\n\n💡 TASK 7\n\nAssign the current year to a variable called this_year.\nAssign one of your ages to a variable called my_age\nCalculate your year of birth by subtracting my_age from this_year\nPrint the result.\n\n\n\n💡 TASK 8\nCalculate how old you’ll be in 2040 and print the result.\n\nYou can overwrite the value you’ve assigned to any variable by just assigning a new value to it."
  },
  {
    "objectID": "python_sessions/00_session1.html#numbers",
    "href": "python_sessions/00_session1.html#numbers",
    "title": "Starting Python",
    "section": "Numbers",
    "text": "Numbers\nTechnically, there are two kinds of numbers in Python: integers (numbers without decimals places) and floats (numbers with decimal places). This used to be a bigger deal in python2, but python3 converts as necessary. We’ve already done some work with numbers above. The built in arithmetic in python that we can use on numbers is:\n\nx + y addition\nx - y subtraction\nx * y multiplication\nx / y division\nx ** y exponentiation (that is, xy)\nx % y modulus (this gives you the remainder of doing division)\nx // y floor division (this gives you the largest whole number that y can go into x)\n\n\n💡 TASK 9\nMary Shelly has written 1,780 words for Frankenstein Chapter 1, and her publisher has told her there is a strict word limit of 300 words per page. 1. Calculate how many full pages chapter 1 is going to be. Assign this value to the variable full_pages. 2. There’s going to be some words left over. Calculate how many words are going to go onto the overflow page. Assign this value to a variable called overflow."
  },
  {
    "objectID": "python_sessions/00_session1.html#strings",
    "href": "python_sessions/00_session1.html#strings",
    "title": "Starting Python",
    "section": "Strings",
    "text": "Strings\nWe’ve already been doing a lot with strings in these print() statements. But just to be explicit, everything that comes inside \" \" is interpreted as a string, even numbers. If you tried to do\n1 + \"1\"\nYou would get an error, because the first value is a number and the second value is a string.\nWe can use some math-looking-things on strings, though.\n\n💡 TASK 10\nDo the following assignments.\nroot    = \"Lingu\"\naffixes = \"istics\"\nNow, print what happens when you do root + affixes\n\n\n💡 TASK 11\nAgain, do the following assignments.\nframe = \"It's a \"\nword  = \"salad \"\nredup = word * 2\nNow, print the result of what happens when you do frame + redup"
  },
  {
    "objectID": "lectures/data_sparsity/data_sparsity.html",
    "href": "lectures/data_sparsity/data_sparsity.html",
    "title": "Data Sparsity",
    "section": "",
    "text": "Let’s say we’re biologists, working in a rain forest, and put out a bug net to survey the biodiversity of the forest. We catch 10 bugs, and each species is a different color:\n[\\(_1\\), \\(_2\\), \\(_3\\), \\(_4\\), \\(_5\\), \\(_6\\), \\(_7\\), \\(_8\\), \\(_9\\), \\(_{10}\\)]\nWe have 10 bugs in total, so we’ll say \\(N=10\\). This is our “token count.” We’ll use the \\(i\\) subscript to refer to each individual bug (or token).\nIf we made a table of each bug species, it would look like:\n\n\n\nspecies\nindex \\(j\\)\ncount\n\n\n\n\n\n1\n5\n\n\n\n2\n2\n\n\n\n3\n1\n\n\n\n4\n1\n\n\n\n5\n1\n\n\n\nLet’s use \\(M\\) to represent the total number of species, so \\(M=5\\) here. This is our type count, and we’ll the subscript \\(j\\) to represent the index of specific types.\nWe can mathematically represent the count of each species like so.\n\\[\nc_j = C(\\class{fa fa-bug}{}_j)\n\\]\nHere, the function \\(C()\\) takes a specific species representation \\(\\class{fa fa-bug}{}_j\\) as input, and returns the specific count \\(c_j\\) for how many times that species showed up in our net. So when \\(j = {\\color{#785EF0}{1}}\\), \\(\\color{#785EF0}{c_1}=5\\), and when \\(j = {\\color{#FFB000}{4}}\\), \\(\\color{#FFB000}{c_4}=1\\).\nHere’s a plot, with the species id \\(j\\) on the x-axis, and the number of times that species appeared in the net \\(c_j\\) on the y-axis.\n\n\n\n\n\n\n\nWhat is the probability that tomorrow, when we put the net out again, that the first bug we catch will be from species ? Usually in these cases, we’ll use past experience to predict the future. Today, of the \\(N=10\\) bugs we caught, \\(\\color{#785EF0}{c_1}=5\\) of them were species . We can represent this as a fraction like so:\n\\[\n\\frac{{\\color{#785EF0}{\\class{fa fa-bug}{}}}_1,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_2,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_3,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_4,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_5}\n{{\\color{#785EF0}{\\class{fa fa-bug}{}}}_1,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_2,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_3,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_4,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_5,\n      {\\color{#DC267F}{\\class{fa fa-bug}{}}}_6,\n      {\\color{#DC267F}{\\class{fa fa-bug}{}}}_7,\n      {\\color{#FE6100}{\\class{fa fa-bug}{}}}_8,\n      {\\color{#FFB000}{\\class{fa fa-bug}{}}}_9,\n      {\\color{#4C8C05}{\\class{fa fa-bug}{}}}_{10}}\n\\]\nOr, we can simplify it a little bit. The top part (the numerator) is equal to \\(\\color{#785EF0}{c_1}=5\\), and the bottom part (the denominator) is equal to the total number of bugs, \\(N\\). Simplifying then:\n\\[\n\\frac{\\color{#785EF0}{c_1}}{N} = \\frac{5}{10} = 0.5\n\\]\nWe’ll use this as our guesstimate of the probability that the very next bug we catch will be from species . Let’s use the function \\(\\hat{P}()\\) to mean “our method for guessing the probability”, and \\(\\hat{p}\\) to represent the guess we came to. We could express “our guess that the first bug we catch will be ” like so.\n\\[\n{\\color{#785EF0}{\\hat{p}_1}} = \\hat{P}({\\color{#785EF0}{\\class{fa fa-bug}{}}}) = \\frac{\\color{#785EF0}{c_1}}{N} = \\frac{5}{10} = 0.5\n\\]\nWe can then generalize our method to any bug like so:\n\\[\n\\hat{p}_j = \\hat{P}(\\class{fa fa-bug}{}_j) = \\frac{c_j}{N}\n\\]\n\n\n\nLet’s say we set out the net again, and the first bug we catch is actually . This is a new species of bug that wasn’t in the net the first time. Makes enough sense, the forest is very large. However, what probability would we have given catching this new species?\nWell, \\(\\color{#35F448}{c_6} = C({\\color{#35F448}{\\class{fa fa-bug}{}}}) = 0\\). So our estimate of the probability would have been \\({\\color{#35F448}{\\hat{p}_6}} = \\hat{P}({\\color{#35F448}{\\class{fa fa-bug}{}}}) = \\frac{\\color{#35F448}{c_6}}{N} = \\frac{0}{10} = 0\\).\nWell obviously, the probability that we would catch a bug from species  wasn’t 0, because events with 0 probability don’t happen, and we did catch the bug. Admittedly, \\(N=10\\) is a small sample to try and base a probability estimate on, so how large would we need the sample to be before we could make probabity estimates for all possible bug species, assuming we stick with the probability estimating function \\(\\hat{P}(\\class{fa fa-bug}{}_j) = \\frac{c_j}{N}\\)?"
  },
  {
    "objectID": "lectures/data_sparsity/data_sparsity.html#youd-need-fa-infinity",
    "href": "lectures/data_sparsity/data_sparsity.html#youd-need-fa-infinity",
    "title": "Data Sparsity",
    "section": "You’d need ",
    "text": "You’d need \nThis kind of data problem does arise for counting species, but this is really a tortured analogy for language data.1 For example, let’s take all of the words from Chapter 1 of Mary Shelly’s Frankenstein, downloaded from Project Gutenberg. I’ll count how often each word occurred, and assign it a rank, with 1 being given to the word that occurred the most.1 For me, I used this analogy to include colorful images of bugs in the lecture notes. For Good (1953), they had to use a tortured analogy since the methods for fixing probability estimates were still classified after being used to crack the Nazi Enigma Code in WWII.\n\n\n\n\n\n\n\n\n\nJust to draw the parallels between the two analogies:\n\n\n\n\n\n\n\n\nvariable\nin the analogy\nin Frankenstein Chapter 1\n\n\n\n\n\\(N\\)\nThe total number of bugs caught in the net. (\\(N=10\\))\nThe total number of words in the first chapter. (\\(N=1,780\\)).\n\n\n\\(x_i\\)\nAn individual bug. e.g. \\(_1\\)\nAn individual word token. In chapter 1, \\(x_1\\) = “i”\n\n\n\\(w_j\\)\nA bug species. \nA word type. The indices are frequency ordered, so for chapter 1 \\(w_1\\) = “of”\n\n\n\\(c_j\\)\nThe count of how many individuals there are of a species.\nThe count of how many tokens there are of a type.\n\n\n\nHere’s a table of the top 10 most frequent word types.\n\n\n\n\n\n\\(w_j\\)\n\\(c_j\\)\n\\(j\\)\n\n\n\n\nof\n75\n1\n\n\nthe\n75\n2\n\n\nand\n70\n3\n\n\nto\n61\n4\n\n\na\n52\n5\n\n\nher\n52\n6\n\n\nwas\n40\n7\n\n\nmy\n33\n8\n\n\nin\n32\n9\n\n\nhis\n29\n10\n\n\n\n\n\nIf we plot out all of the word types with the rank (\\(j\\)) on the x-axis and the count of each word type (\\(c_j\\)) on the y-axis, we get a pattern that if you’re not already familiar with it, you will be.\n\n\n\n\n\nThis is a “Zipfian Distribution” a.k.a. a “Pareto Distribution” a.k.a. a “Power law,” and it has a few features which make it ~problematic~ for all sorts of analyses.\nFor example, let’s come back to the issue of predicting the probability of the next word we’re going to see. Language Models are “string prediction models,” after all, and in order to get a prediction for a specific string, you need to have seen the string in the training data. Remember how our bug prediction method had no way of predicting that we’d see a  because it had never seen one before?\nThere are a lot of possible string types of “English” that we have not observed in Chapter 1 of Frankenstein. Good & Turing proposed that you could guesstimate that the probability of seeing a never before seen “species” was about equal to the proportion of “species” you’d only seen once. With just Chapter 1, that’s a pretty high probability that there are words you haven’t seen yet.\n\n\n\n\n\nseen once?\ntotal\nproportion\n\n\n\n\nno\n1216\n0.683\n\n\nyes\n564\n0.317\n\n\n\n\n\nSo, let’s increase our sample size. Here’s the same plot of rank by count for chapters 1 through 5.\n\n\n\n\n\n\n\n\n\n\nseen once?\ntotal\nproportion\n\n\n\n\nno\n9928\n0.858\n\n\nyes\n1649\n0.142\n\n\n\n\n\nWe increased the size of the whole corpus by a factor of 10, but we’ve still got a pretty high probability of encountering an unseen word.\nLet’s expand it out to the whole book now.\n\n\n\n\n\n\n\n\n\n\nseen once?\ntotal\nproportion\n\n\n\n\nno\n72122\n0.96\n\n\nyes\n3021\n0.04\n\n\n\n\n\n\n🎵 Ain’t no corpus large enough 🎵\nAs it turns out, there’s no corpus large enough to guarantee observing every possible word at least once, for a few reasons.\n\nThe infinite generative capacity of language! The set of all possible words is, in principle infinitely large.\nThese power law distributions will always have the a lot of tokens with a frequency of 1, and even just those tokens are going to have their probabilities poorly estimated.\n\nTo illustrate this, I downloaded the 1-grams of just words beginning with [Aa] from the Google Ngrams data set. This is an ngram dataset based on all of the books scanned by the Google Books project. It’s 4 columns wide, 86,618,505 rows long, and 1.8G large, and even then I think it’s a truncated version of the data set, because the fewest number of years any given word appears is exactly 40.\nIf we take just all of the words that start with [Aa] published in the year 2000, the most common frequency for a word to be is still just 1, even if it is a small proportion of all tokens.\n\n\n\nFrequencies of frequencies in words starting with [Aa] from the year 2000 in google ngrams \n\n\n\n\n\n\n\nword frequency\nnumber of types with frequency\nproportion of all tokens\n\n\n\n\n1\n205141\n4.77e-10\n\n\n2\n152142\n9.55e-10\n\n\n3\n107350\n1.43e-09\n\n\n4\n80215\n1.91e-09\n\n\n5\n60634\n2.39e-09\n\n\n6\n47862\n2.86e-09\n\n\n\n\n\n\n\nAn aside\nI’ll be plotting the rank vs the frequency with logarithmic axes from here on. Linear axes give equal visual space for every incremental change in the x and y values, while lograrithmic axes put more space between smaller numbers than larger numbers.\n\n\n\n\n\n\nrank by frequency on linear scales\n\n\n\n\n\n\n\nrank by frequency on logarithmic scales\n\n\n\n\n\n\n\nIt gets worse\nWe can maybe get very far with our data sparsity for how often we’ll see each individual word by increasing the size of our corpus size, but 1gram word counts are rarely as far as we’ll want to go.\nTo come back to our bugs example, let’s say that bug species  actually hunts bug species . If we just caught a  in our net, it’s a lot more likely that we’ll catch a  next, coming after the helpless  than it would be if we hadn’t just caught a . To know what exactly the probability catching  and then a  is, we’d need to count up every 2 bug sequence we’ve seen.\nBringing this back to words, 2 word sequences are called “bigrams” and 3 word sequences are called “trigrams,” and they are also distributed according to a Power Law, and each larger string of words has a worse data sparsity one than the one before. But each larger string of words means more context, which makes for better predictions."
  },
  {
    "objectID": "lectures/data_sparsity/data_sparsity.html#some-notes-on-power-laws",
    "href": "lectures/data_sparsity/data_sparsity.html#some-notes-on-power-laws",
    "title": "Data Sparsity",
    "section": "Some Notes on Power Laws",
    "text": "Some Notes on Power Laws\nThe power law distribution is pervasive in linguistic data, in almost every domain where we might count how often something happens or is observed. This is absolutely a fact that must be taken into account when we develop our theories or build our models. Some people also think it is an important fact to be explained about language, but I’m deeply skeptical.\nA lot of things follow power law distributions. The general property of these distributions is that the second most frequent thing will have a frequency about as half as the most frequent thing, the third most frequent thing will have a frequency about a third of the most frequent thing, etc. We could put that mathematically as:\n\\[\nc_j = \\frac{c_1}{j}\n\\]\nFor example, here’s the log-log plot of baby name rank by baby name frequency in the US between 1880 and 2017.22 Data from the babynames R package, which in turn got the data from the Social Security Administration.\n\n\n\n\n\nrank by frequency of baby names\n\n\n\n\nThe log-log plot isn’t perfectly straight (it’s common enough for data like this to have two “regimes”).\nHere’s the number of ratings each movie on IMDB has received.\n\n\n\n\n\nIf we break down the movies by their genre, we get the same kind of result.\n\n\n\n\n\nOther things that have been shown to exhibit power law distributions (Newman 2005; Jiang and Jia 2011) are\n\nUS city populations\nnumber of citations academic papers get\nwebsite traffic\nnumber of copies books sell\nearthquake magnitudes\n\nThese are all possibly examples of “preferential attachment”, but we can also create an example that doesn’t involve preferential attachment, and still wind up with a power-law. Let’s take the first 12 words from Frankenstein:\n\n\n\n\"to\"\"mrs\"\"saville\"\"england\"\"st\"\"petersburgh\"\"dec\"\"11th\"\"17\"\"you\"\"will\"\"rejoice\"\n\n\n\nNow, let’s paste them all together into one long string with spaces.\n\n\n\n\"to mrs saville england st petersburgh dec 11th 17 you will rejoice\"\n\n\nAnd now, let’s choose another arbitrary symbol to split up words besides \" \". I’ll go with e.\n\n\n\n\"to mrs savill\"\" \"\"ngland st p\"\"t\"\"rsburgh d\"\"c 11th 17 you will r\"\"joic\"\"\"\n\n\n\nThe results aren’t words. They’re hardly useful substrings. But, if we do this to the entire novel and plot out the rank and count of thes substrings like they were words, we still get a power law distribution.\n\n\n\n\n\nIn fact, if I take the top 4 most frequent letters, besides spaces, that occur in the text and use them as substring delimiters, the resulting substring distributions are all power-law distributed.\n\n\n\n\n\nThey even have other similar properties often associated with power law distributions in language. For example, it’s often been noted that more frequent words tend to be shorter. These weird substrings exhibit that pattern even more strongly than actual words do!\n\n\n\n\n\nThis is all to say, be cautious about explanations for power-law distributions that are"
  },
  {
    "objectID": "lectures/data_sparsity/data_sparsity.html#extra",
    "href": "lectures/data_sparsity/data_sparsity.html#extra",
    "title": "Data Sparsity",
    "section": "Extra",
    "text": "Extra\nTo work out just how accurate the Good-Turing estimate is, I did the following experiment.\nStarting from the beginning of the book, I coded each word \\(w_i\\) for whether or not it had already appeared in the book, 1 if yes, 0 if no. This is my best shot at writing that out in mathematical notation.\n\\[\na_i = \\left\\{\\begin{array}{ll}1,& x_i\\in x_{1:i-1}\\\\\n                             0,& x_1 \\notin x_{1:i-1}\\end{array}\\right\\}\n\\]\nThen for every position in the book, I made a table of counts of all the words up to that point in the book so far, and got the proportion of word tokens that had appeared only once. Again, here’s my best stab at writing that out mathematically.\n\\[\nc_{ji} = C(w_j), w_j \\in x_{i:i-1}\n\\]\n\\[\nr_i = \\sum_{j=1}\\left\\{\\begin{array}{ll}1,&c_{ji}=1\\\\0,& c_{ji} >1 \\end{array}\\right\\}\n\\]\n\\[\ng_i = \\frac{r_i}{i-1}\n\\]\n\nfrank_words$first_appearance <- NA\nfrank_words$first_appearance[1] <- 1\n\nfrank_words$gt_est <- NA\nfrank_words$gt_est[1] <- 1\nfor(i in 2:nrow(frank_words)){\n  i_minus <- i-1\n  prev_corp <- frank_words$word[1:i_minus]\n  this_word <- frank_words$word[i]\n  \n  frank_words$first_appearance[i] <- ifelse(this_word %in% prev_corp, 0, 1)\n  frank_words$gt_est[i] <- sum(table(prev_corp) == 1)/i_minus\n}\n\n\n\n\nThen, I plotted the Good-Turing estimate for every position as well as a non-linear logistic regression smooth."
  },
  {
    "objectID": "lectures/word_vectors/03_word2vec.html",
    "href": "lectures/word_vectors/03_word2vec.html",
    "title": "word2vec",
    "section": "",
    "text": "When we built term-context matrices, the values that went into each cell of the word-by-context matrix were fairly concrete. Let’s look these two example of the word monster as our target word and its surrounding context\n['wretch', 'the', 'miserable', 'monster', 'whom', 'i', 'had']\n['i', 'was', 'the', 'monster', 'that', 'he', 'said']\nWith these two windows of context around the target word, we’d go in and fill in the term-context matrix with the counts of every word that appeared in the context of monster. I’m also going to fill in some made up numbers for the word fiend.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngrabbed\nthe\nwretch\nwhom\nhad\nthat\nhe\nwas\ni\nsaid\nlurched\n\n\n\n\n…\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonster\n0\n2\n1\n1\n1\n1\n1\n1\n2\n1\n0\n\n\nfiend\n1\n1\n0\n0\n1\n1\n2\n1\n0\n0\n2\n\n\n…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith these vectors for monster and fiend we could get a measure of their similarity by taking their dot product (multiplying the values elementwise, then summing up).\nmonster  0  2  1  1  1  1  1  1  2  1  0\n  x\nfiend    1  1  0  0  1  1  2  1  0  0  0  \n  = \n         1  2  0  0  1  1  2  1  0  0  0\n                       SUM\n                        =\n                        8\nA problem with term-context matrices (that can’t even be solved by converting them to Positive Pointwise Mutual Information) is that they are sparse, mostly filled with 0s. This can be a problem mathematically, since having a lot of 0s in a data set is really inviting a “divide-by-zero” error at some point, but also a problem data-wise. If I had an enormous corpus, made a term-context matrix out of it, and tried emailing it to you, the file size would be really large, but the vast majority of the file size is taken up just by 0."
  },
  {
    "objectID": "lectures/word_vectors/03_word2vec.html#getting-more-abstract-and-dense",
    "href": "lectures/word_vectors/03_word2vec.html#getting-more-abstract-and-dense",
    "title": "word2vec",
    "section": "Getting more abstract, and dense",
    "text": "Getting more abstract, and dense\nThere are ~7,000 unique vocabulary items in Frankenstein meaning the term-context vector for every word is ~7,000 numbers long. What if instead of using 7,000 numbers to describe a word’s context, we used something smaller, like 300 numbers, none of which are 0, to describe the word’s context.\n\nA prediction task\nThe way we’ll get to these vectors of numbers is with a prediction task. We’ll give our prediction model a vector associated with a target word, say monster, and then we’ll give it a vector for a word that actually appears in its context, and another that doesn’t.\n\nword: monster = [, , , , …]\ncontext: miserable = [, , , ,…]\ndistractor: waddle = [, , , , …]\n\nOne way to decide which context word is the real context word is by taking the dot product of each with monster. If we’ve got good numbers in our vectors, the real context word should have a larger dot product.\n\\[\n\\text{monster} \\cdot \\text{miserable} > \\text{monster} \\cdot \\text{waddle}\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nI haven’t said yet how we get good numbers into our vectors to make this process possible! If it seems like we’ve skipped a step, it’s because we have, just so that we can talk about the process conceptually.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis kind of prediction task is also called a “skip-gram model.” If we flipped it around and took context words and tried to predict whether or not a target word appeared in the context, that would be called a “Continuous Bag of Words” model, or CBOW.\n\n\n\n\nDot products to probabilities\nThe dot product of the target word vector and the possible context word vectors will just be some number. For illustration, I’m going to make some up.\n\\[\n\\text{monster} \\cdot\\text{miserable} = 0.2\n\\]\n\\[\n\\text{monster}\\cdot\\text{waddle} = -0.5\n\\]\nSince the task we’re doing is a prediction task, we need to convert these numbers into probabilities. I’m going to the trouble of defining the math for how to do this, rather than just leaving it as the “the largest one wins” for a few reasons:\n\nRe-expressing these dot product numbers as probabilities is actually crucial for the training process of getting “good numbers” into our dot products.\nPeople talk about this process a lot, so it’s useful to know what they’re talking about.\n\nThe mathematical expression for squeezing an arbitrary vector of numbers, with values that can range between -\\(\\infty\\) and \\(\\infty\\), into a vector of probabilities ranging between 0 and 1 is:\n\\[\n\\frac{\\exp(x_i)}{\\sum_{i=1}^n\\exp(x_i)}\n\\]\nThis formula has a bunch of different names. In fact, several subfields of linguistics use this formula, and I don’t think they all realize they’re talking about the same thing. Some things this formula can be called:\n\nWhen there’s just 2 values being converted:\n\n“Sigmoid”, often abbreviated as \\(\\sigma\\) or \\(\\sigma(x_i)\\)\nthe “logistic transform”, which is the math at the heart of “logistic regression” aka “Varbrul”\n\nWhen there are multiple values being converted:\n\n“Softmax”, also sometimes abbreviated as \\(\\sigma\\) or \\(\\sigma(x_i)\\)\nIt’s the formula at the heart of “multinomial regression”\nIt’s also the formula for Maximum Entropy models, a.k.a. “MaxEnt Grammars” (Hayes and Wilson 2008)\n\n\nHere’s how to write it out as a python function, using numpy.\n\nimport numpy as np\n\ndef sigmoid(arr):\n  \"\"\"\n    return the sigmoid, or logit, or softmax transform of an array of numbers\n  \"\"\"\n  out = np.exp(arr)/(np.exp(arr).sum())\n  return(out)\n\n\ndists = np.array([0.2, -0.5])\nprobs = sigmoid(dists)\nprint(probs)\n\n[0.66818777 0.33181223]\n\n\n\n\nThe results\nAs SLP point out, we actually need two vectors for each word. One for its use as the target word, and one for its behavior as a context word.\n\nmonster_t\nmonster_c\n\nApparently some approaches will just add together these two vectors elementwise, and others will throw away the context vectors.\nEither way, assuming we’ve gotten some good numbers into these vectors, let’s think about what has been encoded into them. Let’s take two word that are probably very interchangeable in Frankenstein, and two words that probably aren’t.\n\ninterchangeable: monster and dæmon\nnot interchangeable: monster and called\n\nIf monster and dæmon are very interchangeable in the text, that means they should have many of the same words in their contexts. That means that for some context word vector, \\(c\\), we should expect \\(\\text{monster}\\cdot c\\) to have a roughly similar result as \\(\\text{dæmon} \\cdot c\\), since they’re going to give roughly the same probability to \\(c\\) being in their context. The only way for that to really happen is if the values in both of their vectors are also very similar to each other.\nOn the other hand, monster and called aren’t very interchangeable, meaning they don’t have many shared words in their contexts. So for some context word vector \\(c\\), the dot products \\(\\text{monster}\\cdot c\\) and \\(\\text{called} \\cdot c\\) should be pretty different, since these two words should make different predictions for the words that appear in their context. And the only way for that to happen is for the values in their vectors to be pretty different from each other.\nSo, what winds up happening is the word vectors for monster, dæmon and called encode information about the words they appear around, and with that information we can also calculate the cosine similarities between each of them. In that sense, it’s really similar to the term-context counts we were working with before, but now with much smaller vectors (300 numbers instead thousands) and without a lot of zeros."
  },
  {
    "objectID": "lectures/word_vectors/03_word2vec.html#how-its-done",
    "href": "lectures/word_vectors/03_word2vec.html#how-its-done",
    "title": "word2vec",
    "section": "How it’s done",
    "text": "How it’s done\nThe way we get good numbers into our word vectors is by starting out with random numbers. Let’s say we started out with this window of words around monster\n['wretch', 'the', 'miserable', 'monster', 'whom', 'i', 'had']\nAnd we told the prediction model to figure out which of the following words were in the context:\n\nwretch\ngreen\nlurched\n\nThe model is actually initialized with random values for the target word vector \\(\\text{monster}_t\\) and the context vectors \\(\\text{wretch}_c\\), \\(\\text{green} _c\\) and \\(\\text{lurched}_c\\). So even though we were hoping for a vector like \\([1, 0, 0]\\) for these three words (1 meaning “is a context word” and 0 meaning “is not a context word”), we might get back a vector (after doing dot-products and the softmax transformation) like \\([0.25, 0.30, 0.45]\\).\nSo, that’s a bad result, but the good news is we can calculate exactly how bad with a “loss function.” We already talked a little bit about one kind of loss function, the Mean Squared Error. For a categorical decision like this, the loss function is called “Cross Entropy” and is calculate like so:\n\nTake the log of the predicted probabilities,\nMultiply them elementwise by the expected outcomes (zeros and a 1)\nSum them all together\nmultiply by -1\n\nHere’s how that goes for the illustrative numbers:\nExpected:          [  1,     0,     0    ]\n    x\nlog(Prediction:)   [ -1.39, -1.20, -0.79 ]\n    =\n                   [ -1.39,  0,     0    ]\n                             SUM\n                             -1.39\n                             NEGATIVE\n                             1.39\nAnother way we can express elementwise multiplication and summing would be as a dot product. So, where \\(L_{CE}\\) stands for “Cross Entropy Loss”, \\(y\\) is the expected outputs, and \\(p\\) is the probabilities from the model\n\\[\nL_{CE} = -(y\\cdot \\log(p))\n\\]\nThe way the Cross Entropy Loss relates to probabilities can be visualized like this:\n\n\n\n\n\nWhile being able to quantify how bad the prediction was might not seem all that useful in itself, once you have a simple function like this, you can also calculate how much and in which direction you should change the values in your vectors. The math behind that process is a bit beyond the scope of this course, but the process is called “Gradient Decent”, and when used in training neural networks is called “backpropagation.”"
  },
  {
    "objectID": "lectures/word_vectors/03_word2vec.html#what-we-can-do-with-it.",
    "href": "lectures/word_vectors/03_word2vec.html#what-we-can-do-with-it.",
    "title": "word2vec",
    "section": "What we can do with it.",
    "text": "What we can do with it.\nOne thing we can be sure of is that we can’t look at the numbers in these word vectors and find some column that corresponds to a semantic feature we understand. But we can find some patterns in them that seem to correspond to the word’s meaning.\nFor example, let’s download a pre-trained word vector model1 using the gensim package.\n\nimport gensim.downloader as api\nwv = api.load('glove-wiki-gigaword-100')\n\nWe can get the word vectors for any word that appears in the model’s vocabulary.\n\nprint(wv[\"linguistics\"])\n\n[ 0.40662   0.459     0.080326  0.64767   0.63911  -0.28373   0.26237\n -0.9772   -0.20358   0.54826  -1.1537   -0.50849   0.27668   0.83333\n  0.11316   0.52335  -0.17736   0.19002  -0.17361  -0.23783  -0.63136\n  0.45049   0.60977   0.1982    0.22974  -0.68825   0.25491  -0.933\n -0.17222  -0.55944  -1.4325    0.32381  -1.0433   -0.27245  -0.66088\n  0.035583 -0.72584   0.3761   -0.23195   0.050886 -0.86028  -0.43717\n -0.8794   -0.037928 -0.69986   0.10677   0.90005   0.26923  -0.36821\n  0.98975   0.84426  -0.025086  0.47444  -0.31434  -0.31343   0.34649\n -0.043019 -1.0718    0.14563   0.45778   0.069034  0.49238  -0.39944\n -0.664     0.44024  -0.055936  0.16554   0.1716    0.81233   1.0646\n -0.38757   1.3677    0.56161  -0.27431  -0.77989  -0.28478  -0.11712\n  0.88058  -0.33901  -0.15304  -0.78882  -0.46359  -0.56632  -0.03353\n -0.79085   0.97591  -0.005715 -0.25515   0.060361 -0.48979   0.11511\n -0.12553   0.10717   0.58306   1.065     0.015627 -0.31298  -0.69456\n  0.059494  0.51897 ]\n\n\nAnd we can find words in the model that are most similar to a key word.\nfrom tabulate import tabulate\nprint(tabulate(wv.most_similar(\"linguistics\")))\n\n\n\nanthropology\n0.819153\n\n\nsociology\n0.763624\n\n\nphilology\n0.758713\n\n\nmathematics\n0.711027\n\n\ncomparative\n0.700243\n\n\nbiology\n0.698893\n\n\nzoology\n0.698872\n\n\npsychology\n0.695841\n\n\nneuroscience\n0.688815\n\n\nphilosophy\n0.661165\n\n\n\nWhat people have also found is that you can do analogies with word vectors. To demonstrate, let’s grab the word vectors for u.k., its capital, london, and france.\n\nuk_vec = wv[\"u.k.\"]\nlondon_vec = wv[\"london\"]\nfrance_vec = wv[\"france\"]\n\nWe haven’t pulled out the vector for France’s capital, france, but if word meaning has been successfully encoded in these vectors, then the angle (in the huge 100 dimensional space) between london and u.k. should be about the same as it is between france and its capital. Another way to think about it would be to say if we subtracted the u.k. meaning from london, we’d be left with a vector that encodes something about “capitalness” in it. And if we added that to france, we might wind up with a vector that points to a location close to France’s capital!\ncapitalness_vec = london_vec - uk_vec\nfrance_capital = france_vec + capitalness_vec\nprint(tabulate(wv.most_similar(france_capital)))\n\n\n\nparis\n0.849697\n\n\nfrance\n0.791088\n\n\nlondon\n0.763334\n\n\nprohertrib\n0.725074\n\n\nrome\n0.677958\n\n\nbritain\n0.669977\n\n\nfrench\n0.656768\n\n\nlyon\n0.634014\n\n\ntook\n0.629788\n\n\nlater\n0.623878"
  },
  {
    "objectID": "lectures/word_vectors/03_word2vec.html#visualizing",
    "href": "lectures/word_vectors/03_word2vec.html#visualizing",
    "title": "word2vec",
    "section": "Visualizing",
    "text": "Visualizing\nIn order to visualize data from the word vectors, we need to do some “dimensionality reduction” (a topic for another lecture). Bit here’s some basic code to do it.\nFirst, I want to populate a list of words that are all kind of related to each other, which this function ought to do.\n\n\n\n\n\n\nNote\n\n\n\nThis is a “recursive” function. The argument size controlls how many similar words you pull out, and depth controls how many times you try to pull out similar words to those vectors. If you want to figure out how many iterations the function could be going through it’s size ** depth or \\(\\text{size}^\\text{depth}\\). So be careful when setting your depth argument too high!\nThe vocabulary that eventually gets returned won’t be as large as size ** depth, though, since it’s only going to include words once, and words in similar areas of the vector space will probably appear in each other’s searches.\n\n\n\ndef get_network(wv, words, vocab = set(), size = 10, depth = 2):\n  \"\"\"\n    from a list of seed words, get a network of words\n  \"\"\"\n  vocab = vocab.union(set(words))\n  if depth == 0:\n    return(vocab)\n  else:\n    for w in words:\n      sims = wv.most_similar(w, topn = size)\n      sim_word = [s[0] for s in sims if not s in vocab]\n      vocab = vocab.union(set(sim_word))\n      vocab = get_network(wv, sim_word, vocab = vocab, size = size, depth = depth-1)\n  return(vocab)\n\nI’ll grab a bunch of words with the starting point of “linguistics.”\n\nling_similar = get_network(wv, \n                            words = [\"linguistics\"], \n                            vocab = set(), \n                            size = 10, \n                            depth = 4)\n\nAfter grabbing this set of words, I’ll get their word vectors, and convert them to an array.\n\nlingsim_v = [wv[word] for word in ling_similar]\nlingsim_v = np.array(lingsim_v)\n\nAt this point, I have to apologize for not knowing how to make graphs in python, and I don’t want to confuse matters by starting to write R code. What you see here is my best attempts after googling, and making graphs with plotly with just the default settings.\n\n#importing plotly\nimport plotly.express as px\n\nOne way we can plot the big matrix of values we have is as a “heatmap” or “image”. In this figure, the x axis represents each word, the y axis represents the positions in the word vectors, and the colors represent the values in the word vectors.\n\nfig = px.imshow(lingsim_v.T)\nfig.show()\n\n\n                        \n                                            \n\n\nWhile that is pretty, it’s not all that informative about what information is represented in the data. And we can’t really make a 100 dimensional plot either. Instead, we need to do boil down these 100 dimensions into 3 or 3 that we can plot. Principle Components Analysis (PCA) is one method that will work, but a popular method for word embeddings and neural networks specifically is t-SNE.2 Here’s how to do t-SNE on our “linguistics” matrix:\n\nfrom sklearn.manifold import TSNE\n\n\n# the `perplexity` argument affects how \"clumpy\"\n# the result is. Larger numbers have less clumpyness\ntsne = TSNE(n_components=2, perplexity = 5)\nprojections = tsne.fit_transform(lingsim_v)\n\n\nfig = px.scatter(\n    projections, \n    # 0 and 1 refer to the columns of\n    # projections\n    x=0, y=1,\n    # I wanted just text, no points\n    # so I created an array of 0s the same length\n    # as the number of words\n    size = np.zeros(len(ling_similar)),\n    # plot the text of each word.\n    text = list(ling_similar),\n    width=800, height=800\n)\nfig.show()"
  },
  {
    "objectID": "lectures/word_vectors/03_word2vec.html#training-our-own-word2vec-models",
    "href": "lectures/word_vectors/03_word2vec.html#training-our-own-word2vec-models",
    "title": "word2vec",
    "section": "Training our own word2vec models",
    "text": "Training our own word2vec models\nWe can train our own word2vec models using gensim. We need to feed it a list of lists. Something like:\n[['i', 'spoke', 'to', 'the', 'monster'],\n ['he', 'looked', 'at', 'me', 'strangely']]\nTo get this list of list data, I’ll import some of our good tokenizer friends from nltk, and also this time remove stopwords from the data.\n\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import sent_tokenize, RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom pprint import pprint\n\n\nwith open (\"gen/books/shelley/84.txt\", 'r') as f:\n  text = f.read()\n  \ntext = text.replace(\"\\n\", \" \").lower()\nsentences = sent_tokenize(text)\nsent_word = [RegexpTokenizer(r\"\\w+\").tokenize(sent) for sent in sentences]\ncorpus = [[w for w in sent if not w in stopwords.words('english')] for sent in sent_word]\npprint(corpus[10:12])\n\n[['productions',\n  'features',\n  'may',\n  'without',\n  'example',\n  'phenomena',\n  'heavenly',\n  'bodies',\n  'undoubtedly',\n  'undiscovered',\n  'solitudes'],\n ['may', 'expected', 'country', 'eternal', 'light']]\n\n\nI’ll set a relatively word vector size (100) and pretty narrow window (2 words to each side) and train for a good number of epochs (100).\n\nmodel = Word2Vec(sentences = corpus, \n                 vector_size = 100, \n                 window = 2,\n                 epochs = 100)\n\nHere’s a table of the most similar words to “monster” in the novel.\nprint(\n  tabulate(\n    model.wv.most_similar(\"monster\")\n  )\n)\n\n\n\nhellish\n0.56288\n\n\nfingers\n0.526299\n\n\ngigantic\n0.488169\n\n\nlifeless\n0.479828\n\n\nfilthy\n0.474239\n\n\nwickedness\n0.471034\n\n\nshuddered\n0.460071\n\n\nface\n0.45477\n\n\nrage\n0.453865\n\n\nimagined\n0.449275\n\n\n\nThis model has a lot fewer words in it than the big GloVE model we downloaded above, so we can get all of the word vectors as a matrix and plot the t-SNE of the whole book.\n\nfrank_matrix = model.wv.get_normed_vectors()\n\n\nfrank_matrix.shape\n\n(1719, 100)\n\n\n\ntsne = TSNE(n_components=2, perplexity = 5)\nprojections = tsne.fit_transform(frank_matrix)\n\n\nvocab = model.wv.index_to_key\n\n\nfig = px.scatter(\n    projections, \n    x=0, y=1,\n    hover_name = np.array(vocab),\n    width=800, \n    height=800\n)\nfig.show()"
  },
  {
    "objectID": "lectures/word_vectors/index.html",
    "href": "lectures/word_vectors/index.html",
    "title": "Word Vectors",
    "section": "",
    "text": "There’s a few different components we can start understanding about “word vectors.”\n\nThe concept of representing words with numeric “vectors”\nA little bit of linear algebra\nA little (really just a sprinkling) of geometry & trig\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nModified\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n2022-10-26\n\n\n2022-10-29\n\n\nWord Vectors - Concepts\n\n\nword vectors\n\n\n\n\n2022-11-01\n\n\n2022-11-07\n\n\nTerm-Document and Term-Context matrices\n\n\nword vectors\n\n\n\n\n2022-11-07\n\n\n2022-11-17\n\n\nword2vec\n\n\nword vectors\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures/word_vectors/01_concept.html#features",
    "href": "lectures/word_vectors/01_concept.html#features",
    "title": "Word Vectors - Concepts",
    "section": "Features",
    "text": "Features\n\nCategorical Features\nIn linguistics, you’ve probably already encountered our tendency to categorize things using a bunch of features. For example, in Phonology we often categorize phonemes according to “distinctive features.”\n\n\n\n\nvoice\ncontinuant\nLAB\nCOR\nDORS\nanterior\n\n\n\n\np\n➖\n➖\n➕\n➖\n➖\n➖\n\n\nt\n➖\n➖\n➖\n➕\n➖\n➖\n\n\nk\n➖\n➖\n➖\n➖\n➕\n➖\n\n\nb\n➕\n➖\n➕\n➖\n➖\n➖\n\n\nd\n➕\n➖\n➖\n➕\n➖\n➖\n\n\ng\n➕\n➖\n➖\n➖\n➕\n➖\n\n\nf\n➖\n➕\n➕\n➖\n➖\n➖\n\n\ns\n➖\n➕\n➖\n➕\n➖\n➖\n\n\nʃ\n➖\n➕\n➖\n➕\n➖\n➕\n\n\n\nMore relevant to the topic of word vectors, we could do the same with semantic features and words\n\n\n\n\ndomesticated\nfeline\n\n\n\n\ncat\n➕\n➕\n\n\npuma\n➖\n➕\n\n\ndog\n➕\n➖\n\n\nwolf\n➖\n➖\n\n\n\n\n\nNumeric Features\nInstead of using categorical values for these features, let’s use a numeric score. These represent my own subjective scores for these animals.\n\n\n\n\ndomesticated\nfeline\n\n\n\n\ncat\n70\n100\n\n\npuma\n0\n90\n\n\ndog\n90\n30\n\n\nwolf\n10\n10\n\n\n\nThe sequence of numbers associated with “cat”, [70, 100], we’ll call a “vector”. A lot of the work we’re going to do with vectors can be understood if we start with two dimensional vectors like this. We can plot each animal as a point in the [domesticated, feline] “vector space”.\n\n\n\n\n\n\n\n\nFigure 1: four animals plotted in the domesticated/feline space"
  },
  {
    "objectID": "lectures/word_vectors/01_concept.html#vectors",
    "href": "lectures/word_vectors/01_concept.html#vectors",
    "title": "Word Vectors - Concepts",
    "section": "Vectors",
    "text": "Vectors\nThe word “vector” might conjure up ideas of “direction” for you, as it should! The way we really want to think about vectors when we’re doing word vectors is like a line with an arrow at the end, pointing at the location in “vector space” where each animal is.\n\n\n\n\n\nFigure 2: four animals plotted in the domesticated/feline space\n\n\n\n\n\nThe “magnitude” of a vector.\nWe’re going to have to calculate the magnitudes of vectors, so let’s start with calculating the magnitude of the “dog” vector.\n\n\n\n\n\nFigure 3: dog plotted in the domesticated/feline space\n\n\n\n\nThe magnitude of “dog” in this vector space is the length of the line that reaches from the [0,0] point to the location of “dog”. The mathematical notation we’d use to indicate “the length of the dog vector” is \\(|\\text{dog}|\\). We can work out the distance of the vector by thinking about it like a right triangle.\n\n\n\n\n\nFigure 4: dog plotted in the domesticated/feline space\n\n\n\n\nLooking at the dog vector this way, we can use the Pythagorean Theorem to get its length\n\\[\n|\\text{dog}|^2 = \\text{domesticated}^2 + \\text{feline}^2\n\\]\n\\[\n|\\text{dog}| = \\sqrt{\\text{domesticated}^2 + \\text{feline}^2}\n\\]\nI won’t go through the actual numbers here, but it turns out the magnitude of dog is 94.87.\n\nimport numpy as np\n\ndog = np.array([90, 30])\ndog_mag1 = np.sqrt(sum([x**2 for x in dog]))\n\n#or\n\ndog_mag2 = np.linalg.norm(dog)\n\nprint(f\"{dog_mag1:.2f} or {dog_mag2:.2f}\")\n\n94.87 or 94.87\n\n\n\nIn General\nThe way things worked out for dog is how we’d calculate the magnitude of any vector of any dimensionality. You square each value, sum them up, then take the square root of that sum.\n\\[\n|v|^2 = v_1^2 + v_2^2 + v_3^2  + \\dots +v_i^2\n\\]\n\\[\n|v|^2 = \\sum_{i = 1}^nv_i^2\n\\]\n\\[\n|v| = \\sqrt{\\sum_{i = 1}^nv_i^2}\n\\]\n\n\n\nComparing Vectors\nNow, let’s compare the vectors for “cat” and “dog” in this “vector space”\n\n\n\n\n\nFigure 5: dog and cat plotted in the domesticated/feline space\n\n\n\n\nHow should we compare the closeness of these two vectors in the vector space? What’s most common is to estimate the angle between the two, usually notated with \\(\\theta\\), or more specifially, to get the cosine of the angle, \\(\\cos\\theta\\).\n\n\n\n\n\n\n\n\nFigure 6: dog and cat plotted in the domesticated/feline space\n\n\n\n\nWhere did cosine come in?? This is a bit of a throwback to trigonometry, again being related to formulas for estimating angles of triangles.\nThe specific formula to get \\(\\cos\\theta\\) for dog and cat involves a “dot product”, which for dog and cat in particular goes like this \\[\n\\text{dog}\\cdot \\text{cat} = (\\text{dog}_{\\text{domesticated}}\\times\\text{cat}_{\\text{domesticated}}) +  (\\text{dog}_{\\text{feline}}\\times\\text{cat}_{\\text{feline}})\n\\]\n\ndog = np.array([90, 30])\ncat = np.array([70, 100])\n\ndot1 = sum([x * y for x,y in zip(dog, cat)])\n\n# or!\n\ndot2 = np.dot(dog, cat)\n\nprint(f\"{dot1} or {dot2}\")\n\n9300 or 9300\n\n\nIn general, the dot product of any two vectors will be\n\\[\na\\cdot b = a_1b_1 + a_2b_2 + a_3b_3 +\\dots a_ib_i\n\\]\n\\[\na\\cdot b= \\sum_{i=1}^n a_ib_i\n\\]\nOne way to think of the dot product here is if two vectors have very similar values along many dimensions, their dot product will be large. On the other hand, if they’re very different, and one had a lot of zeros where the other has large values, the dot product will be small.\nThe full formula for \\(\\cos\\theta\\) normalizes the dot product by dividing it by the product the magnitude of dog and cat.\n\\[\n\\cos\\theta = \\frac{\\text{dog}\\cdot\\text{cat}}{|\\text{dog}||\\text{cat}|}\n\\]\nThe reason why we’re dividing like this is because if the two vectors had the same direction, their dot product would equal multiplying their magnitudes.\n\n\n\n\n\nFigure 7: two vectors with the same \\(\\theta\\)\n\n\n\n\n\nbig_dog = np.array([90, 30])\nsmall_dog = np.array([30, 10])\n\nbig_dog_mag = np.linalg.norm(big_dog)\nsmall_dog_mag = np.linalg.norm(small_dog)\nprint(f\"Product of magnitudes is {(big_dog_mag * small_dog_mag):.0f}\")\n\nProduct of magnitudes is 3000\n\nbig_small_dot = np.dot(big_dog, small_dog)\nprint(f\"Dot produtct of vectors is {big_small_dot}\")\n\nDot produtct of vectors is 3000\n\n\nNormalizing like this means that \\(\\cos\\theta\\) is always going to be some number between -1 and 1, and for the vectors we’re going to be looking at, usually between 0 and 1.\nFor the actual case of dog and cat\n\ndog = np.array([90, 30])\ncat = np.array([70, 100])\n\ndog_dot_cat = np.dot(dog, cat)\ndog_mag = np.linalg.norm(dog)\ncat_mag = np.linalg.norm(cat)\n\ncat_dog_cos = dog_dot_cat / (dog_mag * cat_mag)\n\nprint(f\"The cosine similarity of dog and cat is {cat_dog_cos:.3f}\")\n\nThe cosine similarity of dog and cat is 0.803\n\n\nor\n\nfrom scipy import spatial\ncat_dog_cos2 = 1 - spatial.distance.cosine(dog, cat)\nprint(f\"The cosine similarity of dog and cat is {cat_dog_cos2:.3f}\")\n\nThe cosine similarity of dog and cat is 0.803\n\n\n\n\nWith more dimensions\nThe basic principles remain the same even if we start including even more dimensions. For example, let’s say we added size to the set of features for each animal.\n\n\n\n\n\n\n\n\nanimal\ndomesticated\nfeline\nsize\n\n\n\n\ncat\n70\n100\n10\n\n\npuma\n0\n90\n90\n\n\ndog\n90\n30\n30\n\n\nwolf\n10\n10\n60\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can do all the same things we did before, with the same math.\n\ndog = np.array([90, 30, 30])\ncat = np.array([70, 100, 10])\n\ndog_mag = np.linalg.norm(dog)\ncat_mag = np.linalg.norm(cat)\n\nprint(f\"dog magnitude: {dog_mag:.2f}, cat magnitude: {cat_mag:.3f}\")\n\ndog magnitude: 99.50, cat magnitude: 122.474\n\ndog_cat_cos = np.dot(dog, cat)/(dog_mag * cat_mag)\nprint(f\"dog and cat cosine similarity: {dog_cat_cos:.2f}\")\n\ndog and cat cosine similarity: 0.79"
  },
  {
    "objectID": "lectures/word_vectors/01_concept.html#what-does-this-have-to-do-with-nlp",
    "href": "lectures/word_vectors/01_concept.html#what-does-this-have-to-do-with-nlp",
    "title": "Word Vectors - Concepts",
    "section": "What does this have to do with NLP?",
    "text": "What does this have to do with NLP?\nBefore we get to word, vectors, we can start talking about “document” vectors. I’ve collapsed the next few code blocks for downloading a bunch of books by Mary Shelley and Jules Verne so we can focus on the “vectors” part.\n\n\na get book function\nimport gutenbergpy.textget\ndef getbook(book, outfile):\n  \"\"\"\n  Download a book from project Gutenberg and save it \n  to the specified outfile\n  \"\"\"\n  print(f\"Downloading Project Gutenberg ID {book}\")\n  raw_book = gutenbergpy.textget.get_text_by_id(book)\n  clean_book = gutenbergpy.textget.strip_headers(raw_book)\n  if not outfile:\n    outfile = f'{book}.txt'\n    print(f\"Saving book as {outfile}\")\n  with open(outfile, 'wb') as file:\n    file.write(clean_book)\n    file.close()\n\n\n\n\nProject Gutenberg information\nmary_shelley_ids = [84, 15238, 18247, 64329]\nmary_shelley_files =  [f\"gen/books/shelley/{x}.txt\" for x in mary_shelley_ids]\nmary_shelley_titles = [\"Frankenstein\", \"Mathilda\", \"The Last Man\", \"Falkner\"]\njules_verne_ids = [103, 164, 1268, 18857]\njules_verne_files = [f\"gen/books/verne/{x}.txt\" for x in jules_verne_ids]\njules_verne_titles = [\"80days\", \"ThousandLeagues\", \"MysteriousIsland\", \"CenterOfTheEarth\"]\n\n\n\nfoo = [getbook(x, f\"gen/books/shelley/{x}.txt\") for x in mary_shelley_ids]\nfoo = [getbook(x, f\"gen/books/verne/{x}.txt\") for x in jules_verne_ids]\n\nWe’re going to very quickly tokenize these books into words, and then get just unigram counts for each book\n\n\n\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\n\n\ndef get_unigram_counts(path):\n  \"\"\"\n    Given a path, generate a counter dictionary of unigrams\n  \"\"\"\n  with open(path, 'r') as f:\n    text = f.read()\n  text = text.replace(\"\\n\", \" \").lower()\n  unigrams = RegexpTokenizer(r\"\\w+\").tokenize(text)\n  count = Counter(unigrams)\n  return(count)\n\n\nshelley_words = {k:get_unigram_counts(v) \n                    for k, v in zip(mary_shelley_titles, mary_shelley_files)}\nverne_words = {k:get_unigram_counts(v) \n                    for k, v in zip(jules_verne_titles, jules_verne_files)}\n\nSo now, shelley_words is a dictionary with keys for each book:\n\nshelley_words.keys()\n\ndict_keys(['Frankenstein', 'Mathilda', 'The Last Man', 'Falkner'])\n\n\nAnd the value associated with each key is the unigram count of word in that book:\n\nshelley_words[\"Frankenstein\"].most_common(10)\n\n[('the', 4195), ('and', 2976), ('i', 2846), ('of', 2642), ('to', 2089), ('my', 1776), ('a', 1391), ('in', 1128), ('was', 1021), ('that', 1018)]\n\n\n\nBooks in word count vector space\nBefore were were classifying animals in the “feline” and “domesticated” vector space. What if we classified These book by Mary Shelley and Jules Verne in the “monster” and “sea” vector space. We’ll just compare them in terms of how many times the word “monster” and “sea” appeared in each of their books.\n\ndef get_term_count(book_dict, term):\n  \"\"\"\n    return a list of the number of times a term has appeared\n    in a book\n  \"\"\"\n  out = [book_dict[book][term] for book in book_dict]\n  return(out)\n\n\n\n\n\nmonster = [\"monster\"] + \\\n          get_term_count(shelley_words, \"monster\") + \\\n          get_term_count(verne_words, \"monster\")\nsea  = [\"sea\"] + \\\n          get_term_count(shelley_words, \"sea\") + \\\n          get_term_count(verne_words, \"sea\")\n          \n\n\n\n\nbook\nmonster\nsea\n\n\n\n\nFrankenstein\n31\n34\n\n\nMathilda\n3\n20\n\n\nThe Last Man\n2\n118\n\n\nFalkner\n2\n31\n\n\n80days\n0\n52\n\n\nThousandLeagues\n44\n357\n\n\nMysteriousIsland\n8\n277\n\n\nCenterOfTheEarth\n19\n122\n\n\n\nSo, in the “monster”, “sea” vector space, we’d say Frankenstein has a vector of [31, 31], and Around the World in 80 Days has a vector of [0, 52]. We can make a vector plot for these books in much the same way we did for the animals in the “feline” and “domesticated” vector space.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Selected Mary Shelley and Jules Verne novels in the monster,sea vector space\n\n\n\n\nWe can also do all of the same vector computations we did before. In Figure 8, Frankenstein and Around the World in 80 Days seem to have the largest angle between them. Let’s calculate it!\n\nfrank = [shelley_words[\"Frankenstein\"][word] for word in [\"monster\", \"sea\"]]\neighty = [verne_words[\"80days\"][word] for word in [\"monster\", \"sea\"]]\n\n\nprint(frank)\n\n[31, 34]\n\nprint(eighty)\n\n[0, 52]\n\n\nLet’s get their cosine similarity the fast way with scipy.spatial.distance.cosine()\n\n# scipy already imported\n1 - spatial.distance.cosine(frank, eighty)\n\n0.7389558439133294\n\n\nThe novels Mathilda and Journey to the Center of the Earth, on the other hand, look like they have almost identical angles.\n\nmathilda = [shelley_words[\"Mathilda\"][word] for word in [\"monster\", \"sea\"]]\ncenter = [verne_words[\"CenterOfTheEarth\"][word] for word in [\"monster\", \"sea\"]]\n\n\nprint(mathilda)\n\n[3, 20]\n\nprint(center)\n\n[19, 122]\n\n\n\n1 - spatial.distance.cosine(mathilda, center)\n\n0.9999842826707133\n\n\nHere’s table of every novel’s cosine similarity from the other in “monster”, “sea” vector space,\n\n\n\n\n\n\n\nFankenstein\nMathilda\nEightyDay\nCenterOfTheEarth\n\n\n\n\nFankenstein\n1\n0.83\n0.74\n0.83\n\n\nMathilda\n\n1\n0.99\n1\n\n\nEightyDay\n\n\n1\n0.99\n\n\nCenterOfTheEarth\n\n\n\n1\n\n\n\n\n\nThe full vector space\nOf course, we are not limited to calculating cosine similarity on just two dimensions. We could use the whole shared vocabulary between two novels to compute the cosine similarity.\n\nshared_vocab = set(list(shelley_words[\"Frankenstein\"].keys()) + \n                   list(verne_words[\"80days\"].keys()))\nprint(f\"Total dimensions: {len(shared_vocab)}\")\n\nTotal dimensions: 10544\n\nfrankenstein_vector = [shelley_words[\"Frankenstein\"][v] for v in shared_vocab]\neighty_vector = [verne_words[\"80days\"][v] for v in shared_vocab]\n\n1 - spatial.distance.cosine(frankenstein_vector, eighty_vector)\n\n0.8759771803390622\n\n\n\n\nAuthor Identification?\n\nmystery1 = getbook(6447, outfile=\"gen/books/shelley/6447.txt\")\n\nDownloading Project Gutenberg ID 6447\n\n\n\nmystery = get_unigram_counts(\"gen/books/shelley/6447.txt\")\n\n\ndef get_dist(book1, book2):\n  \"\"\"\n    given unigram counts from two books\n    return the cosine distance\n  \"\"\"\n  shared_vocab = set(list(book1.keys()) + list(book2.keys()))\n  \n  book1_vec = [book1[v] for v in shared_vocab]\n  book2_vec = [book2[v] for v in shared_vocab]\n  \n  sim = 1-spatial.distance.cosine(book1_vec, book2_vec)\n  return(sim)\n\n\nshelley_dist = [get_dist(shelley_words[book], mystery) for book in shelley_words]\n\n\nnp.mean(shelley_dist)\n\n0.9314257175598841\n\n\n\nverne_dist = [get_dist(verne_words[book], mystery) for book in verne_words]\n\n\nnp.mean(verne_dist)\n\n0.9376880437662538"
  },
  {
    "objectID": "lectures/word_vectors/02_vectors_examples.html#term-document-matrices",
    "href": "lectures/word_vectors/02_vectors_examples.html#term-document-matrices",
    "title": "Term-Document and Term-Context matrices",
    "section": "Term-Document Matrices",
    "text": "Term-Document Matrices\nLast time, we started looking at limited view of Term-Document matrices. Here we’ve got books in the rows, and the words monster and sea as the dimensions of the vector space.\n\n\nCode to download and process books from Project Gutenberg.\nimport gutenbergpy.textget\nfrom nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\nfrom tabulate import tabulate\nfrom os.path import exists\n\ndef getbook(book, outfile):\n  \"\"\"\n  Download a book from project Gutenberg and save it \n  to the specified outfile\n  \"\"\"\n  if exists(outfile):\n    pass\n  else:\n    print(f\"Downloading Project Gutenberg ID {book}\")\n    raw_book = gutenbergpy.textget.get_text_by_id(book)\n    clean_book = gutenbergpy.textget.strip_headers(raw_book)\n    if not outfile:\n      outfile = f'{book}.txt'\n      print(f\"Saving book as {outfile}\")\n    with open(outfile, 'wb') as file:\n      file.write(clean_book)\n      file.close()\n\ndef get_unigram_counts(path):\n  \"\"\"\n    Given a path, generate a counter dictionary of unigrams\n  \"\"\"\n  with open(path, 'r') as f:\n    text = f.read()\n  text = text.replace(\"\\n\", \" \").lower()\n  unigrams = RegexpTokenizer(r\"\\w+\").tokenize(text)\n  count = Counter(unigrams)\n  return(count)\n\ndef get_term_count(book_dict, term):\n  \"\"\"\n    return a list of the number of times a term has appeared\n    in a book\n  \"\"\"\n  out = [book_dict[book][term] for book in book_dict]\n  return(out)\n    \nmary_shelley_ids = [84, 15238, 18247, 64329]\nmary_shelley_files =  [f\"gen/books/shelley/{x}.txt\" for x in mary_shelley_ids]\nmary_shelley_titles = [\"Frankenstein\", \"Mathilda\", \"The Last Man\", \"Falkner\"]\njules_verne_ids = [103, 164, 1268, 18857]\njules_verne_files = [f\"gen/books/verne/{x}.txt\" for x in jules_verne_ids]\njules_verne_titles = [\"80days\", \"ThousandLeagues\", \"MysteriousIsland\", \"CenterOfTheEarth\"]\n\nfoo = [getbook(x, f\"gen/books/shelley/{x}.txt\") for x in mary_shelley_ids]\nfoo = [getbook(x, f\"gen/books/verne/{x}.txt\") for x in jules_verne_ids]\n\n\n\nshelley_words = {k:get_unigram_counts(v) \n                    for k, v in zip(mary_shelley_titles, mary_shelley_files)}\nverne_words = {k:get_unigram_counts(v) \n                    for k, v in zip(jules_verne_titles, jules_verne_files)}\n\nmonster = [\"monster\"] + \\\n          get_term_count(shelley_words, \"monster\") + \\\n          get_term_count(verne_words, \"monster\")\nsea  = [\"sea\"] + \\\n          get_term_count(shelley_words, \"sea\") + \\\n          get_term_count(verne_words, \"sea\")                 \n\ntranspose = list(zip(mary_shelley_titles+jules_verne_titles, monster[1:], sea[1:]))\nprint(tabulate(transpose, headers=[\"book\", \"monster\", \"sea\"]))\n\n\n\nTable 1: Books by Mary Shelley and Jules Verne in the ‘monster’, ‘sea’ vector space.\n\n\nbook\nmonster\nsea\n\n\n\n\nFrankenstein\n31\n34\n\n\nMathilda\n3\n20\n\n\nThe Last Man\n2\n118\n\n\nFalkner\n2\n31\n\n\n80days\n0\n52\n\n\nThousandLeagues\n44\n357\n\n\nMysteriousIsland\n8\n277\n\n\nCenterOfTheEarth\n19\n122\n\n\n\n\n\nI call this a “limited” term-document matrix, since we’re only looking at the frequency of two hand-picked word dimensions. If we’d chosen some other words to serve as the dimensions, some of them will have very high counts, and others will be mostly 0. For example, the appears very frequently in all books, and illustration doesn’t appear at all in most of the books.\n\n\nCode to generate the ‘the’, ‘illustration’ table.\nthe =  [verne_words[book][\"the\"] for book in verne_words] + \\\n                [shelley_words[book][\"the\"] for book in shelley_words]\nillustration = [verne_words[book][\"illustration\"] for book in verne_words] + \\\n                [shelley_words[book][\"illustration\"] for book in shelley_words]\ntitles = [book for book in verne_words] + [book for book in shelley_words]\n\nprint(\n  tabulate(\n    list(zip(titles, the, illustration)), \n    headers=[\"book\", \"the\", \"illustration\"],\n    intfmt=\",\"\n  )\n)\n          \n\n\n\nTable 2: Books by Mary Shelley and Jules Verne in the ‘the’, ‘illustration’ vector space.\n\n\nbook\nthe\nillustration\n\n\n\n\n80days\n4,715\n1\n\n\nThousandLeagues\n8,578\n12\n\n\nMysteriousIsland\n17,003\n0\n\n\nCenterOfTheEarth\n5,651\n4\n\n\nFrankenstein\n4,195\n0\n\n\nMathilda\n2,214\n0\n\n\nThe Last Man\n11,494\n0\n\n\nFalkner\n7,222\n0\n\n\n\n\n\nThere’ also nothing particularly special about any two words chosen words in each book. Ideally we’d be representing each book with the entire word vector.\n\nGetting the whole term-document matrix: python time\nRight now, I have Counter dictionaries for each book stored like this:\n# This is pseudocode\nauthor_words = {\n  book: {w1: c1,\n         w2: c2,\n         ...},\n  ...\n}\nTo get the complete term-document matrix, I’m going to have to:\n\ncombine the words from each dictionary into one big set\nget the count of each word in each dictionary.\n\nI’ll then convert the lists I get into one big numpy matrix.\n\n\n\n\n\n\nNote\n\n\n\nThere are some ways to make term-document matrices with the nltk or scikit-learn that don’t involve writing so much code, but they also don’t show how they work as explicitly as the code below. So for the purpose of teaching, I’m writing it all out long-hand.\n\n\n\nimport numpy as np\n\nFirst, I’ll get a list of the book titles, since this will be handy for making tables later on.\n\nbook_titles = [book for book in shelley_words] + [book for book in verne_words]\nbook_titles\n\n['Frankenstein', 'Mathilda', 'The Last Man', 'Falkner', '80days', 'ThousandLeagues', 'MysteriousIsland', 'CenterOfTheEarth']\n\n\nI need to get one big vocabulary that has just one entry per word that appears in all of the books. I’m using python sets to do this.\n\n# Start with empty set\nbig_vocab= set()\n\n# For every book in Shelley's works, \n# get the Union of `big_vocab` and that book's vocabulary.\nfor book in shelley_words:\n  this_vocab = set(shelley_words[book].keys())\n  big_vocab = big_vocab.union(this_vocab)\n  \n# Repeat for Jules Verne\nfor book in verne_words:\n  this_vocab = set(verne_words[book].keys())\n  big_vocab = big_vocab.union(this_vocab)\n  \n# Convert the set to a list so that we can index it normally\nbig_vocab = list(big_vocab)\n\n# Total vocab size:\nprint(f\"The total vocabulary size is {len(big_vocab):,} words\")\n\nThe total vocabulary size is 24,681 words\n\n\nHere, I create a list of each word’s frequency in each book, then convert it all to a numpy matrix.\n\nword_counts = []\nfor word in big_vocab:\n  document_vector = [shelley_words[book][word] for book in shelley_words] +\\\n                      [verne_words[book][word] for book in verne_words]\n  word_counts.append(document_vector)\n\nword_matrix = np.array(word_counts)\n\nLet’s double check what this matrix looks like:\n\nprint(word_matrix)\n\n[[ 0  0  0 ...  0  2  0]\n [ 0  0  1 ...  0  1  0]\n [ 0  1  1 ...  0  1  0]\n ...\n [ 0  2  0 ...  0  0  0]\n [92  3  0 ...  0  0  0]\n [ 7  1  0 ...  0  0  0]]\n\n\n\nword_matrix.shape\n\n(24681, 8)\n\n\nSo, there are 24,681 rows, and 8 columns in the matrix. So 1 row for each word, 1 column for each book. We can double check that the numbers look like we expect by getting the indices for specific words, and slicing the term-document matrix:\n\nexample_words = [\"the\", \"illustration\", \"monster\", \"sea\"]\nexample_idx = [big_vocab.index(w) for w in example_words]\n\n\nprint(word_matrix[example_idx, :])\n\n[[ 4195  2214 11494  7222  4715  8578 17003  5651]\n [    0     0     0     0     1    12     0     4]\n [   31     3     2     2     0    44     8    19]\n [   34    20   118    31    52   357   277   122]]\n\n\n\n\n\n\n\n\nSparse Matrix\n\n\n\nTerm-document matrices are almost always “sparse” matrices. “Sparse” meaning a lot of its values are 0. We can calculate how many cells of this matrix have counts greater than zero with some numpy tricks.\nFirst, we say word_matrix>0, it will give us back a matrix of the same size, with True where the expression is true and False where the expression is false.\n\nword_matrix>0\n\narray([[False, False, False, ..., False,  True, False],\n       [False, False,  True, ..., False,  True, False],\n       [False,  True,  True, ..., False,  True, False],\n       ...,\n       [False,  True, False, ..., False, False, False],\n       [ True,  True, False, ..., False, False, False],\n       [ True,  True, False, ..., False, False, False]])\n\n\nThe nifty thing is that we can treat a numpy array of True and False like a matrix of 1 and 0 values, where True gets converted to 1 and False gets converted to 0. If we just use np.mean() on this True/False matrix, we’ll just get the proportion of values that are greater than 0!\n\nnp.mean(word_matrix>0)\n\n0.3449515821887282\n\n\nOnly about 34% of all cells in the matrix have a count greater than 0! This is a matrix mostly of 0s.\n\n\n\n\nWhat’s an important word for each document (tf–idf)?\nWe could start comparing documents with the cosine similarity of their word counts. Here’s how we’d do it for Frankenstein (index 0) and Around the world in 80 Days (index 4).\n\nfrom scipy.spatial.distance import cosine\n1 - cosine(word_matrix[:,0], word_matrix[:, 4])\n\n0.8759771803390621\n\n\nLooks like they’re very similar! But then, they would. For most of the words they have in common, those words are going to have very large frequencies.\n\n\n\n\n\n\nFrankenstein\n\n\n\n\n\nthe\n4,195\n\n\nand\n2,976\n\n\ni\n2,846\n\n\nof\n2,642\n\n\nto\n2,089\n\n\nmy\n1,776\n\n\na\n1,391\n\n\nin\n1,128\n\n\nwas\n1,021\n\n\nthat\n1,018\n\n\n\n\n\n\n\n\n80days\n\n\n\n\n\nthe\n4,715\n\n\nand\n1,909\n\n\nof\n1,814\n\n\nto\n1,696\n\n\na\n1,330\n\n\nin\n1,056\n\n\nwas\n1,005\n\n\nhe\n989\n\n\nhis\n858\n\n\nfogg\n646\n\n\n\n\n\n\nWe want to treat frequent words in each document as important for characterizing that document, while at the same time not giving too much weight to words that are frequent in every document. In comes “tf–idf”.\n\nTf–idf\n“Tf–idf” stands for “term frequency-inverse document frequency”. Annoyingly, the “–” in its name is a hyphen, so we’re not doing subtraction.\n“Term frequency” is the frequency of each word within each document. It’s really just the word_matrix we’ve already made. Except we take the log-transform of the frequency.\nWe’ve looked at the log transform before, but just to remind you, it has the effect of squashing down the right side of a distribution, and stretching out the left side of a distribution.\n\n\n\n\n\n\n\n\n\n\nBut remember how most of the numbers in word_matrix are 0?\n\nnp.log10(0)\n\n-inf\n\n<string>:1: RuntimeWarning: divide by zero encountered in log10\n\n\nSo, what we do to fix this is add 1 to every value (yes, again) and take the log10 of that.\n\ntf = np.log10(word_matrix + 1)\n\nNext, for every word we get a count of how many documents it appeared in. So, “the” appeared in all 8 books, so it will have a document frequency of 8. “Illustration” only appeared in 3 books, so it will have a document frequency of 3.\nWe can use another handy feature of numpy, and tell it to sum across the columns (axis=1)\n\ndf = np.sum(word_matrix > 0, axis = 1)\ndf.shape\n\n(24681,)\n\n\n\ndf\n\narray([2, 2, 4, ..., 1, 3, 2])\n\n\nBut the measure we use is inverse document frequency. For that, we actually do \\(\\frac{N}{df}\\) where \\(N\\) is the total number of documents. And then, for good measure, we also take the log10 transform.\n\nidf = np.log10(8/df)\n\nTo get the tf-idf, we just multiply each book’s term frequency vector by the inverse document frequency vector.\n\ntf_idf = tf * idf[:, np.newaxis]\n\n\n\nThe upshot\nAfter all of this, we have a measure for each word within each book that balances out its frequency in this book and its appearance frequency across all books.\n\n\n\n\n\n\n\n\ntf\nidf\ntf-idf\n\n\n\n\nFrequent word in this book (large tf)\nAppears in most books (small idf)\nMediocre tf-idf\n\n\nInfrequent word in this book (small tf)\nAppears in most books (small idf)\nVery small tf-idf\n\n\nFrequent word in this book (large tf)\nAppears in very few books (large idf)\nLarge tf-idf\n\n\n\n\n\nThe Results\nLet’s explore these tf-idf values. First, we can get the indicies of the words in each book with the largest tf-idf values with .argmax(axis=0).\n\nlargest_tfidf = tf_idf.argmax(axis = 0)\nlargest_tfidf_words = [big_vocab[x] for x in largest_tfidf]\n\n\n\n\nFrankenstein\nclerval\n\n\nMathilda\nmathilda\n\n\nThe Last Man\nraymond\n\n\nFalkner\nfalkner\n\n\n80days\nfogg\n\n\nThousandLeagues\nnautilus\n\n\nMysteriousIsland\npencroft\n\n\nCenterOfTheEarth\nhans\n\n\n\nWe can get the indicies of the top 5 using .argsort() like this:\n\ntop_five = (tf_idf * -1).argsort(axis = 0)[0:5, :]\n\ntop_five_words = np.empty(shape = (5,8), dtype = 'object')\nfor i in range(top_five.shape[0]):\n  for j in range(top_five.shape[1]):\n    top_five_words[i,j] = big_vocab[top_five[i,j]]\n\n\n\nFrankenstein\nMathilda\nThe Last Man\nFalkner\n80days\nThousandLeagues\nMysteriousIsland\nCenterOfTheEarth\n\n\n\n\nclerval\nmathilda\nraymond\nfalkner\nfogg\nnautilus\npencroft\nhans\n\n\njustine\n_f\nadrian\nneville\npassepartout\nned\nharding\nsneffels\n\n\nsafie\nmathilda\nperdita\nosborne\nphileas\nconseil\nneb\nhardwigg\n\n\nagatha\nwoodville\nidris\nboyvill\naouda\naronnax\nspilett\nsaknussemm\n\n\ndæmon\na_\nevadne\nraby\ndetective\nnemo\nreporter\nicelandic\n\n\n\nWe can even calculate the cosine similarity of each book from every other book with these tf-idf vectors.\nfrom scipy.spatial.distance import cosine\ndists = np.empty(shape = (8,8))\nfor i in range(8):\n  for j in range(8):\n    dists[i,j] = 1-cosine(tf_idf[:, i], tf_idf[:, j])\nprint(tabulate(dists, headers=book_titles, showindex=book_titles,floatfmt=\".2f\"))\n\n\n\n\nFrankenstein\nMathilda\nThe Last Man\nFalkner\n80days\nThousandLeagues\nMysteriousIsland\nCenterOfTheEarth\n\n\n\n\nFrankenstein\n1.00\n0.11\n0.21\n0.22\n0.05\n0.06\n0.07\n0.08\n\n\nMathilda\n0.11\n1.00\n0.14\n0.11\n0.03\n0.04\n0.04\n0.04\n\n\nThe Last Man\n0.21\n0.14\n1.00\n0.28\n0.08\n0.09\n0.11\n0.10\n\n\nFalkner\n0.22\n0.11\n0.28\n1.00\n0.07\n0.07\n0.08\n0.08\n\n\n80days\n0.05\n0.03\n0.08\n0.07\n1.00\n0.11\n0.10\n0.07\n\n\nThousandLeagues\n0.06\n0.04\n0.09\n0.07\n0.11\n1.00\n0.21\n0.14\n\n\nMysteriousIsland\n0.07\n0.04\n0.11\n0.08\n0.10\n0.21\n1.00\n0.17\n\n\nCenterOfTheEarth\n0.08\n0.04\n0.10\n0.08\n0.07\n0.14\n0.17\n1.00"
  },
  {
    "objectID": "lectures/word_vectors/02_vectors_examples.html#term-context-matrix",
    "href": "lectures/word_vectors/02_vectors_examples.html#term-context-matrix",
    "title": "Term-Document and Term-Context matrices",
    "section": "Term-context matrix",
    "text": "Term-context matrix\nTerm-document matrices can be useful for classifying and describing documents, but if we wanted to come up with vector representations to describe words, we need to build a term-context matrix. The basic intuition behind most vector-semantics draws from the Distributional Hypothesis (Harris 1954), which we can illustrate like this.\nTry to come up with words that you think are likely to appear in the blank:\n\nThe elderly __ spoke.\n\nNow do the same thing with this phrase:\n\nThe playful __ jumped.\n\nYou probably came up with different sets of words in each context. The idea here is that certain words are more likely to appear in certain contexts, and the more contexts two words share, the more similar they are.\n\nA quick and dirty term-context matrix\nWe’ll build a quick and dirty term-context matrix with Frankenstein. Often people exclude “stopwords”, like function words at this stage.\n\nwith open(mary_shelley_files[0], 'r') as f:\n  text = f.read()\nunigrams = RegexpTokenizer(r\"\\w+\").tokenize(text.replace(\"\\n\", \" \").lower())\n\nTo build a term-context matrix, we basically look at a “concordance” of every word in the book. We set a context size of some number of words preceding and some number of words following the target word, and then pull those examples out. Let’s do that for “monster.”\n\ncontext_size = 3\nfor idx in range(context_size, len(unigrams)-context_size):\n  if unigrams[idx] == \"monster\":\n    full_context = unigrams[idx-context_size : idx+1+context_size]\n    print(full_context)\n\n['wretch', 'the', 'miserable', 'monster', 'whom', 'i', 'had']\n['to', 'behold', 'this', 'monster', 'but', 'i', 'feared']\n['imagined', 'that', 'the', 'monster', 'seized', 'me', 'i']\n['form', 'of', 'the', 'monster', 'on', 'whom', 'i']\n['i', 'was', 'the', 'monster', 'that', 'he', 'said']\n['fear', 'lest', 'the', 'monster', 'whom', 'i', 'had']\n['remaining', 'friends', 'abhorred', 'monster', 'fiend', 'that', 'thou']\n['in', 'reality', 'the', 'monster', 'that', 'i', 'am']\n['i', 'then', 'a', 'monster', 'a', 'blot', 'upon']\n['you', 'form', 'a', 'monster', 'so', 'hideous', 'that']\n['only', 'a', 'detestable', 'monster', 'that', 'is', 'indeed']\n['go', 'he', 'cried', 'monster', 'ugly', 'wretch', 'you']\n['with', 'me', 'hideous', 'monster', 'let', 'me', 'go']\n['and', 'let', 'the', 'monster', 'depart', 'with', 'his']\n['promise', 'fulfilled', 'the', 'monster', 'would', 'depart', 'for']\n['but', 'that', 'the', 'monster', 'followed', 'me', 'and']\n['my', 'rage', 'the', 'monster', 'saw', 'my', 'determination']\n['on', 'whom', 'the', 'monster', 'might', 'satisfy', 'his']\n['fingers', 'of', 'the', 'monster', 'already', 'grasping', 'my']\n['eyes', 'of', 'the', 'monster', 'as', 'i', 'first']\n['me', 'and', 'the', 'monster', 'of', 'my', 'creation']\n['happy', 'if', 'the', 'monster', 'executed', 'his', 'threat']\n['magic', 'powers', 'the', 'monster', 'had', 'blinded', 'me']\n['face', 'of', 'the', 'monster', 'he', 'seemed', 'to']\n['their', 'cause', 'the', 'monster', 'whom', 'i', 'had']\n['to', 'seize', 'the', 'monster', 'be', 'assured', 'that']\n['cursed', 'and', 'hellish', 'monster', 'drink', 'deep', 'of']\n['information', 'a', 'gigantic', 'monster', 'they', 'said', 'had']\n['apparition', 'of', 'the', 'monster', 'seen', 'from', 'our']\n['connected', 'such', 'a', 'monster', 'has', 'then', 'really']\n['my', 'lips', 'the', 'monster', 'continued', 'to', 'utter']\n\n\nHere, we’ll call monster the target, or $w$, and every other word in the context a context word, or \\(c\\). To build a term-context matrix, we would need a row of the matrix to be dedicated to the word monster, and columns for every possible word that could occur around monster. We’d then go and add 1 to the relevant column each time we saw a word in the context of monster.\nTo do this in practice, we need to get a vocuabulary of unique words that appear in the book, and also convenient ways to convert a word string into an index, and a convenient way to convert an index to a word.\n\nvocabulary = set(unigrams)\nword_to_index = {w:idx for idx, w in enumerate(vocabulary)}\nindex_to_word = {idx:w for idx, w in enumerate(vocabulary)}\n\nThen, we need to create a matrix full of zeros with a row and column for each word in the vocabulary.\n\nterm_context = np.zeros(shape = (len(vocabulary), len(vocabulary)))\n\nThen, we just loop through the book, adding 1 to every cell where the target word (in the rows) appears in the context of another word (in the columns).\n\ncontext_size = 3\nfor i in range(context_size, len(unigrams)-context_size):\n  word = unigrams[i]\n  word_index = word_to_index[word]\n  prewindow = unigrams[i-context_size : i]\n  postwindow = unigrams[i+1 : i+1+context_size]\n  context = prewindow + postwindow\n  for c in context:\n    c_index = word_to_index[c]\n    term_context[word_index, c_index] += 1\n\nNow, if the term-document matrix was sparse, this is super sparse.\n\nnp.mean(term_context>0)\n\n0.0044127177791784865\n\n\nLet’s get the 5 most common words that appear in the context of “monster”.\n\nmonster_idx = word_to_index[\"monster\"]\nmonster_array = term_context[monster_idx, :]\ntop_five_monster_idx = (monster_array*-1).argsort()[0:5]\ntop_five_monster_word = [index_to_word[idx] for idx in top_five_monster_idx]\ntop_five_monster_word\n\n['the', 'i', 'that', 'of', 'me']\n\n\nAt this stage, we could just use these raw counts to calculate the cosine similarity between words,\n\ndist_from_monster = []\nfor i in range(len(vocabulary)):\n  dist_from_monster.append(cosine(monster_array, term_context[i, :]))\n\n\nmonster_disr_arr = np.array(dist_from_monster)\n\n\nmonster_sim = monster_disr_arr.argsort()[0:10]\nmonster_sim_word = [index_to_word[idx] for idx in monster_sim]\nmonster_sim_word\n\n['monster', 'on', 'which', 'from', 'fiend', 'and', 'in', 'towards', 'at', 'dæmon']\n\n\n\n\nPositive Pointwise Mutual Information\nSimilar problem as before, with words appearing very similar because very frequent words show up in a lot of contexts.\n\njoint_prob = term_context/sum(term_context)\n\nword_C = np.sum(term_context, axis = 1)\nword_prob = word_C / sum(word_C)\n\ncontext_C = np.sum(term_context, axis = 0)\ncontext_prob =context_C/sum(context_C)\n\njoint_exp = np.outer(word_prob, context_prob)\n\nPMI = np.log2(joint_prob/joint_exp)\n\n<string>:1: RuntimeWarning: divide by zero encountered in log2\n\nPMI[PMI < 0] = 0\n\n\nmonster_array = PMI[monster_idx, :]\ndist_from_monster = []\nfor i in range(len(vocabulary)):\n  dist_from_monster.append(cosine(monster_array, PMI[i, :]))\n\n\nmonster_disr_arr = np.array(dist_from_monster)\nmonster_sim = monster_disr_arr.argsort()[0:10]\nmonster_sim_word = [index_to_word[idx] for idx in monster_sim]\nmonster_sim_word\n\n['monster', 'let', 'accurate', 'denote', 'wretch', 'neck', 'hellish', 'behold', 'supposition', 'enjoy']"
  },
  {
    "objectID": "lectures/word_vectors/02_vectors_examples.html#doing-it-not-by-hand",
    "href": "lectures/word_vectors/02_vectors_examples.html#doing-it-not-by-hand",
    "title": "Term-Document and Term-Context matrices",
    "section": "Doing it not “by hand”",
    "text": "Doing it not “by hand”\n\nTf-idf\n\nIn Python\nThe key function here is TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nThis is just info and a helper function to read in the data.\n\nmary_shelley_ids = [84, 15238, 18247, 64329]\nmary_shelley_files =  [f\"gen/books/shelley/{x}.txt\" for x in mary_shelley_ids]\nmary_shelley_titles = [\"Frankenstein\", \"Mathilda\", \"The Last Man\", \"Falkner\"]\njules_verne_ids = [103, 164, 1268, 18857]\njules_verne_files = [f\"gen/books/verne/{x}.txt\" for x in jules_verne_ids]\njules_verne_titles = [\"80days\", \"ThousandLeagues\", \"MysteriousIsland\", \"CenterOfTheEarth\"]\n\n\nall_files = mary_shelley_files + jules_verne_files\ndef read_and_normalize(path):\n  \"\"\"\n    will read a document and normalize its text\n  \"\"\"\n  with open(path, 'r') as f:\n    text = f.read()\n  text = text.replace(\"\\n\", \" \").lower()\n  return(text)\n\nThe important part: documents is a with 8 values in it. Each value is a string, and contains the entire text of each book.\n\ndocuments = [read_and_normalize(path) for path in all_files]\n\nLine 1 sets up the rules we’re going to use for the tf-idf calculation. What TfidfVectorizer does by default does not match the math we did above, and even with these settings, it’s not going to be exactly similar.\n\nvectorizer = TfidfVectorizer(smooth_idf = False, sublinear_tf = True)\ntfidf = vectorizer.fit_transform(documents)\n\nThe resulting tfidf matrix puts the books along the rows and the words along the columns.\n\ntfidf.shape\n\n(8, 24645)\n\n\ncosine_similarity will do a rowwise comparison.\n\nsimilarities = cosine_similarity(tfidf)\nprint(np.around(similarities, 3))\n\n[[1.    0.474 0.534 0.562 0.371 0.387 0.405 0.437]\n [0.474 1.    0.429 0.442 0.299 0.314 0.316 0.351]\n [0.534 0.429 1.    0.57  0.349 0.366 0.394 0.397]\n [0.562 0.442 0.57  1.    0.364 0.363 0.385 0.4  ]\n [0.371 0.299 0.349 0.364 1.    0.401 0.401 0.384]\n [0.387 0.314 0.366 0.363 0.401 1.    0.506 0.464]\n [0.405 0.316 0.394 0.385 0.401 0.506 1.    0.488]\n [0.437 0.351 0.397 0.4   0.384 0.464 0.488 1.   ]]\n\n\n\n# Looking at self-similarity\nshelley_self = similarities[0:4, 0:4]\nshelley_self[np.triu_indices(4, k = 1)].mean()\n\n0.5017177281427782\n\n\n\n# Looking at self-similarity\nverne_self = similarities[4:8, 4:8]\nverne_self[np.triu_indices(4, k = 1)].mean()\n\n0.44065120578921735\n\n\n\n# Looking at cross-similarity\ncross_sim = similarities[0:4, 4:8]\ncross_sim.mean() \n\n0.36861403396731507\n\n\n\n\nIn R\n\n\n\nR\n\nlibrary(gutenbergr)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(lsa)\n\n\n(I’m using a special R package to access variables that I declared in python)\n\n\n\nR\n\nbook_ids <- c(py$mary_shelley_ids, py$jules_verne_ids)\nbook_ids\n\n\n[1]    84 15238 18247 64329   103   164  1268 18857\n\n\n\n\n\nR\n\nbooks <- gutenberg_download(book_ids)\n\n\n\n\n\nR\n\nbooks %>%\n  group_by(gutenberg_id) %>%\n  unnest_tokens(input = text, output = words) %>%\n  count(gutenberg_id, words) %>%\n  ungroup() %>%\n  bind_tf_idf(words, gutenberg_id, n) -> books_tf_idf\n\n\n\n\n\nR\n\nbooks_tf_idf %>%\n  group_by(gutenberg_id) %>%\n  arrange(desc(tf_idf)) %>%\n  slice(1:3) %>%\n  knitr::kable(digits = 3)\n\n\n\n\n\ngutenberg_id\nwords\nn\ntf\nidf\ntf_idf\n\n\n\n\n84\nclerval\n59\n0.001\n2.079\n0.002\n\n\n84\njustine\n54\n0.001\n2.079\n0.001\n\n\n84\nelizabeth\n88\n0.001\n0.981\n0.001\n\n\n103\nfogg\n602\n0.009\n2.079\n0.020\n\n\n103\npassepartout\n404\n0.006\n2.079\n0.013\n\n\n103\nphileas\n256\n0.004\n2.079\n0.008\n\n\n164\nnautilus\n509\n0.005\n2.079\n0.010\n\n\n164\nned\n322\n0.003\n2.079\n0.006\n\n\n164\nconseil\n274\n0.003\n2.079\n0.005\n\n\n1268\npencroft\n1050\n0.005\n2.079\n0.011\n\n\n1268\nharding\n844\n0.004\n2.079\n0.009\n\n\n1268\nneb\n455\n0.002\n2.079\n0.005\n\n\n15238\n_f\n67\n0.001\n2.079\n0.003\n\n\n15238\nmathilda\n56\n0.001\n2.079\n0.002\n\n\n15238\nmathilda\n55\n0.001\n2.079\n0.002\n\n\n18247\nraymond\n340\n0.002\n2.079\n0.004\n\n\n18247\nadrian\n285\n0.002\n2.079\n0.003\n\n\n18247\nidris\n230\n0.001\n2.079\n0.003\n\n\n18857\nhans\n171\n0.002\n2.079\n0.004\n\n\n18857\nuncle\n485\n0.006\n0.693\n0.004\n\n\n18857\nsneffels\n52\n0.001\n2.079\n0.001\n\n\n64329\nfalkner\n432\n0.003\n2.079\n0.006\n\n\n64329\nneville\n277\n0.002\n2.079\n0.004\n\n\n64329\nelizabeth\n470\n0.003\n0.981\n0.003\n\n\n\n\n\n\n\n\nR\n\nfrank <- books_tf_idf %>% filter(gutenberg_id == 84) %>% pull(tf_idf)\n\n\n\n\n\nR\n\nbooks_tf_idf %>%\n  ungroup() %>%\n  complete(gutenberg_id, words, fill=list(tf_idf = 0)) %>%\n  arrange(words) -> tf_idf_complete\n\n\n\n\n\nR\n\ntf_idf_complete %>%\n  filter(gutenberg_id == 84) %>%\n  pull(tf_idf) -> frank_vector\n\n\n\n\n\nR\n\ntf_idf_complete %>%\n  filter(gutenberg_id == 103) %>%\n  pull(tf_idf) -> eighty_vector\n\n\n\n\n\nR\n\ncosine(frank_vector, eighty_vector)\n\n\n            [,1]\n[1,] 0.005146954"
  },
  {
    "objectID": "lectures/lem_stem/index.html#what-tokenizing-does-not-get-for-you",
    "href": "lectures/lem_stem/index.html#what-tokenizing-does-not-get-for-you",
    "title": "Lemmatizing and Stemming",
    "section": "What tokenizing does not get for you",
    "text": "What tokenizing does not get for you\nComing back to the example sentences from the first data processing lecture, properly tokenizing these sentences will only partly help us with our linguistic analysis.\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nfrom tabulate import tabulate\n\nphrase = \"\"\"The 2019 film CATS is a movie about cats. \nCats appear in every scene. \nA cat can always be seen\"\"\"\n\n# case folding\nphrase_lower = phrase.lower()\n\n# tokenization\ntokens = word_tokenize(phrase_lower)\n\n# counting\ntoken_count = Counter(tokens)\n\n# cat focus\ncat_list = [[k,token_count[k]] for k in token_count if \"cat\" in k]\n\nprint(tabulate(cat_list,\n               headers = [\"type\", \"count\"]))\n\n\n\ntype\ncount\n\n\n\n\ncats\n3\n\n\ncat\n1\n\n\n\nWe’ve still got the plural cats being counted as a separate word from cat, which for our weird use case, we don’t want. Our options here are to either “stem” or “lemmatize” our tokens."
  },
  {
    "objectID": "lectures/lem_stem/index.html#stemming",
    "href": "lectures/lem_stem/index.html#stemming",
    "title": "Lemmatizing and Stemming",
    "section": "Stemming",
    "text": "Stemming\nStemming is focused on cutting off morphemes and, to some degree, providing a consistent stem across all types that share a stem. So the outcomes aren’t always a recognizable word. The way it does this is all rule-based. For example, the first step of the Porter stemmer contains the following rewrite rules.\ni.   sses -> ss\nii.  ies -> i\niii. ss -> ss\niv.  s -> \nWhen a word comes into the first step, if its end matches any of the left hand sides, it will get re-written as the right hand side. If it could match multiple, the one it has the longest match with wins, so\n\n“passes” matches i. , so it gets rewritten as “pass”\n“pass” matches iii. and iv., but has the largest overlap with iii. so it gets rewritten as “pass”\n“parties” matches ii., so it gets rewritten as “parti”\n“pas” (as in “faux pas”) matches iv. so it gets rewritten as “pa”\n“cats” matches iv. so it gets rewritten as “cats”\n\nThis works basically correctly to the various /+z/ morphemes in English, but it over does it (“pas” should be left alone) and it produces some stems that don’t look like the actual root word (“parti” vs “party”).\nAfter this step, it contains a lot more hand crafted rules (e.g. ational - > ate).\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import SnowballStemmer\n\np_stemmer = PorterStemmer()\np_stemmed = [p_stemmer.stem(t) for t in tokens]\nfor t in p_stemmed:\n  print(f\"`{t}` |\", end = \" \")\nthe | 2019 | film | cat | is | a | movi | about | cat | . | cat | appear | in | everi | scene | . | a | cat | can | alway | be | seen |\ns_stemmer = SnowballStemmer(\"english\")\ns_stemmed = [s_stemmer.stem(t) for t in tokens]\nfor t in s_stemmed:\n  print(f\"`{t}` |\", end = \" \")\nthe | 2019 | film | cat | is | a | movi | about | cat | . | cat | appear | in | everi | scene | . | a | cat | can | alway | be | seen |\nJust to focus on how the stemmers operate over a specific paradigm:\ncry = [\"cry\", \"cries\", \"crying\", \"cried\", \"crier\"]\n\nprint(\n  tabulate(\n    [[c, s_stemmer.stem(c)] for c in cry],\n    headers=[\"token\", \"stem\"]\n  )\n)\n\n\n\ntoken\nstem\n\n\n\n\ncry\ncri\n\n\ncries\ncri\n\n\ncrying\ncri\n\n\ncried\ncri\n\n\ncrier\ncrier\n\n\n\nAlso, when something like inflectional morphology makes a change to the stem, it won’t get undone by the stemmer.\nrun = [\"run\", \"runs\", \"running\", \"ran\", \"runner\"]\n\nprint(\n  tabulate(\n    [[r, s_stemmer.stem(r)] for r in run],\n    headers=[\"token\", \"stem\"]\n  )\n)\n\n\n\ntoken\nstem\n\n\n\n\nrun\nrun\n\n\nruns\nrun\n\n\nrunning\nrun\n\n\nran\nran\n\n\nrunner\nrunner"
  },
  {
    "objectID": "lectures/lem_stem/index.html#lemmatizing",
    "href": "lectures/lem_stem/index.html#lemmatizing",
    "title": "Lemmatizing and Stemming",
    "section": "Lemmatizing",
    "text": "Lemmatizing\nLemmatizing involves a more complex morphological analysis of words, and as such requires language specific models to work.\n\nnltk lemmatizing\nnltk uses WordNet for its English lemmatizing. WordNet is a large database of lexical relations that have been hand annotated starting in the 1980s. Its outputs are always recognizable words.\n\nwnl = nltk.WordNetLemmatizer()\n\nprint(\n  tabulate(\n    [[c, wnl.lemmatize(c)] for c in cry],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n\n\n\ntoken\nlemma\n\n\n\n\ncry\ncry\n\n\ncries\ncry\n\n\ncrying\ncry\n\n\ncried\ncried\n\n\ncrier\ncrier\n\n\n\nprint(\n  tabulate(\n    [[r, wnl.lemmatize(r)] for r in run],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n\n\n\ntoken\nlemma\n\n\n\n\nrun\nrun\n\n\nruns\nrun\n\n\nrunning\nrunning\n\n\nran\nran\n\n\nrunner\nrunner\n\n\n\n\n\nspaCy lemmatizing\nspaCy has a number of models that do lemmatizing. They list WordNet along with a few other data sources for the model.\n\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\nlemmatizer = nlp.get_pipe(\"lemmatizer\")\n\ndoc = nlp(\" \".join(cry))\nprint(\n  tabulate(\n    [[c.text, c.lemma_] for c in doc],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n\n\n\ntoken\nlemma\n\n\n\n\ncry\ncry\n\n\ncries\ncry\n\n\ncrying\ncry\n\n\ncried\ncry\n\n\ncrier\ncrier\n\n\n\ndoc = nlp(\" \".join(run))\nprint(\n  tabulate(\n    [[r.text, r.lemma_] for r in doc],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n\n\n\ntoken\nlemma\n\n\n\n\nrun\nrun\n\n\nruns\nrun\n\n\nrunning\nrun\n\n\nran\nrun\n\n\nrunner\nrunner"
  },
  {
    "objectID": "lectures/lem_stem/index.html#the-use-of-lemmatizing-and-stemming",
    "href": "lectures/lem_stem/index.html#the-use-of-lemmatizing-and-stemming",
    "title": "Lemmatizing and Stemming",
    "section": "The use of lemmatizing and stemming",
    "text": "The use of lemmatizing and stemming\nFor a lot of the NLP tasks we’re going to be learning about, lemmatizing and stemming don’t factor in as part of the pre-processing pipeline. However, they’re useful tools to have handy when doing linguistic analyses. For example, for all of the importance of “word frequency” in linguistics literature, there’s often not much clarity about how the text was pre-processed to get these word frequencies."
  },
  {
    "objectID": "lectures/data_processing/addendum.html",
    "href": "lectures/data_processing/addendum.html",
    "title": "Addendum",
    "section": "",
    "text": "In the lecture notes before, I showed you how to do tokenizing in the python package nltk. But there’s another big NLP package out there called spaCy. Why did I focus on nltk? I think I can best explain that with this graph:"
  },
  {
    "objectID": "lectures/data_processing/addendum.html#tokenizing-with-spacy",
    "href": "lectures/data_processing/addendum.html#tokenizing-with-spacy",
    "title": "Addendum",
    "section": "Tokenizing with spaCy",
    "text": "Tokenizing with spaCy\n\n\n\n\n# bash\npython -m spacy download en_core_web_sm\n\n\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\nphrase2 = \"\"\"\nCATS had a budget of $100,000,000, most of which \nwent into the so-called 'digital fur technology'. \nIt's a little hard to believe, but it only made \n$75.5 million at the box office. #badmovie :-P\n\"\"\"\n\ndoc = nlp(phrase2.strip().replace(\"\\n\", \" \"))\n\nfor token in doc:\n  print(f\"| `{token.text}`\", end = \" \")\nCATS | had | a | budget | of | $ | 100,000,000 | , | most | of | which |  | went | into | the | so | - | called | ' | digital | fur | technology | ' | . |  | It | 's | a | little | hard | to | believe | , | but | it | only | made |  | $ | 75.5 | million | at | the | box | office | . | # | badmovie | :-P"
  },
  {
    "objectID": "lectures/data_processing/index.html",
    "href": "lectures/data_processing/index.html",
    "title": "Data Processing",
    "section": "",
    "text": "Before we even get to substantive issues of “text normalization” and “tokenization”, we need to also deal with basic data wrangling. For example, let’s say I wanted to download 4 works from Mary Shelly from Project Gutenberg and calculate what the most common 4 word sequences in her work are, I might quickly write some code like this.\n\n\n\n\n# python\n\n# urllib.request will download the books\nimport urllib.request\n\n# using a dictionary just to show the title of books here in the code.\nshelley_dict = {\"Tales and stories\": \"https://www.gutenberg.org/cache/epub/56665/pg56665.txt\",\n                \"Frankenstein\" : \"https://www.gutenberg.org/files/84/84-0.txt\",\n                \"The Last Man\" : \"https://www.gutenberg.org/cache/epub/18247/pg18247.txt\",\n                \"Mathilda\" : \"https://www.gutenberg.org/cache/epub/15238/pg15238.txt\"}\n\n# A collector list for all of the 4 word sequences\nall_grams4 = []\n\n# Loop over every url\nfor url in shelley_dict.values():\n  book_dat = urllib.request.urlopen(url)\n  \n  # this deals with the \n  # 1. character encoding\n  # 2. trailing whitespace\n  # 3. simplistic tokenization on spaces\n  book_lines = [line.decode(\"utf-8-sig\").strip().split(\" \") \n                for line in book_dat]\n  \n  # This flattens the list above into one long list of words\n  book_words = [word \n                for line in book_lines \n                  for word in line \n                    if len(word) > 0]\n  \n  # Collector list of 4grams from just this book\n  grams4 = []\n  \n  # loop over every index postion up to 4 words short of the end.\n  for i in range(len(book_words)-4):\n    \n    # glue together 4 word sequences with \"_\"\n    grams4.append(\"_\".join(book_words[i:(i+4)]))\n    \n  # Add this book's 4grams to all of the books' 4grams\n  all_grams4 += grams4\n\nThe list all_grams4 contains a list of every token of 4grams in these books. Let’s count them up and look at the top 10 most frequent 4 word phrases Mary Shelley used in her writing!\n\n\n\nTable 1: Top 10 4grams from Mary Shelley’s Work\n\n\n4gram\ncount\n\n\n\n\nProject_Gutenberg_Literary_Archive\n52\n\n\nthe_Project_Gutenberg_Literary\n44\n\n\nGutenberg_Literary_Archive_Foundation\n33\n\n\nthe_terms_of_this\n32\n\n\nProject_Gutenberg-tm_electronic_works\n31\n\n\nat_the_same_time\n25\n\n\nin_the_United_States\n24\n\n\nto_the_Project_Gutenberg\n24\n\n\n*_*_*_*\n22\n\n\nfor_the_sake_of\n21\n\n\n\n\n\nSo, either Mary Shelly was obsessed with the Project Gutenberg Literary Archive, and the terms of this and for the sake of, or something else is going on.\nAs it turns out, every plain text Project Gutenberg book has header information with a short version of the users’ rights and other metadata information, and then at the end has the entirety of the Project Gutenberg License, which is written in legal language.\nIn any corpus building project, decisions need to be made about how header, footer, and general boilerplate data like this will be treated. There are handy packages for python and R that make stripping out the legal language easy\n\npython: gutenbergpy\nR: gutenbergr\n\nOr, you might decide to leave it all in. It seems pretty clear this is the approach to the dataset they trained GPT-3 on, because if you prompt it with the first few lines of the Project Gutenberg license, it will continue it.\n\nFigure 1: The original Project Gutenberg License vs what GPT3 reproduces\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting aside the issue of headers and footers, we also need to deal with the fact that “markup” is everywhere. Even in the relatively plain text of Project Gutenberg books, they use underscores _ to indicate italics or emphasized text.\n\n# python\nunderscore_lines = [line \n                      for line in book_lines \n                        if any([\"_\" in word \n                                  for word in line])]\nfor i in range(4):\n  print(\" \".join(underscore_lines[i]))\n\nMathilda _is being published\nof_ Studies in Philology.\nnovelette _Mathilda_ together with the opening pages of its rough\ndraft, _The Fields of Fancy_. They are transcribed from the microfilm\n\n\nThis, again, is something we need to decide whether or not we want to include in our corpora. For these massive language models that focus on text generation, they may want the model to generate markup along with the text, so they might leave it in. Some text markup that’s intended to indicate prosodic patterns could be useful to leave in from a linguistic theory perspective.\nEither way, though, it is still a decision that needs to be made about the data."
  },
  {
    "objectID": "lectures/data_processing/index.html#text-normalization",
    "href": "lectures/data_processing/index.html#text-normalization",
    "title": "Data Processing",
    "section": "Text Normalization",
    "text": "Text Normalization\nI called the issues above “data wrangling”, since it’s mostly about identifying the content we want to be including in our modelling. But once you’ve done that, there are still questions about how we process data for analysis which fall under “text normalization”.\nConsider the following sentences\n\nThe 2019 film Cats is a movie about cats. Cats appear in every scene. A cat can always be seen.\n\nLet’s split this sentence up along whitespace1, and count how many times “cats” appears.\n\nimport re\nphrase = \"\"\"The 2019 film Cats is a movie about cats. \nCats appear in every scene. \nA cat can always be seen\"\"\"\n\nwords = re.split(\"\\s\", phrase)\ncat_c = Counter(words)\n\n\n\n\nTable 2: frequency of “cats”\n\n\n\n\n\n\nword\ncount\n\n\n\n\nCats\n2\n\n\ncats.\n1\n\n\ncat\n1\n\n\n\n\n\nA very important thing to keep in mind is that our language models will treat the words in these rows as three completely separate word types.2 That even includes the period . in the second row. Some typical steps involve\n\nseparating punctuation from words\n“case folding” or converting everything to lowercase.\n\n\nwords2 = re.split(r\"\\s\", phrase)\nwords2 = [re.sub(r\"\\W\", '', word) for word in words2]\nwords2 = [word.lower() for word in words2]\nwords2\n\n['the', '2019', 'film', 'cats', 'is', 'a', 'movie', 'about', 'cats', '', 'cats', 'appear', 'in', 'every', 'scene', '', 'a', 'cat', 'can', 'always', 'be', 'seen']\n\n\n\n\n\nTable 3: frequency of tokenized “cats”\n\n\nword\ncount\n\n\n\n\ncats\n3\n\n\ncat\n1\n\n\n\n\n\nWe’ve now got a slightly better set of counts. With the punctuation stripped and everything pushed to lowercase, there’s now just two word forms: cats and cat.\nOne downside, though, is we’ve also collapsed together the title Cats, which refers to either a Broadway musical or a 2019 film, and the word “cats” which refers to furry felines. Merging these two together could be sub-optimal for later tasks, like, say, sentiment analysis of movie reviews.\n\n‘Cats’ is both a horror and an endurance test, a dispatch from some neon-drenched netherworld where the ghastly is inextricable from the tedious. – LA Times"
  },
  {
    "objectID": "lectures/data_processing/index.html#tokenization-or-text-is-complex",
    "href": "lectures/data_processing/index.html#tokenization-or-text-is-complex",
    "title": "Data Processing",
    "section": "Tokenization (or, text is complex)",
    "text": "Tokenization (or, text is complex)\nSetting aside semantic issues, there are a lot of things that happen inside of text, especially if it is transcribed speech, that makes normalizing text and tokenizing it way more challenging than just splitting up on white space and stripping out punctuation, even just for English.\n\nPlaces to leave in punctuation\nSome examples given by Jurafsky & Martin for where you might want to leave in punctuation are:\n\nYou don’t want to eliminate punctuation from inside Ph.D, or m.p.h.. You also don’t want to eliminate it from some proper names, like ampersands in Procter & Gamble, Texas A&M, A&W, m&m's.\nYou’ll want to keep formatting in numerals, and not split them into separate words. These are all possible numeric formats cross culturally for the same quantity\n\n1,000.55\n1.000,55\n1 000,55\n\nCurrency symbols should probably be kept together with their numerals, and depending on the culture & denomination.\n\n$0.99\n99¢\n€0,99\n\nDates: There are so many different permutations on how dates can be formatted that I shouldn’t list them all here, but here are some.3\n\nyyyy-mm-dd 2022-09-12, yyyy/mm/dd 2022/09/12\nyyyy-m-dd 2022-9-12, yyyy/m/dd 2022/9/12\ndd-mm-yyyy 12-09-2022, dd/mm/yyyy 12/09/2022\ndd-m-yyyy 12-9-2022, dd/m/yyyy 12/9/2022\ndd-mm-yy 12-09-22, dd/mm/yy 12/09/2022\nmm-dd-yyyy 09-12-2022, mm/dd/yyyy 09/12/2022\nm-dd-yyyy 9-12-2022, m/dd/yyyy 9/12/2022\nmm-dd-yy 09-12-22, mm/dd/yy 09/12/22\nm-dd-yy 9-12-22, m/dd/yy 9/12/22\n\nEmoticons,4 where the token is entirely punctuation :), >.<.\n\n\n\nPlaces to split up words\nSometimes the tokens you get back from whitespace tokenization ought to be split up even further. One example might be hyphenated words, like hard-won.\n\nhard-won ➔ hard, won or hard, -, won.\n\nAnother example involves clitics, like n't or 're in English.\n\nisn't ➔ is, n't\ncan't ➔ ca, n't\nwhat're ➔ what, 're\n\n\n\nPlaces to glue words together\nYou might want to also glue together tokens from whitespace tokenization.\n\nNew, York, City ➔ New York City\nSuper, Smash, Brothers ➔ Super Smash Brothers.\n\n\n\nChallenges with speech and text\n\n: $1500\n\n: “one thousand five hundred dollars”\n: “fifteen hundred dollars”\n: “one and a half thousand dollars”\n: “one point five thousand dollars”"
  },
  {
    "objectID": "lectures/data_processing/index.html#tokenizers",
    "href": "lectures/data_processing/index.html#tokenizers",
    "title": "Data Processing",
    "section": "Tokenizers",
    "text": "Tokenizers\nThere seem to be broadly two kinds of tokenizers people use, depending on their goals.\n\nTokenizers that try to hew to linguistic structure, and can generate relatively large vocabulary sizes (number of tokens).\nTokenizers that try to keep the vocabulary size relatively small, to make neural network training possible.\n\n\nWord/language piece based tokenizers\nThere are a number of tokenizers available through the nltk (Natural Language Took Kit) (Bird, Klein, and Loper 2009) python package. They all have slightly different settings and outcomes. Here I’ll compare the PennTreeBank tokenizer, a simpler punctuation-based tokenizer, and a “casual” tokenizer.\n\nPennTreeBank\nThe PennTreeBank tokenizer is built up out of regular expressions (more on that soon). It\n\nseparates out punctuation and non-alphanumeric characters as their own tokens\nSeparates off contractions as their own tokens, using a fixed list\n\n\nfrom nltk.tokenize import sent_tokenize, TreebankWordTokenizer\n\nphrase2 = \"\"\"CATS had a budget of $100,000,000, most of which went into the so-called 'digital fur technology'.\nIt's a little hard to believe, but it only made $75.5 million at the box office. \n#badmovie :-P\n\"\"\"\n\nsentences = sent_tokenize(phrase2)\ntokens = [TreebankWordTokenizer().tokenize(s) for s in sentences]\n\n\n\n\nCATS\nhad\na\nbudget\nof\n$\n\n\n100,000,000\n,\nmost\nof\nwhich\nwent\n\n\ninto\nthe\nso-called\n'digital\nfur\ntechnology\n\n\n'\n.\nIt\n's\na\nlittle\n\n\nhard\nto\nbelieve\n,\nbut\nit\n\n\nonly\nmade\n$\n75.5\nmillion\nat\n\n\nthe\nbox\noffice\n.\n#\nbadmovie\n\n\n:\n-P\n\n\n\n\n\n\n\n\n\nSimple whitespace + punctuation tokenizer\nSplits strings based on whitespace & non-alphanum\n\nfrom nltk.tokenize import wordpunct_tokenize\ntokens = [wordpunct_tokenize(s) for s in sentences]\n\n\n\n\nCATS\nhad\na\nbudget\nof\n$\n\n\n100\n,\n000\n,\n000\n,\n\n\nmost\nof\nwhich\nwent\ninto\nthe\n\n\nso\n-\ncalled\n'\ndigital\nfur\n\n\ntechnology\n'.\nIt\n'\ns\na\n\n\nlittle\nhard\nto\nbelieve\n,\nbut\n\n\nit\nonly\nmade\n$\n75\n.\n\n\n5\nmillion\nat\nthe\nbox\noffice\n\n\n.\n#\nbadmovie\n:-\nP\n\n\n\n\n\n\nTweet Tokenizer\nIntended to be more apt for tokenizing tweets.\n\nfrom nltk.tokenize import TweetTokenizer\ntokens = [TweetTokenizer().tokenize(s) for s in sentences]\n\n\n\n\nCATS\nhad\na\nbudget\nof\n$\n\n\n100,000\n,\n000\n,\nmost\nof\n\n\nwhich\nwent\ninto\nthe\nso-called\n'\n\n\ndigital\nfur\ntechnology\n'\n.\nIt's\n\n\na\nlittle\nhard\nto\nbelieve\n,\n\n\nbut\nit\nonly\nmade\n$\n75.5\n\n\nmillion\nat\nthe\nbox\noffice\n.\n\n\n#badmovie\n:-P\n\n\n\n\n\n\n\n\n\n\nFixed Vocab Tokenizing\nThe downside of tokenizers like the three above is that you can’t pre-specify how many types you will get out. That is, you can’t pre-specify your vocabulary size. That isn’t ideal for neural-network based models, which need to use matrices of finite and pre-specified size. So there are also tokenizers that keep a fixed cap on the vocabulary size, even if they result in tokens that aren’t really linguistically meaningful.\n\nByte Pair Encoding\nByte Pair Encoding is one approach to tokenizing where we can pre-specify the maximum vocabulary size either by setting a maximum vocabulary hyperparameter, or by setting a maximum iteration hyperparameter.\nLet’s start out with a fake training of a byte pair encoder with the simple vocabulary “cats can’t canter”. We kick things off treating every character as a token, plus a specialized start-of-word symbol, which I’m representing with _.\n\n\n\nTokens\n\n\n_ c a t s\n_ c a n ' t\n_ c a n t e r\n\n\n\n\nTypes\n\n\n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", '_'}\n\n\n\n\n\n\nThis is, in principle, the smallest and simplest tokenization we could do for any input text. While the total number of words is infinite, the total number of characters or symbols we use to create those words is finite.\nThe next step is to count up all of the pairs (or bigrams) of tokens in the training data. In this case, both (_, c) and (c, a) appear equally commonly, so I make a decision and say (_, c) is the one we’ll process first. We’ll paste them together, call them a new type, and replace all (_, c) sequences with _c.\n\n\n\ntokens\n\n\n_c a t s\n_c a n ' t\n_c a n t e r\n\n\n\n\ntypes\n\n\n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", ' ',\n`_c`}\n\n\n\nRepeating the process, the most frequently occurring bigram is now (_c, a), so we’ll add _ca as a new type, and replace all (_c, a) sequences with _ca.\n\n\n\ntokens\n\n\n_ca t s \n_ca n ' t _\n_ca n t e r _\n\n\n\n\ntypes\n\n\n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", ' ', \n'_c', '_ca'}\n\n\n\nFinally, the last most frequent sequence is (_ca, n), so we’ll add _can to the vocabulary, and collapse (_ca, n) sequences.\n\n\n\ntokens\n\n\n_ca t s\n_can ' t\n_can t e r\n\n\n\n\ntypes\n\n\n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", ' ', \n'_c', '_ca', '_can'}\n\n\n\nWe’ll stop at that point, but we could either continue for a fixed number of iterations, or until our type, or vocabulary size reaches a fixed number.\n\n\nTraining\nThe python library sentencepiece has a method for training a BPE tokenizer. We’ll run it on Frankenstein with a vocabulary limited to about ~1,000 items to see what we’ll get.\n\nimport gutenbergpy.textget\nfrom gutenbergpy.gutenbergcache import GutenbergCacheSettings\nimport sentencepiece as spm\n\nGutenbergCacheSettings.set(TextFilesCacheFolder = \"gen/texts/\")\n\nraw_book = gutenbergpy.textget.get_text_by_id(84)\n\nclean_book  = gutenbergpy.textget.strip_headers(raw_book)\nwith open(\"gen/texts/frank.txt\", 'wb') as file:\n  x = file.write(clean_book)\n  file.close()\n\n\nspm.SentencePieceTrainer.train(input = \"gen/texts/frank.txt\", \n                               model_prefix = \"gen/m\",\n                               vocab_size = 1000, \n                               model_type = \"bpe\")\n\nThe code above trained the tokenizer and saved it as a model. Here we’ll load it and run it on a sample paragraph to look at the output.\n\nsp = spm.SentencePieceProcessor(model_file='gen/m.model')\n\n\npara = \"\"\"\nYou will rejoice to hear that no disaster has accompanied the\ncommencement of an enterprise which you have regarded with such evil\nforebodings. I arrived here yesterday, and my first task is to assure\nmy dear sister of my welfare and increasing confidence in the success\nof my undertaking\n\"\"\"\n\nprint(sp.encode_as_pieces(para))\n\n['▁You', '▁will', '▁re', 'j', 'o', 'ice', '▁to', '▁he', 'ar', '▁that', '▁no', '▁dis', 'as', 'ter', '▁has', '▁acc', 'om', 'p', 'an', 'ied', '▁the', '▁comm', 'ence', 'ment', '▁of', '▁an', '▁enter', 'pr', 'ise', '▁which', '▁you', '▁have', '▁reg', 'ard', 'ed', '▁with', '▁such', '▁ev', 'il', '▁f', 'ore', 'b', 'od', 'ings', '.', '▁I', '▁arri', 'ved', '▁he', 're', '▁y', 'es', 'ter', 'd', 'ay', ',', '▁and', '▁my', '▁first', '▁t', 'as', 'k', '▁is', '▁to', '▁ass', 'ure', '▁my', '▁dear', '▁s', 'is', 'ter', '▁of', '▁my', '▁we', 'lf', 'are', '▁and', '▁inc', 're', 'as', 'ing', '▁conf', 'id', 'ence', '▁in', '▁the', '▁su', 'cc', 'ess', '▁of', '▁my', '▁under', 't', 'aking']\n\n\n\n\nThe Benefit?\nWhy tokenize things this way? On the one hand, we wind up with tokens that don’t align with any particular linguistic structure. On the other, we’ve got a predictable vocabulary size, and a dramatically less serious data sparsity problem.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmethod\nonce\ntotal\nproportion once\n\n\n\n\nbpe\n9\n1015\n0.009\n\n\nptb\n3533\n7710\n0.458"
  },
  {
    "objectID": "lectures/ngram/02_smoothing.html",
    "href": "lectures/ngram/02_smoothing.html",
    "title": "ngram - Smoothing",
    "section": "",
    "text": "The notes on Perplexity, describe how we can get a measure of how well a given n-gram model predicts strings in a test set of data. Roughly speaking:\n\nThe better the model gets, the higher a probability it will assign to each \\(P(w_i|w_{i-1})\\) .\nThe higher the probabilities, the lower the perplexities.\nThe lower the perplexities, the better the model\n\nAs a quick demonstration, I’ve written some code here in collapsible sections to build a bigram model of Frankenstein, and to get the conditional probabilities for every bigram in an input sentence.\n\n\n\npython\n\nimport nltk\nfrom collections import Counter\nimport gutenbergpy.textget\nfrom tabulate import tabulate\nimport numpy as np\n\n\n\n\n\npython\n\n\ngetbook() function\ndef getbook(book, outfile):\n  \"\"\"\n  Download a book from project Gutenberg and save it \n  to the specified outfile\n  \"\"\"\n  print(f\"Downloading Project Gutenberg ID {book}\")\n  raw_book = gutenbergpy.textget.get_text_by_id(book)\n  clean_book = gutenbergpy.textget.strip_headers(raw_book)\n  if not outfile:\n    outfile = f'{book}.txt'\n    print(f\"Saving book as {outfile}\")\n  with open(outfile, 'wb') as file:\n    file.write(clean_book)\n    file.close()\n\n\n\n\n\n\npython\n\ngetbook(book = 84, outfile = \"gen/frankenstein.txt\")\n\n\nDownloading Project Gutenberg ID 84\n\n\n\n\n\npython\n\n\nFrom a file string to ngrams\ndef ngramize(filename, n = 2):\n  \"\"\"\n    given a file name, generate the ngrams and n-1 grams\n  \"\"\"\n  with open(filename, 'r') as f:\n    lines = f.read()\n    \n  sentences = nltk.sent_tokenize(lines)\n  sentences = [sent.strip().replace(\"\\n\", \" \") \n                      for sent in sentences]\n                      \n  sentences_tok = [nltk.word_tokenize(sent) \n                      for sent in sentences]\n                      \n  sentences_padn = [list(nltk.lm.preprocessing.pad_both_ends(sent, n = n)) \n                      for sent in sentences_tok]\n                      \n  sentences_ngram = [list(nltk.ngrams(sent, n = n)) \n                      for sent in sentences_padn]\n  sentences_ngram_minus = [list(nltk.ngrams(sent, n = n-1)) \n                      for sent in sentences_padn]                      \n  \n  flat_ngram = sum(sentences_ngram, [])\n  flat_ngram_minus = sum(sentences_ngram_minus, [])  \n                      \n  return(flat_ngram, flat_ngram_minus)\n\n\n\n\n\n\npython\n\n\nGetting bigrams and unigrams from frankenstein\nbigram, unigram = ngramize(\"gen/frankenstein.txt\", n = 2)\n\n\n\n\n\n\npython\n\n\nGetting counts of bigrams and unigrams\nbigram_count = Counter(bigram)\nunigram_count = Counter(unigram)\n\n\n\n\n\n\npython\n\n\nA function to get the conditional probability of a bigram\ndef get_conditional_prob(x, bigram_count, unigram_count):\n  \"\"\"\n    for a tuple x, get the conditional probability of x[1] | x[0]\n  \"\"\"\n  if x in bigram_count:\n    cond = bigram_count[x] / unigram_count[x[0:-1]]\n  else:\n    cond = 0\n    \n  return(cond)\n\n\n\n\n\n\npython\n\n\nA function to get the conditional probability of every ngram in a sentence\ndef get_sentence_probs(sentence, bigram_count, unigram_count, n = 2):\n  \"\"\"\n    given a sentence, get its list of conditional probabilities\n  \"\"\"\n  sent_tokens = nltk.word_tokenize(sentence)\n  sent_pad = nltk.lm.preprocessing.pad_both_ends(sent_tokens, n = n)\n  sent_ngram = nltk.ngrams(sent_pad, n = n)\n  sent_conditionals = [get_conditional_prob(gram, bigram_count, unigram_count) \n                        for gram in sent_ngram]\n  return(sent_conditionals)\n\n\n\n\n\n\npython\n\n\nGiven a sentence, get the conditional probability expression, for printing.\ndef get_conditional_strings(sentence, n = 2):\n  \"\"\"\n    given a sentence, return the string of conditionals\n  \"\"\"\n  sent_tokens = nltk.word_tokenize(sentence)\n  sent_pad = nltk.lm.preprocessing.pad_both_ends(sent_tokens, n = n)\n  sent_pad = [x.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\") for x in sent_pad]\n  sent_ngram = nltk.ngrams(sent_pad, n = n)\n  out_cond = [f\"P({x[-1]} | {' '.join(x[0:-1])})\" for x in sent_ngram]\n  return(out_cond)\n\n\n\nHaving built the bigram model with the code above, we can take this sample sentence:\n\nI saw the old man.\n\nWe can calculate the conditional probability of every word in the sentence given the word before, as well as the surprisal for each word.1\n\n\n\npython\n\nsentence = \"I saw the old man.\"\ncond_probs = get_sentence_probs(sentence, bigram_count, unigram_count, n = 2)\ncond_surp = [-np.log2(x) for x in cond_probs]\ncond_strings = get_conditional_strings(sentence, n = 2)\n\n\n\n\n\nconditional\nprobability\nsurprisal\n\n\n\n\nP(I | <s>)\n0.1876\n2.4139\n\n\nP(saw | I)\n0.0162\n5.9476\n\n\nP(the | saw)\n0.2340\n2.0952\n\n\nP(old | the)\n0.0064\n7.2865\n\n\nP(man | old)\n0.6800\n0.5564\n\n\nP(. | man)\n0.1364\n2.8745\n\n\nP(</s> | .)\n0.9993\n0.0011\n\n\n\nSumming up the surprisal column, we get the total surprisal of the sentence (about 21 bits). We can then get the number of bits per word (about 3) which gives us our ngram perplexity for the sentence (about 8).\n\n\n\ntotal surprisal\nsurprisal/word\nperplexity\n\n\n\n\n21.1752\n3.0250\n8.1400\n\n\n\n\n\nBut, not everything is so neat and tidy. Let’s try this again for the sentence\n\nI saw the same man.\n\n\n\n\npython\n\nsentence = \"I saw the same man.\"\ncond_probs = get_sentence_probs(sentence, bigram_count, unigram_count, n = 2)\ncond_surp = [-np.log2(x) for x in cond_probs]\ncond_strings = get_conditional_strings(sentence, n = 2)\n\n\n\n\n\nconditional\nprobability\nsurprisal\n\n\n\n\nP(I | <s>)\n0.1876\n2.4139\n\n\nP(saw | I)\n0.0162\n5.9476\n\n\nP(the | saw)\n0.2340\n2.0952\n\n\nP(same | the)\n0.0154\n6.0235\n\n\nP(man | same)\n0.0000\n\n\n\nP(. | man)\n0.1364\n2.8745\n\n\nP(</s> | .)\n0.9993\n0.0011\n\n\n\n\n\n\ntotal surprisal\nsurprisal/word\nperplexity\n\n\n\n\n\n\n!\n\n\n\nIt looks like the bigram (\"same\", \"man\") just didn’t appear in the novel. This is zero percolates up through all of our calculations.\n\\[\nC(\\text{same man}) = 0\n\\]\n\\[\nP(\\text{same man}) = \\frac{C(\\text{same man)}}{N} = \\frac{0}{N} = 0\n\\]\n\\[\nP(\\text{man}~|~\\text{same}) = \\frac{P(\\text{same man)}}{P(\\text{same)}} = \\frac{0}{P(\\text{same})} = 0\n\\]\n\\[\ns(\\text{man}~|~\\text{same}) = -\\log_2(P(\\text{man}~|~\\text{same})) = -\\log_2(0) = \\infty\n\\]\n\\[\npp(\\text{I saw the same man.)} = \\frac{\\sum_{i=1}^Ns(w_i|w_{i-1})}{N} = \\frac{\\dots+\\infty+\\dots}{N} = \\infty\n\\]\nIn other words, our bigram model’s “mind” is completely blown by a sentence with the sequence same man in it.\n\n\n\nFigure 1: Our our ngram model, upon seeing same man\n\n\nThis is, of course data sparsity rearing its head again. On the one hand, we are building an n-gram model out of a fairly small corpus. But on the other, the data sparsity problem will never go away, and we are always going to be left with the following two issues:\n\nOut Of Vocabulary items\nMissing ngrams of words that were in the vocabulary."
  },
  {
    "objectID": "lectures/ngram/02_smoothing.html#oov---out-of-vocabulary",
    "href": "lectures/ngram/02_smoothing.html#oov---out-of-vocabulary",
    "title": "ngram - Smoothing",
    "section": "OOV - Out of Vocabulary",
    "text": "OOV - Out of Vocabulary\n“Out Of Vocabulary”, commonly referred to OOV, problems, are going to come up if you ever do any computational work with language of any variety.\n\n\n\n\n\n\nOOV Example\n\n\n\nA lot of phoneticians today use “forced alignment”, which tries to time align words and phones to audio. Step one of the process is taking a transcription, tokenizing it, then looking up each token in a pre-specified pronunciation dictionary. A commonly used pronunciation dictionary is the CMU pronunciation dictionary. Here’s what a few entries of it around Frankenstein look like\n...\nFRANKENFOOD  F R AE1 NG K AH0 N F UW2 D\nFRANKENHEIMER  F R AE1 NG K AH0 N HH AY2 M ER0\nFRANKENSTEIN  F R AE1 NG K AH0 N S T AY2 N\nFRANKENSTEIN(1)  F R AE1 NG K AH0 N S T IY2 N\nFRANKENSTEIN'S  F R AE1 NG K AH0 N S T AY2 N Z\nFRANKENSTEIN'S(1)  F R AE1 NG K AH0 N S T IY2 N Z\nFRANKFORT  F R AE1 NG K F ER0 T\nFRANKFORT'S  F R AE1 NG K F ER0 T S\nFRANKFURT  F R AE1 NG K F ER0 T\n...\nLet’s say I tokenized this sentence and looked up each word\n\nI ate the po’boy.\n\nWe’d wind up with this:\nI  AY1\nATE  EY1 T\nTHE  DH AH0\n<UNK>  <UNK>\nWe’re getting <UNK> for “po’boy” because it’s not in the CMU dictionary. It’s an Out Of Vocabulary, or OOV word.\n\n\nOur example of perplexity blowing up was due to a specific bigram, ('same', 'man') not appearing in the corpus, even though each individual word does appear. The same thing will happen if any individual word in a sentence is oov.\n\n\n\npython\n\n# literally blowing the mind of a victorian child eating a cool ranch dorito\nsentence = \"I ate a cool ranch Dorito.\"\ncond_probs = get_sentence_probs(sentence, bigram_count, unigram_count, n = 2)\ncond_surp = [-np.log2(x) for x in cond_probs]\ncond_strings = get_conditional_strings(sentence, n = 2)\n\n\n\n\n\nconditional\nprobability\nsurprisal\n\n\n\n\nP(I | <s>)\n0.1876\n2.4139\n\n\nP(ate | I)\n0.0007\n10.4712\n\n\nP(a | ate)\n0.2500\n2.0000\n\n\nP(cool | a)\n0.0000\n\n\n\nP(ranch | cool)\n0.0000\n\n\n\nP(Dorito | ranch)\n0.0000\n\n\n\nP(. | Dorito)\n0.0000\n\n\n\nP(</s> | .)\n0.9993\n0.0011\n\n\n\n\nSolutions?\nOne approach SLP suggests is to convert every vocabulary item that occurs below a certain frequency to <UNK>, then re-estimate all of the ngram values. Here, I’m\n\n# Getting a list of unigrams that occurred once\nto_unk = [x for x in unigram_count if unigram_count[x] == 1]\n\n# <UNK> conversion\nunigram_unk = [(\"<UNK>\",) if x in to_unk else x for x in unigram]\nbigram_unk = [(\"<UNK>\", \"<UNK>\") if ((x[0],) in to_unk and (x[1],) in to_unk) else\n              (\"<UNK>\", x[1]) if (x[0],) in to_unk else\n              (x[0], \"<UNK>\") if (x[1],) in to_unk else\n              x for x in bigram ]\n\n# <UNK> count              \nunigram_unk_count = Counter(unigram_unk)\nbigram_unk_count = Counter(bigram_unk)\n\n\n\n\npython\n\n\nA function to get the conditional probability of every ngram in a sentence\ndef get_sentence_unk_probs(sentence, bigram_count, unigram_count, n = 2):\n  \"\"\"\n    given a sentence, get its list of conditional probabilities\n  \"\"\"\n  sent_tokens = nltk.word_tokenize(sentence)\n  sent_tokens_unk = [x if (x,) in unigram_count else \"<UNK>\" for x in sent_tokens]\n  sent_pad = nltk.lm.preprocessing.pad_both_ends(sent_tokens_unk, n = n)\n  sent_ngram = nltk.ngrams(sent_pad, n = n)\n  sent_conditionals = [get_conditional_prob(gram, bigram_count, unigram_count) \n                        for gram in sent_ngram]\n  return(sent_conditionals)\n\n\n\n\nsentence = \"I ate a Dorito.\"\ncond_probs = get_sentence_unk_probs(sentence, bigram_unk_count, unigram_unk_count, n = 2)\ncond_surp = [-np.log2(x) for x in cond_probs]\ncond_strings = get_conditional_unk_strings(sentence, unigram_count, n = 2)\n\n\n\n\nconditional\nprobability\nsurprisal\n\n\n\n\nP(I | <s>)\n0.1876\n2.4139\n\n\nP(ate | I)\n0.0007\n10.4712\n\n\nP(a | ate)\n0.2500\n2.0000\n\n\nP(<UNK> | a)\n0.1173\n3.0912\n\n\nP(. | <UNK>)\n0.0600\n4.0588\n\n\nP(</s> | .)\n0.9993\n0.0011\n\n\n\nConverting low frequency words to <UNK> means that now when the ngram model meets a word it doesn’t know, like Dorito, there is still some probability it can assign."
  },
  {
    "objectID": "lectures/ngram/02_smoothing.html#real-zeros",
    "href": "lectures/ngram/02_smoothing.html#real-zeros",
    "title": "ngram - Smoothing",
    "section": "Real Zeros",
    "text": "Real Zeros\nThis <UNK>ification of the data doesn’t solve everything, though. Here’s the longer sentence:\n\nsentence = \"I ate a cool ranch Dorito.\"\ncond_probs = get_sentence_unk_probs(sentence, bigram_unk_count, unigram_unk_count, n = 2)\ncond_surp = [-np.log2(x) for x in cond_probs]\ncond_strings = get_conditional_unk_strings(sentence, unigram_unk_count, n = 2)\n\n\n\n\nconditional\nprobability\nsurprisal\n\n\n\n\nP(I | <s>)\n0.1876\n2.4139\n\n\nP(ate | I)\n0.0007\n10.4712\n\n\nP(a | ate)\n0.2500\n2.0000\n\n\nP(cool | a)\n0.0000\n\n\n\nP(<UNK> | cool)\n0.0000\n\n\n\nP(<UNK> | <UNK>)\n0.0391\n4.6782\n\n\nP(. | <UNK>)\n0.0600\n4.0588\n\n\nP(</s> | .)\n0.9993\n0.0011\n\n\n\nThe problem here is that there is a known word, cool, that just happens never to occur in the bigrams (a, cool) or (cool, <UNK>). Maybe what we want is some way of assigning a small probability, of bigrams that could have happened, but didn’t.\n\nAdd 1 smoothing (Laplace Smoothing)\nThe first, simple idea, is to make a grid of all possible bigrams, and add 1 to all of their counts.\n\n\n\npython\n\n\nA function to get the add 1 smoothed conditional probability of a bigram\ndef get_conditional_prob_add1(x, bigram_count, unigram_count):\n  \"\"\"\n    for a tuple x, get the conditional probability of x[1] | x[0]\n  \"\"\"\n  if x in bigram_count:\n    cond = (bigram_count[x]+1) / (unigram_count[x[0:-1]] + len(unigram_count))\n  else:\n    cond = 1/ (unigram_count[x[0:-1]] + len(unigram_count))\n    \n  return(cond)\n\n\n\n\n\n\npython\n\n\nA function to get the conditional probability of every ngram in a sentence\ndef get_sentence_unk_probs_add1(sentence, bigram_count, unigram_count, n = 2):\n  \"\"\"\n    given a sentence, get its list of conditional probabilities\n  \"\"\"\n  sent_tokens = nltk.word_tokenize(sentence)\n  sent_tokens_unk = [x if (x,) in unigram_count else \"<UNK>\" for x in sent_tokens]\n  sent_pad = nltk.lm.preprocessing.pad_both_ends(sent_tokens_unk, n = n)\n  sent_ngram = nltk.ngrams(sent_pad, n = n)\n  sent_conditionals = [get_conditional_prob_add1(gram, bigram_count, unigram_count) \n                        for gram in sent_ngram]\n  return(sent_conditionals)\n\n\n\n\nsentence = \"I ate a cool ranch Dorito.\" \ncond_probs = get_sentence_unk_probs_add1(sentence, bigram_unk_count, unigram_unk_count, n = 2)\ncond_surp = [-np.log2(x) for x in cond_probs]\ncond_strings = get_conditional_unk_strings(sentence, unigram_unk_count, n = 2)\n\n\n\n\nconditional\nprobability\nsurprisal\n\n\n\n\nP(I | <s>)\n0.0797\n3.6498\n\n\nP(ate | I)\n0.0004\n11.1921\n\n\nP(a | ate)\n0.0005\n11.0307\n\n\nP(cool | a)\n0.0002\n12.4299\n\n\nP(<UNK> | cool)\n0.0002\n12.0300\n\n\nP(<UNK> | <UNK>)\n0.0180\n5.7941\n\n\nP(. | <UNK>)\n0.0276\n5.1784\n\n\nP(</s> | .)\n0.3912\n1.3539\n\n\n\n2 things to notice here:\n\nThere are no more zeros!\nThe probabilities are all different!\n\nThe probabilities jumped around because by adding 1 to every bigram count, we’ve given many bigrams a larger portion of the probability pie than they had before, and in a probability space, everything has to sum to 1. So that means we’ve also taken away a portion of the probability space from many bigrams.\n\n\n\n\nconditional\nbigram count\nw1 count\nadd 1 prob\nimplied counts\n\n\n\n\nP(I | <s>)\n577\n3,075\n0.0797\n244.9828\n\n\nP(ate | I)\n2\n2,839\n0.0004\n1.2134\n\n\nP(a | ate)\n1\n4\n0.0005\n0.0019\n\n\nP(cool | a)\n0\n1,338\n0.0002\n0.2425\n\n\nP(<UNK> | cool)\n0\n2\n0.0002\n0.0005\n\n\nP(<UNK> | <UNK>)\n138\n3,533\n0.0180\n63.6700\n\n\nP(. | <UNK>)\n212\n3,533\n0.0276\n97.5663\n\n\nP(</s> | .)\n2,686\n2,688\n0.3912\n1,051.6389\n\n\n\n\n\nAbsolute Discounting\nThe add 1 method effectively shaved off a little bit of probability from bigrams we did see to give it to bigrams we didn’t see. For example, we had 2 observations of (I, ate), but after redistributing probabilities, we’d effectively shaved off 0.79 observations. Things are even more extreme for other bigrams. Like (<s>, I) which got 323 observations shaved off, to redistribute to unseen bigrams.\nThe idea behind Absolute Discounting is instead of shaving variable amounts of probability off of every ngram, we instead shave off a fixed amount. The Greek letter \\(\\delta\\) is used to indicate this “shave off” amount.\nOur total number of observed bigrams, after <UNK>ifying, 36,744. If we shaved off 0.25 observations off of each bigram, that would give us \\(36,744\\times0.75=27,558\\) observations to spread around to the bigrams we didn’t observe. If we just did that uniformly, the unobserved bigrams would just get a sliver of that probability mass. There are 4,179 unigrams in our data, meaning we would expect there to be \\(4179^2=17,464,041\\) possible bigrams, that means there are \\(17,464,041-36,744 = 17,427,297\\) bigrams trying to get a piece of those 8,936 observations we just shaved off, coming out to just 0.0016 observations each.\nSome more clever approaches try not to distribute the probability surplus evenly, though. For example Kneser-Ney smoothing tries to distribute it proportionally to how often the \\(w_i\\) word in a \\((w_{i-1}w_i)\\) bigram appears as the second word in a bigram."
  },
  {
    "objectID": "lectures/ngram/01-ngram-eval.html#evaluating-ngram-models",
    "href": "lectures/ngram/01-ngram-eval.html#evaluating-ngram-models",
    "title": "ngrams - Perplexity",
    "section": "Evaluating NGram Models",
    "text": "Evaluating NGram Models\nNGram Models are often described in terms of their perplexity, which is a technical term from Information Theory. Rather than just dump the formula in here, let’s walk through it, since these information theoretic notions kind of keep coming up."
  },
  {
    "objectID": "lectures/ngram/01-ngram-eval.html#information---in-bits",
    "href": "lectures/ngram/01-ngram-eval.html#information---in-bits",
    "title": "ngrams - Perplexity",
    "section": "Information - In Bits",
    "text": "Information - In Bits\n\nFrom bits to probability - a light switch analogy\n\nOne light switch\nLet’s say we have a house with one room, and that one room has one light switch, and that light switch could be on  or off . Here’s a table of all of the possible lighting states of the house:\n\nLighting configurations, 1 switch\n\n\nLighting Config\nSwitch 1\n\n\n\n\non\n\n\n\noff\n\n\n\n\nWith just 1 switch, we have two possible lighting configurations. If each lighting configuration was equally possible, then the probability of seeing either lighting configuration is 0.5. In information theory terms, our switch is a “bit” (just like the computer bit), and you need 1 bit to represent a 50% probability.\n\\[\n1\\text{bit} = 2~\\text{states} = \\frac{1}{2} \\text{probability} = 0.5\n\\]\n\n\nTwo light switches\nNow, if our house had two rooms, (a living room and a kitchen), and each room had a switch, we can also workout how many different lighting configurations there are for the whole house.\n\nLighting configurations, 2 switches\n\n\n\n\n\n\n\nlighting configurations\nLiving Room Switch\nKitchen Switch\n\n\n\n\nliving room on, kitchen on\n\n\n\n\nliving room on, kitchen off\n\n\n\n\nliving room off, kitchen on\n\n\n\n\nliving room off, kitchen off\n\n\n\n\n\nWith 2 switches, we can describe 4 different lighting configurations in the house. And again, if every lighting configuration was equally likely, that means there is a \\(\\frac{1}{4}=0.25\\) probability we are describing with these two bits.\n\\[\n2\\text{bits} = 4~\\text{states}=\\frac{1}{4}=0.25\n\\]\n\n\nThree Light switches\nLet’s add 1 more room to the house (and then we’ll stop) that also has a switch. Here’s the table of house lighting configurations.\n\n\n\n\n\n\n\n\n\nlighting configuration\nLiving Room Switch\nKitchen Switch\nBedroom Switch\n\n\n\n\nliving room on, kitchen on, bedroom on\n\n\n\n\n\nliving room on, kitchen on, bedroom off\n\n\n\n\n\nliving room on, kitchen off, bedroom on\n\n\n\n\n\nliving room on, kitchen off, bedroom off\n\n\n\n\n\nliving room off, kitchen on, bedroom on\n\n\n\n\n\nliving room off, kitchen on, bedroom off\n\n\n\n\n\nliving room off, kitchen off, bedroom on\n\n\n\n\n\nliving room off, kitchen off, bedroom off\n\n\n\n\n\n\nWith 3 switches, we can describe 8 different house light configurations\n\\[\n3\\text{bits} = 8~\\text{states} = \\frac{1}{8} = 0.125\n\\]\n\n\nN Light switches\nThere is a general formula for figuring out how many states can be described by N bits, and therefore the probability of events they can represent.\n\\[\nN~\\text{bits} = 2^N ~\\text{states} = \\frac{1}{2^N}~\\text{probability}\n\\]\nThe number 2 got in there because that’s how many different options there are for each switch (on  or off )."
  },
  {
    "objectID": "lectures/ngram/01-ngram-eval.html#from-probability-to-bits-a.k.a.-surprisal",
    "href": "lectures/ngram/01-ngram-eval.html#from-probability-to-bits-a.k.a.-surprisal",
    "title": "ngrams - Perplexity",
    "section": "From probability to bits (a.k.a. “surprisal”)",
    "text": "From probability to bits (a.k.a. “surprisal”)\nOk, what if we didn’t know how many bits, or switches we had, but we knew the probability of something, and we want to know how many bits we need to represent that probability. For example, maybe we estimated the unigram probability of “the” in the novel Frankenstein.\n\n\n\n\n\n\nR\n\nfrank %>%\n  count(word) %>%\n  mutate(prob = n/sum(n)) %>%\n  filter(word == \"the\")\n\n\n# A tibble: 1 × 3\n  word      n   prob\n  <chr> <int>  <dbl>\n1 the    4194 0.0558\n\n\nWe’re in a classic math word problem: Solve for N\n\\[\n0.056 = \\frac{1}{2^N}\n\\]\nI’ve put the math steps to work this out in the collapsed Details block below, but to get N here, we need to take the negative \\(\\log_2\\) of the probability\n\n\\[\n\\frac{1}{2^N} = 2^{-N}\n\\]\n\\[\n\\log_2(2^{-N}) = -N\n\\]\n\\[\n-\\log_2(2^{-N}) = N\n\\]\n\n\\[\nN = -\\log_2(0.056) = 4.16\n\\]\nWe’ve obviously moved away from the concrete analogy of light switches, since it’s impossible to have 4 and 0.16 switches in your house. But this is a measure of how much information the probability takes up. It’s also often called surprisal as a technical term.\n\nWhy “Surprisal”\nImagine I came up to you and said:\n\nThe sun rose this morning.\n\nThat’s not especially informative or surprising, since the sun rises every morning. The sun rising in the morning is a very high probability event,1 so it’s not surprising it happens, and in the information theory world, we don’t need very many bits for it.\nOn the other hand, if someone came up to me and said:\n\nThe sun failed to rise this morning.\n\nThat is surprising! It’s also very informative. Thank you for telling me! I wasn’t expecting that! The smaller the probability of an event, the more surprising and informative it is if it happens, the larger the surprisal value is.\nHere’s a plot showing the relationship between the unigram probability of a word in Frankenstein, and its surprisal in bits."
  },
  {
    "objectID": "lectures/ngram/01-ngram-eval.html#expected-surprisal-a.k.a-entropy",
    "href": "lectures/ngram/01-ngram-eval.html#expected-surprisal-a.k.a-entropy",
    "title": "ngrams - Perplexity",
    "section": "Expected Surprisal, (a.k.a Entropy)",
    "text": "Expected Surprisal, (a.k.a Entropy)\nSo, we can calculate the probability of individual words in a book like Frankenstein, and from that probability, we can calculate each word’s surprisal.\nThe next thing to ask is what is the expected surprisal in a book like Frankenstein? As we’re reading the book, very common words will happen very often, and less common words will happen less often. What is our overall experience of reading the book like, in terms of surprisal? Here’s a table of some words that have a wide range of frequencies in the book.\n\n\n\nR\n\nfrank %>%\n  count(word) %>%\n  arrange(desc(n)) %>%\n  mutate(total = sum(n),\n         prob = n/sum(n),\n         surprisal = -log2(prob)) %>%\n  filter(word %in% c(\"the\", \"monster\", \"snow\", \"russia\"))\n\n\n# A tibble: 4 × 5\n  word        n total      prob surprisal\n  <chr>   <int> <int>     <dbl>     <dbl>\n1 the      4194 75143 0.0558         4.16\n2 monster    31 75143 0.000413      11.2 \n3 snow       16 75143 0.000213      12.2 \n4 russia      2 75143 0.0000266     15.2 \n\n\nWe could try just taking the average of the surprisal column to get the “average surprisal”, but that’s not quite right in terms of capturing the expected surprisal. Yes, words like snow and russia have a large surprisals, so they should drag our estimate upwards, but they don’t, by definition, happen all that often, so they shouldn’t drag it up too much.\nWhat we do instead is multiply the surprisal value of each word by its probability, and sum it up! This will capture our experience of the having a small surprisal and happening often, and words like snow having a large surprisal, but happening less often.\nThis “expected surprisal” is called entropy, and is often represented by \\(H(X)\\)\n\\[\n\\begin{array}{ll}\n\\text{surprisal:} & {s(x_i)=-\\log_2(p(x_i))}\\\\\n\\text{entropy:} & H(X) = \\sum_{i=1}^np(x_i)s(x_i)\n\\end{array}\n\\]\n\n\n\nR\n\nfrank %>%\n  count(word) %>%\n  arrange(desc(n)) %>%\n  mutate(prob = n/sum(n),\n         surprisal = -log2(prob)) %>%\n  summarise(entropy = sum(prob * surprisal))\n\n\n# A tibble: 1 × 1\n  entropy\n    <dbl>\n1    9.30\n\n\nSo, on average, while reading Frankenstein, (and only paying attention to unigram distribution), we have an expected surprisal (a.k.a, entropy) of \\(\\approx\\) 9.3 bits."
  },
  {
    "objectID": "lectures/ngram/01-ngram-eval.html#from-bits-back-to-states-a.k.a.-perplexity",
    "href": "lectures/ngram/01-ngram-eval.html#from-bits-back-to-states-a.k.a.-perplexity",
    "title": "ngrams - Perplexity",
    "section": "From bits back to states (a.k.a. Perplexity)",
    "text": "From bits back to states (a.k.a. Perplexity)\nNow there are just over 7,000 unique word types in Frankenstein.2\n\n\n\nR\n\nfrank %>%\n  count(word) %>%\n  nrow()\n\n\n[1] 7020\n\n\nBut because not every word is equally likely to show up, the expected surprisal, or entropy of the book is 9.3 bits. Just above, we saw that we can calculate the number of unique states we can encode with N bits with this formula:\n\\[\nN~\\text{bits} = 2^N~\\text{states}\n\\]\nIf we do this with the entropy value we got in bits, that would be\n\\[\nH(\\text{Frankenstein}) \\approx 9.3~\\text{bits}\n\\]\n\\[\n9.3\\text{bits} = 2^{9.3}~\\text{states} \\approx 630~\\text{states}\n\\]This is the estimated “perplexity” of the unigram model. Here’s how to think about it. If we already know the probability of every word that appears in the book, and we use that probability distribution to guess each next word in the book, it’s going to be as successful as trying to guess which next state is coming up out of 630 equally probable states."
  },
  {
    "objectID": "lectures/ngram/01-ngram-eval.html#perplexity-of-ngram-models",
    "href": "lectures/ngram/01-ngram-eval.html#perplexity-of-ngram-models",
    "title": "ngrams - Perplexity",
    "section": "Perplexity of ngram models",
    "text": "Perplexity of ngram models\nIn the first lecture on ngram models, we built a boring bigram model that looked like this.\n\n\n\n\n\n   \n\na\n\n a   \n\nbook\n\n book   \n\na->book\n\n  0.5   \n\ndog\n\n dog   \n\na->dog\n\n  0.5   \n\nEND\n\n END   \n\nbook->END\n\n  1   \n\ndog->END\n\n  1   \n\nI\n\n I   \n\nread\n\n read   \n\nI->read\n\n  0.25   \n\nsaw\n\n saw   \n\nI->saw\n\n  0.75   \n\nread->a\n\n  1   \n\nsaw->a\n\n  0.75   \n\nthe\n\n the   \n\nsaw->the\n\n  0.25   \n\nthe->dog\n\n  1   \n\nSTART\n\n START   \n\nSTART->I\n\n  0.8   \n\nWe\n\n We   \n\nSTART->We\n\n  0.2   \n\nWe->saw\n\n  1  \n\n\n\n\n\nAnd, we worked out that we could estimate the probability a new sentence like this:\n\nP(We | <START>) \\(\\times\\) P(saw | We) \\(\\times\\) P(the | saw) \\(\\times\\) P(dog | saw) \\(\\times\\) P(<END> | dog)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwords\na\nbook\ndog\nEND\nI\nread\nsaw\nthe\nWe\n\n\n\n\na\n0\n0.5\n0.5\n0\n0\n0\n0\n0\n0\n\n\nbook\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\ndog\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nI\n0\n0\n0\n0\n0\n0.25\n0.75\n0\n0\n\n\nread\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw\n0.75\n0\n0\n0\n0\n0\n0\n0.25\n0\n\n\nSTART\n0\n0\n0\n0\n0.8\n0\n0\n0\n0.2\n\n\nthe\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nWe\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n\n\\[\nP(X) = 0.2 \\times 1 \\times 1\\times 0.25 \\times 1 = 0.05\n\\]\nWith this probability, we can figure out how many total bits we need to encode this probability (a.k.a. the surprisal)\n\\[\ns(X) = -\\log_2(p(X)) = -\\log_2(0.05) = 4.32\n\\]\nNow, where evaluating ngram models diverge a bit from what we were doing before is we now figure out what the surprisial is per word, so we get bits per word (including the sentence ending tag).\n\\[\nspw(X)=\\frac{-\\log_2(p(X))}{N} = \\frac{-\\log_2(0.05)}{5} =0.86\n\\] To get the perplexity of this sentence, given the bigram model, we follow the formula for getting the number of states given a number of bits.\n\\[\npp(X) = 2^{spw(X)} = 2^{0.86} = 1.82\n\\]\n\nComparing models\nIf model A assigns higher probabilities to the sentences in a test set than model B, then model A will have a smaller perplexity.\nAnother way to think about the perplexity if ngram models, as Jurafsky & Martin point out, is that it’s the “weighted average branching factor”. Looking back at the bigram diagram above, it’s like saying “Any time you are at a node, you are, on average, choosing between 1.82 possible branches to go down.”"
  },
  {
    "objectID": "lectures/ngram/index.html",
    "href": "lectures/ngram/index.html",
    "title": "ngram Language Models",
    "section": "",
    "text": "I’m going to split up the “ngram model” materials into explaining how they work in principle, vs the how we have to make engineering decisions to make them work in reality."
  },
  {
    "objectID": "lectures/ngram/index.html#language-prediction",
    "href": "lectures/ngram/index.html#language-prediction",
    "title": "ngram Language Models",
    "section": "Language Prediction",
    "text": "Language Prediction\nWhen we are perceiving language, we are constantly and in real-time making predictions about what we are about to hear next. While we’re going to be talking about this in terms of predicting the next word, It’s been shown that we do this even partway through a word (Allopenna, Magnuson, and Tanenhaus 1998).\nSo, let’s say I spoke this much of a sentence to you:\n\nI could tell he was angry from the tone of his___\n\nAnd then a sudden noise obscured the final word, and you only caught part of it. Which of the following three words was I probably trying to say?\n\nboys\nchoice\nvoice\n\nYour ability to guess which word it was is based on your i) experience with English turns of phrase and ii) the information in the context.\nOne goal of Language Models is to assign probabilities across the vocabulary for what the next word will be, and hopefully assign higher probabilities to the “correct” answer than the “incorrect” answer. Applications for this kind of prediction range from speech-to-text (which could suffer from a very similar circumstance as the fictional one above) to autocomplete or spellcheck."
  },
  {
    "objectID": "lectures/ngram/index.html#using-context-ngrams",
    "href": "lectures/ngram/index.html#using-context-ngrams",
    "title": "ngram Language Models",
    "section": "Using context (ngrams)",
    "text": "Using context (ngrams)\nIn the example sentence above, one way we could go about trying to predict which word is most likely is to count up how many times the phrase “I could tell he was angry from the tone of his___” is finished by the candidate words. Here’s a table of google hits for the three possible phrases, as well as all hits for just the context phrase.\n\n\n\n“I could tell he was angry from the tone of his”\ncount\n\n\n\n\nboys\n0\n\n\nchoice\n0\n\n\nvoice\n3\n\n\n“I could tell he was angry from the tone of his”\n3\n\n\n\nWe’re going to start diving into mathematical formulas now (fortunately the numbers are easy right now).\nTo represent the count of a word or string of words in a corpus. We’ll use \\(C(\\text{word})\\). So given the table above we have\n\\[\n\\displaylines{C(\\text{I could tell he was angry from the tone of his})=3\\\\\nC(\\text{I could tell he was angry from the tone of his boys})=0\\\\\nC(\\text{I could tell he was angry from the tone of his choice})=0\\\\\nC(\\text{I could tell he was angry from the tone of his voice})=3}\n\\]\nTo describe the probability that the next word is “choice” given that we’ve already heard “I could tell he was angry from the tone of his”, we’ll use the notation \\(P(\\text{choice} | \\text{I could tell he was angry from the tone of his})\\). To calculate that probability, we’ll divide the total count of the whole phrase by the count of the preceding context.\n\\[\nP(\\text{choice} | \\text{I could tell he was angry from the tone of his}) = \\frac{C(\\text{I could tell he was angry by the tone of his choice})}{C(\\text{I could tell he was angry by the tone of his})} = \\frac{0}{3} = 0\n\\]\nIn fact, we can estimate the probability of an entire sentence with the Probability Chain Rule. The probability of a sequence of events like \\(P(X_1X_2X_3)\\) can be estimated by multiplying out their conditional probabilities like so:\n\\[\nP(X_1X_2X_3) = P(X_1)P(X_2|X_1)P(X_3|X_1X_2)\n\\]\nOr, to use a phrase as an example:1\n\\[\nP(\\text{du hast mich gefragt})=P(\\text{du})P(\\text{hast}|\\text{du})P(\\text{mich}|\\text{du hast})P(\\text{gefragt}|\\text{du hast mich})\n\\]\n\nData Sparsity rears its head\nThe problem with data sparsity rears its head, though. As we can already see in the table above, long phrases, although possible, might not appear in any corpus, giving us a very unreliable probability estimate.\nInstead of using the whole history, we can use a smaller context in a more strictly defined window. So, instead of looking at the whole sentence, what if we looked at counts of just “of his” from the example sentence.\n\n\n\n“of his”\ncount (in millions)\n\n\n\n\nboys\n2.2\n\n\nchoice\n14.2\n\n\nvoice\n44.5\n\n\n“of his”\n2,400.0\n\n\n\n\\[\n\\displaylines{\nP(\\text{boys} | \\text{of his}) = \\frac{C(\\text{of his boys)}}{C(\\text{of his})}=\\frac{2.2}{2400} = 0.0009\\\\\nP({\\text{choice}|\\text{of his}})= \\frac{C(\\text{of his choice)}}{C(\\text{of his})}=\\frac{14.2}{2400} = 0.005\\\\\nP({\\text{voice}|\\text{of his}})= \\frac{C(\\text{of his voice)}}{C(\\text{of his})}=\\frac{44.5}{2400} = 0.018}\n\\]\nThe continuation “voice” here is still relatively low probability, but has the highest probability of our candidate set.\nThis is the basic approach of an ngram model. Instead of using all available words to calculate the probability of the next word, we’ll approximate it with a smaller window. The example in the table above is a “trigram” model.\n\n\n\n\n\n\n“gram” names\n\n\n\n\nunigram:\n\nCounting up every individual (1) word, and try to estimate the probability of word in isolation.\n\nbigram:\n\nCount up every sequence of two words, and try to estimate the probability of a word given just one word before it,\n\ntrigram\n\nCount up every sequence of three words, and try to estimate the probability of a word given just the two words before it.\n\n\n“Trigrams” are the last n-gram with a special name. The rest are just called “4-gram” or “5-gram”.\n\n\n\n\n\n\n\nBuilding up a bigram model\nLet’s look at what happens as we gradually build up a bigram model we’ll start with one sentence.\nI saw the dog\n\n\n\n\n\n\n\n\n   \n\ndog\n\n dog   \n\nEND\n\n END   \n\ndog->END\n\n  1   \n\nI\n\n I   \n\nsaw\n\n saw   \n\nI->saw\n\n  1   \n\nthe\n\n the   \n\nsaw->the\n\n  1   \n\nthe->dog\n\n  1   \n\nSTART\n\n START   \n\nSTART->I\n\n  1  \n\n\n\n\n\nI saw the dog\nWe saw a dog\n\n\n\n\n\n\n\n\n   \n\na\n\n a   \n\ndog\n\n dog   \n\na->dog\n\n  1   \n\nEND\n\n END   \n\ndog->END\n\n  1   \n\nI\n\n I   \n\nsaw\n\n saw   \n\nI->saw\n\n  1   \n\nsaw->a\n\n  0.5   \n\nthe\n\n the   \n\nsaw->the\n\n  0.5   \n\nthe->dog\n\n  1   \n\nSTART\n\n START   \n\nSTART->I\n\n  0.5   \n\nWe\n\n We   \n\nSTART->We\n\n  0.5   \n\nWe->saw\n\n  1  \n\n\n\n\n\nI saw the dog\nWe saw a dog\nI read a book\n\n\n\n\n\n\n\n\n   \n\na\n\n a   \n\nbook\n\n book   \n\na->book\n\n  0.5   \n\ndog\n\n dog   \n\na->dog\n\n  0.5   \n\nEND\n\n END   \n\nbook->END\n\n  1   \n\ndog->END\n\n  1   \n\nI\n\n I   \n\nread\n\n read   \n\nI->read\n\n  0.5   \n\nsaw\n\n saw   \n\nI->saw\n\n  0.5   \n\nread->a\n\n  1   \n\nsaw->a\n\n  0.5   \n\nthe\n\n the   \n\nsaw->the\n\n  0.5   \n\nthe->dog\n\n  1   \n\nSTART\n\n START   \n\nSTART->I\n\n  0.67   \n\nWe\n\n We   \n\nSTART->We\n\n  0.33   \n\nWe->saw\n\n  1  \n\n\n\n\n\nI saw the dog\nWe saw a dog\nI read a book\nI saw a book\nI saw a dog\n\n\n\n\nFigure 1: Before and after update.\n\n\n\n\n\n\n\n   \n\na\n\n a   \n\nbook\n\n book   \n\na->book\n\n  0.5   \n\ndog\n\n dog   \n\na->dog\n\n  0.5   \n\nEND\n\n END   \n\nbook->END\n\n  1   \n\ndog->END\n\n  1   \n\nI\n\n I   \n\nread\n\n read   \n\nI->read\n\n  0.5   \n\nsaw\n\n saw   \n\nI->saw\n\n  0.5   \n\nread->a\n\n  1   \n\nsaw->a\n\n  0.5   \n\nthe\n\n the   \n\nsaw->the\n\n  0.5   \n\nthe->dog\n\n  1   \n\nSTART\n\n START   \n\nSTART->I\n\n  0.67   \n\nWe\n\n We   \n\nSTART->We\n\n  0.33   \n\nWe->saw\n\n  1  \n\n\n\n\n\n\n\n\n\n\n   \n\na\n\n a   \n\nbook\n\n book   \n\na->book\n\n  0.5   \n\ndog\n\n dog   \n\na->dog\n\n  0.5   \n\nEND\n\n END   \n\nbook->END\n\n  1   \n\ndog->END\n\n  1   \n\nI\n\n I   \n\nread\n\n read   \n\nI->read\n\n  0.25   \n\nsaw\n\n saw   \n\nI->saw\n\n  0.75   \n\nread->a\n\n  1   \n\nsaw->a\n\n  0.75   \n\nthe\n\n the   \n\nsaw->the\n\n  0.25   \n\nthe->dog\n\n  1   \n\nSTART\n\n START   \n\nSTART->I\n\n  0.8   \n\nWe\n\n We   \n\nSTART->We\n\n  0.2   \n\nWe->saw\n\n  1"
  },
  {
    "objectID": "lectures/ngram/index.html#the-probability-of-a-sentence",
    "href": "lectures/ngram/index.html#the-probability-of-a-sentence",
    "title": "ngram Language Models",
    "section": "The probability of a sentence",
    "text": "The probability of a sentence\nAnother way to visualize the final state diagram from above is with a matrix, with the “from” words along the rows and the “to” words along the columns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwords\na\nbook\ndog\nEND\nI\nread\nsaw\nthe\nWe\n\n\n\n\na\n0\n0.5\n0.5\n0\n0\n0\n0\n0\n0\n\n\nbook\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\ndog\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nI\n0\n0\n0\n0\n0\n0.25\n0.75\n0\n0\n\n\nread\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw\n0.75\n0\n0\n0\n0\n0\n0\n0.25\n0\n\n\nSTART\n0\n0\n0\n0\n0.8\n0\n0\n0\n0.2\n\n\nthe\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nWe\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\nThere is a non-zero number for every arrow in the state diagram. Every 0 value in the table represents a possible bigram that wasn’t observed (so, no arrow in the diagram).\nGiven these bigram probabilities we estimated from the corpus and our assumption that we can approximate the probability of whole sentences with smaller ngram probabilities, we can estimate the probability of a new sentence like so:\n\nWe saw the dog.\n\n\nP(We | <START>) \\(\\times\\) P(saw | We) \\(\\times\\) P(the | saw) \\(\\times\\) P(dog | saw) \\(\\times\\) P(<END> | dog)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwords\na\nbook\ndog\nEND\nI\nread\nsaw\nthe\nWe\n\n\n\n\na\n0\n0.5\n0.5\n0\n0\n0\n0\n0\n0\n\n\nbook\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\ndog\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nI\n0\n0\n0\n0\n0\n0.25\n0.75\n0\n0\n\n\nread\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw\n0.75\n0\n0\n0\n0\n0\n0\n0.25\n0\n\n\nSTART\n0\n0\n0\n0\n0.8\n0\n0\n0\n0.2\n\n\nthe\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nWe\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n\nWe can re-write the probability formula above like so:\n\\[\nP(s) = \\prod_{i=1}^n P(w_i|w_{i-1})\n\\]\nWe can also plug in the probabilities of these bigrams into the formula to get our estimated probility of the sentence.\n\\[\nP(s) = 0.2 \\times 1 \\times 1\\times 0.25 \\times 1 = 0.05\n\\]\n\n\n\n\n\n\nLog probabilities\n\n\n\nOnce you start multiplying probabilities, you’re going to get smaller and smaller numbers.\n\n\n\n\n\nthis\n\nequals\n\n\n\n\n0.5\n=\n0.500\n\n\n0.5 × 0.5\n=\n0.250\n\n\n0.5 × 0.5 × 0.5\n=\n0.125\n\n\n0.5 × 0.5 × 0.5 × 0.5\n=\n0.062\n\n\n0.5 × 0.5 × 0.5 × 0.5 × 0.5\n=\n0.031\n\n\n\n\n\nEven one very small probability (which you’ll get sometimes) can start sending the overall estimate into infintesimally small numbers close to 0, which computers may not be able to represent.\nSo, it’s also common to see the log-probability (a.k.a. the log-likelihood, in this case) being calculated instead. The way logarithms work, you add together values that you would have multiplied in the probability space.\n\\[\n\\log(P(\\text{We saw the dog}))=\\log(P(\\text{We | <START>})) +  \\log(P(\\text{saw | We}))+\\dots\n\\]\n\\[\n\\log(P(s)) = \\sum_{i=1}^n \\log(P(w_i|w_{i-1}))\n\\]\n\\[\n\\log(P(s)) = -1.609438 + 0 + 0 + -1.386294 + 0 = -2.995732\n\\]\n\n\n\nLarger ngrams\nLanguage models that take a larger window of adjacent words (3, or 4 grams) work in the same way, and are more “accurate” but are harder to visualize.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprev\na\nbook\ndog\nEND\nread\nsaw\nthe\n\n\n\n\na_a\n0\n0\n0\n0\n0\n0\n0\n\n\na_book\n0\n0\n0\n1\n0\n0\n0\n\n\na_dog\n0\n0\n0\n1\n0\n0\n0\n\n\na_I\n0\n0\n0\n0\n0\n0\n0\n\n\na_read\n0\n0\n0\n0\n0\n0\n0\n\n\na_saw\n0\n0\n0\n0\n0\n0\n0\n\n\na_the\n0\n0\n0\n0\n0\n0\n0\n\n\na_We\n0\n0\n0\n0\n0\n0\n0\n\n\nI_a\n0\n0\n0\n0\n0\n0\n0\n\n\nI_book\n0\n0\n0\n0\n0\n0\n0\n\n\nI_dog\n0\n0\n0\n0\n0\n0\n0\n\n\nI_I\n0\n0\n0\n0\n0\n0\n0\n\n\nI_read\n1\n0\n0\n0\n0\n0\n0\n\n\nI_saw\n0.666666666666667\n0\n0\n0\n0\n0\n0.333333333333333\n\n\nI_the\n0\n0\n0\n0\n0\n0\n0\n\n\nI_We\n0\n0\n0\n0\n0\n0\n0\n\n\nread_a\n0\n1\n0\n0\n0\n0\n0\n\n\nread_book\n0\n0\n0\n0\n0\n0\n0\n\n\nread_dog\n0\n0\n0\n0\n0\n0\n0\n\n\nread_I\n0\n0\n0\n0\n0\n0\n0\n\n\nread_read\n0\n0\n0\n0\n0\n0\n0\n\n\nread_saw\n0\n0\n0\n0\n0\n0\n0\n\n\nread_the\n0\n0\n0\n0\n0\n0\n0\n\n\nread_We\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw_a\n0\n0.333333333333333\n0.666666666666667\n0\n0\n0\n0\n\n\nsaw_book\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw_dog\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw_I\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw_read\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw_saw\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw_the\n0\n0\n1\n0\n0\n0\n0\n\n\nsaw_We\n0\n0\n0\n0\n0\n0\n0\n\n\nSTART_a\n0\n0\n0\n0\n0\n0\n0\n\n\nSTART_book\n0\n0\n0\n0\n0\n0\n0\n\n\nSTART_dog\n0\n0\n0\n0\n0\n0\n0\n\n\nSTART_I\n0\n0\n0\n0\n0.25\n0.75\n0\n\n\nSTART_read\n0\n0\n0\n0\n0\n0\n0\n\n\nSTART_saw\n0\n0\n0\n0\n0\n0\n0\n\n\nSTART_the\n0\n0\n0\n0\n0\n0\n0\n\n\nSTART_We\n0\n0\n0\n0\n0\n1\n0\n\n\nthe_a\n0\n0\n0\n0\n0\n0\n0\n\n\nthe_book\n0\n0\n0\n0\n0\n0\n0\n\n\nthe_dog\n0\n0\n0\n1\n0\n0\n0\n\n\nthe_I\n0\n0\n0\n0\n0\n0\n0\n\n\nthe_read\n0\n0\n0\n0\n0\n0\n0\n\n\nthe_saw\n0\n0\n0\n0\n0\n0\n0\n\n\nthe_the\n0\n0\n0\n0\n0\n0\n0\n\n\nthe_We\n0\n0\n0\n0\n0\n0\n0\n\n\nWe_a\n0\n0\n0\n0\n0\n0\n0\n\n\nWe_book\n0\n0\n0\n0\n0\n0\n0\n\n\nWe_dog\n0\n0\n0\n0\n0\n0\n0\n\n\nWe_I\n0\n0\n0\n0\n0\n0\n0\n\n\nWe_read\n0\n0\n0\n0\n0\n0\n0\n\n\nWe_saw\n1\n0\n0\n0\n0\n0\n0\n\n\nWe_the\n0\n0\n0\n0\n0\n0\n0\n\n\nWe_We\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "lectures/ngram/index.html#generating-text",
    "href": "lectures/ngram/index.html#generating-text",
    "title": "ngram Language Models",
    "section": "Generating text",
    "text": "Generating text\nOnce we’ve estimated all of these transition probabilities, we can turn them around to generate text, if we want. Let’s take the final bigram “model” we had from before:\n\n\n\n\n\n   \n\na\n\n a   \n\nbook\n\n book   \n\na->book\n\n  0.5   \n\ndog\n\n dog   \n\na->dog\n\n  0.5   \n\nEND\n\n END   \n\nbook->END\n\n  1   \n\ndog->END\n\n  1   \n\nI\n\n I   \n\nread\n\n read   \n\nI->read\n\n  0.25   \n\nsaw\n\n saw   \n\nI->saw\n\n  0.75   \n\nread->a\n\n  1   \n\nsaw->a\n\n  0.75   \n\nthe\n\n the   \n\nsaw->the\n\n  0.25   \n\nthe->dog\n\n  1   \n\nSTART\n\n START   \n\nSTART->I\n\n  0.8   \n\nWe\n\n We   \n\nSTART->We\n\n  0.2   \n\nWe->saw\n\n  1  \n\n\n\n\n\nIf we start at <START> and roll some dice, there’s a 80% chance we’ll move to I and a 20% chance we’ll move to We.\n\n<START>\n\n = 26\nWe\n\n = 67\nsaw\n\n = 67\na\n\n = 67\ndog\n\n = 23\n<END>\n\n\n\n\n\n\nOur bigram model on our boring corpus generates boring results. But here’s the output of a tigram model estimated over Frankenstein.\n\n\n\n\n\nDetails\n\nThe version of the data I’m working with here hasn’t been sentence-ized (so no <START> or <BEGIN> tags), and has also had punctuation stripped out (see function here). So after estimating the trigram probabilities, seed the generator by sampling over all trigrams, then progress by sampling over the distribution of \\(P(w_i|w_{i-2}w_{i-1})\\).\n\n\n\nTrigram R Code\nfrank %>%\n  select(word) %>%\n  mutate(nextw1 = lead(word),\n         nextw2 = lead(word, n = 2)) %>%\n  group_by(word, nextw1) %>%\n  mutate(n_tot = n()) %>%\n  group_by(word, nextw1, nextw2) %>%\n  mutate(n = n(),\n         p = n/n_tot) %>%\n  summarise(p = mean(p),\n            n = n())%>%\n  ungroup()->trigram\n\n\n\ngenerator R Code\n#set.seed(517)\ngenerate_frankenstein <- function(trigram, n = 100){\n  trigram %>%\n    mutate(all_p = n/sum(n)) %>%\n    sample_n(size = 1, weight = all_p)->init_df\n  \n  init_vec <- c(init_df$word, init_df$nextw1, init_df$nextw2)\n  for(i in seq(n)){\n    hist = rev(rev(init_vec)[1:2])\n    trigram %>%\n      filter(word == hist[1],\n             nextw1 == hist[2]) %>%\n      sample_n(size = 1, weight = p) %>%\n      pull(nextw2) -> new_w\n      init_vec <- c(init_vec, new_w)\n  }\n  return(str_c(init_vec, collapse = \" \"))\n}\n\ncat(paste0(\"> \", generate_frankenstein(trigram, n = 100)))\n\n\nmorning the moon was just rising from the trees or cut in stone that guided me and above all as belonging to a hell of intense tortures such as even dante could not sacrifice the whole kind but where is he why m clerval i was impatient to arrive at great proficiency in that study is certainly unlawful that is to assure as far as others have gone tracing a secure way over the chairs clapped my hands before my eyes which obliged me to heaven for nothing contributes so much time spent in observing my friends dread a dangerous relapse alas why"
  },
  {
    "objectID": "lectures/ngram/index.html#sparsity-again",
    "href": "lectures/ngram/index.html#sparsity-again",
    "title": "ngram Language Models",
    "section": "Sparsity, again",
    "text": "Sparsity, again\n\n\n\n\n\n\n\n\n\nThe rectangle represents a matrix, with the y-axis representing “from” words and the x-axis representing “to” words in Frankenstein. There could be a point in any location in the rectangle, representing a time that word \\(w_n\\) followed word \\(w_{n-1}\\). Each point represents a cell in that matrix where any data was observed."
  },
  {
    "objectID": "lectures/evaluation/index.html",
    "href": "lectures/evaluation/index.html",
    "title": "Evaluating models",
    "section": "",
    "text": "Language Models (including ngram models) are focused on “string prediction”, meaning we need to evaluate them like we would any predictive model. In both statistics and machine learning, there are some conventionalized approaches to this task that we can discuss in general."
  },
  {
    "objectID": "lectures/evaluation/index.html#training-vs-testing",
    "href": "lectures/evaluation/index.html#training-vs-testing",
    "title": "Evaluating models",
    "section": "Training vs testing",
    "text": "Training vs testing\nThe best way to evaluate a prediction model is to see how good its predictions are on some data that you already know what the predictions should be. The workflow, then, is\n\n“Train” the model on a training data set.\n“Test” the model on a test data set.\n\nNow, it’s rarely the case that people collect and curate a large training dataset to train a model, then go out and collect and curate a whole nother test dataset to test the model’s predictions. Instead, what they do is take their original dataset and split it into two pieces: “train” and “test”. Usually, “train” contains most of the data (80% to 90%), while “test” is a smaller held-out dataset (10% to 20% of the data).\n\nWhy not train and test on the whole dataset?\nWhen the model is “learning” how to make its predictions, the values it learns to make those predictions with will always be dependent on the training data. For example, compare the mini-bigram models below, one based on Frankenstein and the other based on Count of Monte Cristo.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitle\nword\nnext word\nn\n\n\n\n\nFrankenstein; Or, The Modern Prometheus\nthe\nsame\n62\n\n\nFrankenstein; Or, The Modern Prometheus\nthe\nmost\n56\n\n\nFrankenstein; Or, The Modern Prometheus\nthe\ncottage\n41\n\n\nFrankenstein; Or, The Modern Prometheus\nthe\nsun\n39\n\n\nFrankenstein; Or, The Modern Prometheus\nthe\nfirst\n35\n\n\n\n\n\n\n\n\n\n\ntitle\nword\nnext word\nn\n\n\n\n\nThe Count of Monte Cristo, Illustrated\nthe\ncount\n1107\n\n\nThe Count of Monte Cristo, Illustrated\nthe\nyoung\n530\n\n\nThe Count of Monte Cristo, Illustrated\nthe\nsame\n442\n\n\nThe Count of Monte Cristo, Illustrated\nthe\ndoor\n410\n\n\nThe Count of Monte Cristo, Illustrated\nthe\nfirst\n295\n\n\n\n\n\n\n\nIf we used the bigram model trained on Frankenstein to predict the most likely word to come after “the” on Frankenstein itself, we would do pretty well! But if we tried to use it to predict the most likely word to come after “the” in The Count of Monte Cristo, we’d do a lot worse! The highest frequency the w bigram in The Count of Monte Cristo (the count) doesn’t even appear in Frankenstein.\n\n\nBias vs Variance\nThis issue of the predictions of a model being too particularized to training data is related to the concept of the “Bias-Variance Tradeoff”.\n\nHigh Bias, Low Variance: One way to think of a high-Bias model is that it “over simplifies” the relationships in the data, but it also means that it will generalize to new data sets in a way that’s comparable to the training set.\n\nhigh-Bias model of morphology: All past tense is formed by adding -ed. This will produce some errors for verbs like run, and go, but will generalize well to new verbs, like yeet.\n\nLow Bias, High Variance: A high-Variance model captures much more detail to the relationships in the data, but that means it might be “overfit” on the training data.\n\nhigh-Variance model of morphology: You just have to memorize every present ➡️ past mapping verb by verb. This won’t any errors on verbs we already know like wait, run or go, but won’t generalize well to new verbs, like yeet.\n\n\n\n\nMeasuring error (or Loss)\nInevitably, the model we’re fitting will make predictions that are wrong, and then we need some way to quantify, or measure, how wrong the predictions were. The specific measure we use is going to depend a lot on the kind of prediction task we have, but some you might have already seen (or will see) are\n\nMean Squared Error, or MSE\nCross Entropy\nF(1) score\nAccuracy\nBLEU score\n\nOften the numbers these scores put out aren’t meaningful in and of themselves, but rather are used to compare models. But when you see a “score” or a “loss function” mentioned, understand it, generally, to be a measure of the difference between the values we expected in the test set, vs the values the model predicted."
  },
  {
    "objectID": "lectures/evaluation/index.html#a-linear-model-example",
    "href": "lectures/evaluation/index.html#a-linear-model-example",
    "title": "Evaluating models",
    "section": "A Linear Model example",
    "text": "A Linear Model example\nAs a brief example, let’s look at a data set of body measurements of penguins.\n\n\n\nArtwork by @allison_horst\n\n\n\nhead(palmerpenguins::penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\nnrow(palmerpenguins::penguins)\n\n[1] 344\n\n\nWe’re going to look at two models that try to predict the bill length of these penguins, one trying to predict it based on body mass, and the other based on bill depth.\n\nFigure 1: Body measurements of penguins\n\n\n\n\n\n\n(a) Bill length predicted by body mass\n\n\n\n\n\n\n\n(b) Bill length predicted by bill depth\n\n\n\n\n\n\nFirst, well split the data into train and test sets. I’m going to choose a random 80% of the data to be the training set, and use remaining 20% to be the test set.\n\nset.seed(517)\n\npenguins_id <- penguins %>% \n                  drop_na() %>%\n                  mutate(row_id = 1:n())\ntrain <- penguins_id %>%\n          slice_sample(prop = 0.8)\ntest <- penguins_id %>%\n          filter(!row_id %in% train$row_id)\nnrow(train)\n\n[1] 266\n\nnrow(test)\n\n[1] 67\n\n\nNow, I’ll fit two linear models with the training set. The linear model is saying “the predicted bill length for a given predictor (body mass or bill depth) is whatever value is on the line.”\n\nFigure 2: Models trained on train.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese are both “High Bias” models. Hardly any of the points are exactly on the lines.\nNow, let’s see how well these models perform on the held out test data.\\\n\n\n\n\nFigure 3: Model performance on the test set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s kind of hard to see from just the figures which model performed better on the test set. To evaluate the error, or loss, of these models on the test set, I’ll use the Mean Squared error, which is calculated like so:\n\nFor every data point, subtract the predicted value from the actual value, to get the difference between the prediction and reality.\nMultiply each difference by itself, a.k.a. square it.1\nGet the mean of these squared errors.\n\nIn math terms, let’s say\n\n\\(y_i\\) is the actual value of some data point.\n\\(\\hat{y}_i\\) is the predicted value\n\\(N\\) is the total number of data points.\n\nThen the mean squared error is\n\\[\n\\text{MSE} = \\frac{\\sum_{i=1}^N(y_i-\\hat{y}_i)^2}{N}\n\\]\nI’ll show you the actual code I’m using to get the MSE, in case it demystifies the mathematical formula.\n\ndiff_body        = test$bill_length_mm - test$body_pred\nsquare_diff_body = diff_body^2\nmse_body         = mean(square_diff_body)\nprint(mse_body)\n\n[1] 16.72451\n\ndiff_bill        = test$bill_length_mm - test$bill_pred\nsquare_diff_bill = diff_bill^2\nmse_bill         = mean(square_diff_bill)\nprint(mse_bill)\n\n[1] 26.83769\n\n\nIt looks like the model predicting bill length using body mass has a smaller MSE than the model predicting bill length using bill depth. So if we had to choose between these two models, we’d go with the body mass model."
  },
  {
    "objectID": "lectures/evaluation/index.html#precision-recall-and-f-measure",
    "href": "lectures/evaluation/index.html#precision-recall-and-f-measure",
    "title": "Evaluating models",
    "section": "Precision, Recall, and F-Measure",
    "text": "Precision, Recall, and F-Measure\nFor NLP tasks, we’re not usually trying to predict a continuous measure, but rather trying to categorize words, sentences, or documents. In these kinds of tasks, when we evalulate a model, we’ll often want to use measures known as “Precision”, “Recall” and “F-measure”.\nTo illustrate these measures, let’s say we’ve built a robot to pick raspberries, and we want it to pick all of the ripe berries (if we leave any behind, they’ll rot!) and none of the unripe berries (if we pick them before they’re ripe, we’ve missed out on future berries!). Let’s present the robot with this following scene that has 9 berries in the foreground. 5 of them are ripe and 4 of them are unripe.\n\n\n\n\n\n\nScenario 1: It picks all of the berries\nLet’s say the robot picked all of the berries.\n\n\n\nPicked\nUnpicked\n\n\n\n\nripe berry\n\n\n\nripe berry\n\n\n\nripe berry\n\n\n\nripe berry\n\n\n\nripe berry\n\n\n\nunripe berry\n\n\n\nunripe berry\n\n\n\nunripe berry\n\n\n\nunripe berry\n\n\n\n\nThis strategy has high “Recall”. There were a total of 5 ripe berries, and the robot picked all 5 of them.\n\\[\n\\text{recall} = \\frac{\\text{ripe picked berries}}{\\text{ripe berries}} = \\frac{5}{5} = 1\n\\] But, it has low “precision”. Of all the berries it picked, a lot of them were unripe.\n\\[\n\\text{precision} = \\frac{\\text{ripe picked berries}}{\\text{picked berries}} = \\frac{5}{9} = 0.55\n\\]\nThe “F-measure” or “F1 score” is a way to combine the precision and recall score into one overall performance score.\n\\[\nF_1 = 2\\frac{\\text{precision}\\times \\text{recall}}{\\text{precision} + \\text{recall}}=2\\frac{0.55\\times 1}{0.55 + 1}=2\\frac{0.55}{1.55} = 0.71\n\\]\n\n\nScenario 2: The robot picks only 1 ripe raspberry\n\n\n\nPicked\nUnpicked\n\n\n\n\nripe berry\n\n\n\n\nripe berry\n\n\n\nripe berry\n\n\n\nripe berry\n\n\n\nripe berry\n\n\n\nunripe berry\n\n\n\nunripe berry\n\n\n\nunripe berry\n\n\n\nunripe berry\n\n\n\nThis strategy has a very low recall. There are 5 ripe berries, and it has only picked 1 of them\n\\[\n\\text{recall} = \\frac{\\text{ripe picked berries}}{\\text{ripe berries}} = \\frac{1}{5} = 0.2\n\\] But, it has an extremely high precision. Of all of the berries it picked, it only picked ripe ones!\n\\[\n\\text{precision} = \\frac{\\text{ripe picked berries}}{\\text{picked berries}} = \\frac{1}{1} = 1\n\\] The very low precision winds up dragging down the overall F-measure. \\[\nF_1 = 2\\frac{\\text{precision}\\times \\text{recall}}{\\text{precision} + \\text{recall}}=2\\frac{1 \\times 0.2}{1 + 0.2}=2\\frac{0.2}{1.2} = 0.33\n\\]\n\n\nScenario 3: The robot alternates between picking and not picking\n\n\n\nPicked\nUnpicked\n\n\n\n\nripe berry\n\n\n\n\nripe berry\n\n\nripe berry\n\n\n\n\nripe berry\n\n\nripe berry\n\n\n\n\nunripe berry\n\n\nunripe berry\n\n\n\n\nunripe berry\n\n\nunripe berry\n\n\n\n\n\\[\n\\text{recall} = \\frac{\\text{ripe picked berries}}{\\text{ripe berries}} = \\frac{3}{5} = 0.6\n\\]\n\\[\n\\text{precision} = \\frac{\\text{ripe picked berries}}{\\text{picked berries}} = \\frac{3}{5} = 0.6\n\\]\n\\[\nF_1 = 2\\frac{\\text{precision}\\times \\text{recall}}{\\text{precision} + \\text{recall}}=2\\frac{0.6 \\times 0.6}{0.6 + 0.6}=2\\frac{0.36}{1.2} = 0.6\n\\]\n\n\nA non-berry example\nOne kind of NLP task is “Named Entity Recognition” (NER), or detecting and identifying the kind of named entities in a text. Here’s an example of spaCy doing that with a the sentence\n\nDr. Josef Fruehwald is teaching Lin517 at the University of Kentucky in the Fall 2022 semester.\n\n# python\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n\ntext = \"Dr. Josef Fruehwald is teaching Lin517 at the University of Kentucky in the Fall 2022 semester.\"\n\ndoc = nlp(text)\nprint(displacy.render(doc, style = 'ent'))\n\nDr.   Josef Fruehwald PERSON  is teaching  Lin517 GPE  at  the University of Kentucky ORG  in the Fall  2022 DATE  semester.\n\nWhen looking at the original sentence, we could think of each word, or span of words, as a berry, and the named entities as the ripe berries. The precision and recall here would be\n\\[\n\\text{precision}=\\frac{\\text{named entitites identified}}{\\text{identified words}}\n\\]\n\\[\n\\text{recall}=\\frac{\\text{named entities identified}}{\\text{named entities}}\n\\]\nIn this example, additional adjustments might need to be made for whether the span of the named entities is correct. 2022 is correctly a date, but maybe Fall should be included in its span. Also, it has identified Lin517 as a GeoPolitical Entity (GPE)."
  },
  {
    "objectID": "lectures/evaluation/index.html#edit-distance",
    "href": "lectures/evaluation/index.html#edit-distance",
    "title": "Evaluating models",
    "section": "Edit Distance",
    "text": "Edit Distance"
  },
  {
    "objectID": "lectures/what_is_nlp/index.html#nlp-in-computational-linguistics",
    "href": "lectures/what_is_nlp/index.html#nlp-in-computational-linguistics",
    "title": "What is NLP?(for this course)",
    "section": "NLP \\(\\in\\) Computational Linguistics",
    "text": "NLP \\(\\in\\) Computational Linguistics\nIn set notation, \\(\\in\\) means “is an element of”. That is, there’s a large set of things called “Computational Linguistics”, and NLP is a part of that larger set.\n“Computational Linguistics” covers a very broad range of topics. Natural Language Processing is currently an area of research and application that receives a lot of attention & money, but Computational Linguistics is a much broader umbrella term. The Association for Computational Linguistics defines it as\n\nComputational linguistics is the scientific study of language from a computational perspective. Computational linguists are interested in providing computational models of various kinds of linguistic phenomena. These models may be “knowledge-based” (“hand-crafted”) or “data-driven” (“statistical” or “empirical”). Work in computational linguistics is in some cases motivated from a scientific perspective in that one is trying to provide a computational explanation for a particular linguistic or psycholinguistic phenomenon; and in other cases the motivation may be more purely technological in that one wants to provide a working component of a speech or natural language system. Indeed, the work of computational linguists is incorporated into many working systems today, including speech recognition systems, text-to-speech synthesizers, automated voice response systems, web search engines, text editors, language instruction materials, to name just a few."
  },
  {
    "objectID": "lectures/what_is_nlp/index.html#some-examples-of-x-x-in-textcltextandx-ne-textnlp",
    "href": "lectures/what_is_nlp/index.html#some-examples-of-x-x-in-textcltextandx-ne-textnlp",
    "title": "What is NLP?(for this course)",
    "section": "Some examples of \\(\\{x | x \\in \\text{CL}~\\text{and}~x \\ne \\text{NLP}\\}\\)",
    "text": "Some examples of \\(\\{x | x \\in \\text{CL}~\\text{and}~x \\ne \\text{NLP}\\}\\)\nThis is set notation that means “the set of things, such that each thing is in Computational Linguistics, and the thing is not Natural Language Processing\n\nFormalizing Theory\nOne use of computational linguistics is to formalize linguistic theories into a computational framework. This might seem weird, since a lot of linguistic theory already looks very formal. But giving mathy looking definitions in a verbal description of a theory is a very different thing from implementing that theory in code that will run.\nSome examples are\n\nMinimalist Parsers (Berwick and Stabler 2019) implementing parsers for Minimalist Syntax\nThe Gradual Learning Algorithm (Boersma and Hayes 2001) implementing constraint re-ranking in Optimality Theory\nThe Tolerance Principle (Charles. Yang 2016), formalizing how learners might acquire rules that have exceptions.\n\nThe interesting thing with formalizing verbal theories, computationally, is that things that might seem like big differences in the verbal theories could turn out to be computationally identical, and some things that might not seem like a big difference can turn out to be massively different computationally.\n\n\nConceptual Experiments\nYou can use general computational principles to flesh out what you would expect to happen given under specific theories, or to use specific computational implementations of linguistic theory to explore their consequences.\nHere’s a little example from dialectology. We have two proposed principles:\n\nGarde’s Principle: Mergers are irreversible by linguistic means\n\nonce a community gets merger, like the cot/caught merger, it cannot get back the distinction\n\nHerzog’s Corollary: Mergers expand at the expense of distinctions.\n\nonce a community develops a merger, like the cot/caught merger, it will inevitably spread geographically to other communities\n\n\nWe can translate these two principles into a “finite state automaton” below.\n\n\n\n\n\n\n\nfinite_state_machine\n\n  \n\nd\n\n ɔ/ɑ distinction   \n\nd->d\n\n  0.90   \n\nm\n\n ɔ/ɑ merger   \n\nd->m\n\n  0.10   \n\nm->d\n\n  0.01   \n\nm->m\n\n  0.99   \n\ninit\n\n   \n\ninit->d\n\n   \n\n\n\n\n\nA verbal translation of this diagram would be\n\nWe start out in a state of distinguishing between /ɔ/ and /ɑ/. With each step in time (“generation”), we probably keep distinguishing between /ɔ/ and /ɑ/ with a 0.9 probability, but there’s some chance we become a merged community. Once we become a merged community, we are overwhelmingly likely to remain a merged community with a 0.99 probability. But there is a very little probability that we might go back to being merged at 0.01 probability.\n\nUnder these circumstances, are we inevitably going to become a merged community? How long until we reach the maximum probability of becoming a merged community? We can answer these questions with a conceptual experiment, converting the description and diagram above into a transition probability matrix, and then just doing a bunch of matrix multiplications.\n\n# python\nimport numpy as np\n\nd_change = np.array([0.90, 0.10])\nm_change = np.array([0.01, 0.99])\n\nchange_mat = np.row_stack((d_change, m_change))\nprint(change_mat)\n\n[[0.9  0.1 ]\n [0.01 0.99]]\n\n\n\n# python\ninitial_state = np.array((1,0))\nn_generations = 100\ncollector = [initial_state]\n\ncurrent_state = initial_state\nfor i in range(n_generations):\n  new_state = current_state @ change_mat\n  collector.append(new_state)\n  current_state = new_state\n  \nresults_mat = np.row_stack(collector)\n\n\n\n\n\n\nLooks like with the probabilities set up this way, we’re not guaranteed to become a merged community. The probability is very high (about 0.91), but not for certain. We might say, seeing this, that unless the Garde’s Principle is absolute (it’s impossible to undo a merger by any means) then Herzog’s Corollary won’t necessarily hold.\nOther examples of conceptual experiments are\n\nC. D. Yang (2000) used a model of variable grammar learning to see if he could predict which grammars (e.g. V2 vs no-V2) would win over time.\nSneller, Fruehwald, and Yang (2019) used the tolerance principle to see if a specific phonological change in Philadelphia could plausibly develop on its own, or if it had to be due to dialect contact.\nLinzen and Baroni (2021) used RNNs (a kind of neural network) to see if “garden path” sentences (e.g. “The horse raced past the barn fell.”1) were difficult just because the word at the pivot point was especially unlikely.\n\n\n\nAgent Based Modelling\nThis doesn’t always fall under the rubric of “computational linguistics,” but agent-based modelling involves programming virtual “agents” that then “interact” with each other. Part of what you program into the simulation is rules for how agents interact with each other, and what information they exchange or adopt when they do. It’s often used to model the effect of social network structure.\n\nDe Boer (2001) models vowel system acquisition and development over time.\nStanford and Kenny (2013) explore models of geographic spread of variation.\nKauhanen (2017) explores whether any linguistic variant needs to have an advantage over another in order to become the dominant form.\n\n\n\nBuilding and using computational tools and data\nOf course, there is a massive amount of effort that goes into constructing linguistic corpora, and developing computational tools to analyze those corpora."
  },
  {
    "objectID": "lectures/what_is_nlp/index.html#nlp",
    "href": "lectures/what_is_nlp/index.html#nlp",
    "title": "What is NLP?(for this course)",
    "section": "NLP",
    "text": "NLP\nFor this class, we’ll be mostly focusing on the “Language Modeling” component of NLP, and we’ll be following the definition of “Language Model” from Bender and Koller (2020) as a model trained to predict what string or word is most likely in the context of other words. For example, from the following sentence, can you guess the missing word?\n\nI could tell he was mad from the tone of his [____]\n\n\nUsing the predictions\nLanguage model predictions are really useful for many applications. For example, let’s say you built an autocaptioning system that took audio and processed it into a transcription. You might have a situation where the following sentence gets transcribed.\n\nPoker and blackjack are both  games people play at casinos.\n\nThe digital signal, , in this sentence is consistent with two possible words here\n\ncar\ncard\n\nUs humans here know that in the context of “poker”, “blackjack”, “games” and “casinos”, the more likely word is “card”, not “car.” But a simple model that’s just processing acoustics doesn’t know that. So to improve your captioning, you’d probably want to incorporate a language model that takes the context into account and boosts the probability of “card”.\nThis is just one example, but there are many other kinds of string prediction tasks, such as:\n\nGiven a string in language A, predict the string in language B (a.k.a. machine translation).\nGiven a whole paragraph, predict a summary of the paragraph (summarization).\nGiven a question, predict an answer (question answering).\nGiven a prompt, continue the text in the same style (text generation).\n\n\n\nUsing the representations\nIn the process of training models to do text generation, they develop internal representations of strings of text that can be useful for other purposes. For example, a common NLP task is “sentiment analysis,” that could be used to analyze, say, reviews of products online.2\nOne really very simplistic approach would be to get a dictionary of words that have been scored for their “positivity” and “negativity.” Then, every one of those words or a tweet or what ever has one of those words in it, you add its score as a total sentiment score.\n\n# R\nlibrary(tidytext)\nset.seed(101)\nget_sentiments(\"afinn\") %>%\n  sample_n(10) %>%\n  kable()\n\n\n\n\nword\nvalue\n\n\n\n\nlobby\n-2\n\n\nstricken\n-2\n\n\nloser\n-3\n\n\njealous\n-2\n\n\nbreakthrough\n3\n\n\ninability\n-2\n\n\nharshest\n-2\n\n\nranter\n-3\n\n\ncried\n-2\n\n\nwarfare\n-2\n\n\n\n\n\nHere’s an example with a notable tweet.\n\n# R\ntweet <- \"IF THE ZOO BANS ME FOR HOLLERING AT THE ANIMALS I WILL FACE GOD AND WALK BACKWARDS INTO HELL\"\ntweet_df <- tibble(word = tweet %>%\n                     tolower() %>%\n                     str_split(\" \") %>%\n                     simplify()) %>%\n  left_join(get_sentiments(\"afinn\")) %>%\n  replace_na(replace = list(value = 0))\n\n\nfull tweetsum\n\n\n\n# R\ntweet_df\n\n# A tibble: 19 × 2\n   word      value\n   <chr>     <dbl>\n 1 if            0\n 2 the           0\n 3 zoo           0\n 4 bans          0\n 5 me            0\n 6 for           0\n 7 hollering     0\n 8 at            0\n 9 the           0\n10 animals       0\n11 i             0\n12 will          0\n13 face          0\n14 god           1\n15 and           0\n16 walk          0\n17 backwards     0\n18 into          0\n19 hell         -4\n\n\n\n\n\n# R\ntweet_df %>%\n  summarise(sentiment = sum(value))\n\n# A tibble: 1 × 1\n  sentiment\n      <dbl>\n1        -3\n\n\n\n\n\nHowever, this is a kind of lackluster approach to sentiment analysis nowadays. Many language models now now, as a by product of their string prediction training, have more complex representations of words than just a score between -5 and 5, and have representations of whole strings that can be used (so it won’t give the same score to “good” and “not good”).\n\n# python\nfrom transformers import pipeline\n\n# warning, this will download approx\n# 1.3G of data.\nsentiment_analysis = pipeline(\"sentiment-analysis\",\n                              model=\"siebert/sentiment-roberta-large-english\")\n\n\n# python\nprint(sentiment_analysis(\"IF THE ZOO BANS ME FOR HOLLERING AT THE ANIMALS I WILL FACE GOD AND WALK BACKWARDS INTO HELL\"))\nprint(sentiment_analysis(\"This ain't bad!\"))\n\n[{'label': 'NEGATIVE', 'score': 0.9990140199661255}]\n[{'label': 'POSITIVE', 'score': 0.9944340586662292}]"
  },
  {
    "objectID": "lectures/neural_networks/index.html",
    "href": "lectures/neural_networks/index.html",
    "title": "Neural Nets",
    "section": "",
    "text": "The thing we’re going to be talking about in this lecture has had many names over the years, including\n\nParallel Distributed Processing\nConnectionism\nArtificial Neural Networks\nNeural Networks\nDeep Learning\nDeep Neural Networks\nAI\n\nThe basic idea is that we can re-weight and combine input data into a new array of features that can be used as predictors if output values."
  },
  {
    "objectID": "lectures/neural_networks/index.html#the-relationship-between-neural-nets-and-regression",
    "href": "lectures/neural_networks/index.html#the-relationship-between-neural-nets-and-regression",
    "title": "Neural Nets",
    "section": "The relationship between neural nets and regression",
    "text": "The relationship between neural nets and regression\nLet’s look at how a linear regression works one more time before we get into neural networks.\n\nimport numpy as np\nimport pandas as pd\nfrom palmerpenguins import load_penguins\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\npenguins = load_penguins()\n\n\npenguin_body = penguins.dropna()[\n                  [\"bill_length_mm\", \n                   \"bill_depth_mm\", \n                   \"flipper_length_mm\", \n                   \"body_mass_g\"]]\npenguin_body.head()\n\n   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0            39.1           18.7              181.0       3750.0\n1            39.5           17.4              186.0       3800.0\n2            40.3           18.0              195.0       3250.0\n4            36.7           19.3              193.0       3450.0\n5            39.3           20.6              190.0       3650.0\n\n\nLet’s say I wanted to use the bill measurements and flipper measurements to predict the penguin’s body mass. We can do this with a “linear regression.” The linear regression will give us:\n\nAn array of numbers we need to multiply each body measurement by and sum up\nA constant value to add to the result\n\nAnd the resulting number will be as close as we can get possible to the actual body mass of the penguin. The way it figures out the best numbers to multiply each body measurement by, and the constant value to add, is by comparing the results we get from that operation to the observed penguin body weight, and minimizing a loss function (mean squared error).\n\nlm = LinearRegression()\nX = penguin_body[[\"bill_length_mm\", \n                   \"bill_depth_mm\", \n                   \"flipper_length_mm\"]]\ny = penguin_body[\"body_mass_g\"]\n\n\nlinmodel = lm.fit(X, y)\n\n\nlinmodel.coef_\n\narray([ 3.29286254, 17.83639105, 50.76213167])\n\n\n\nlinmodel.intercept_\n\n-6445.476043030191\n\n\n\nmean_squared_error(linmodel.predict(X), y)\n\n152597.3327433967\n\n\nWe can get the predicted body mass of the first penguin by doing matrix multiplication and adding in the constant.\n\n(np.array(X)[0, :] @ linmodel.coef_) + linmodel.intercept_\n\n3204.761227286078\n\n\nAnd if we compare this to the actual penguin’s body mass, it’s not too far off!\n\ny[0]\n\n3750.0\n\n\n\nDiagramming the model\nWe can visualize this regression model with a nodes and arrows figure like this:\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n   \n\nbody_mass\n\n body_mass   \n\nA->body_mass\n\n    \n\nbill_length\n\n bill_length   \n\nbill_length->A\n\n  3.29   \n\nbill_depth\n\n bill_depth   \n\nbill_depth->A\n\n  17.84   \n\nflipper_length\n\n flipper_length   \n\nflipper_length->A\n\n  50.76   \n\nintercept\n\n intercept   \n\nintercept->body_mass\n\n  -6445.47  \n\n\n\n\n\nThese numbers from the model have the benefit of being interpretable. For example, every 1 millimeter a penguin’s bill length gets, we expect its body mass to also increase by 3.29 grams. If we had one reference penguin, and found another one that’s bill was 2 mm longer, but with flippers 1 mm shorter, we would expect its body mass to be (2*3.29) + (-1 * 50.76) = -44.18 grams less.\n\n\nCombining features\nBut what if the relationships between input data and output measurements was more complicated? Maybe the two separate bill measurements ought to be combined into one, more holistic “bill size” measurement first.\n\n\n\n\n\n\n\nG\n\n  \n\nbill_length\n\n bill_length   \n\nbill_combo\n\n bill_combo   \n\nbill_length->bill_combo\n\n    \n\nbill_depth\n\n bill_depth   \n\nbill_depth->bill_combo\n\n   \n\n\n\n\n\nAnd maybe we should have a “length of things” combined measure, and combine bill length and flipper length variable also.\n\n\n\n\n\n\n\nG\n\n  \n\nbill_length\n\n bill_length   \n\nlength_combo\n\n length_combo   \n\nbill_length->length_combo\n\n    \n\nflipper_length\n\n flipper_length   \n\nflipper_length->length_combo\n\n   \n\n\n\n\n\nIf we then used these synthetic variables in our model, it would now look like this:\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n   \n\nbody_mass\n\n body_mass   \n\nA->body_mass\n\n    \n\nbill_length\n\n bill_length   \n\nbill_combo\n\n bill_combo   \n\nbill_length->bill_combo\n\n    \n\nlength_combo\n\n length_combo   \n\nbill_length->length_combo\n\n    \n\nbill_combo->A\n\n    \n\nbill_depth\n\n bill_depth   \n\nbill_depth->bill_combo\n\n    \n\nlength_combo->A\n\n    \n\nflipper_length\n\n flipper_length   \n\nflipper_length->length_combo\n\n    \n\nbias1\n\n bias1   \n\nbias1->A\n\n    \n\nbias2\n\n bias2   \n\nbias2->body_mass\n\n   \n\n\n\n\n\nThis is now a kind of “neural network” model. In practice, the synthetic variables are not hand crafted like this. Instead, people set up a “hidden layer” with a set number of “nodes”, and then those nodes get fully connected to all of the inputs. No one knows how the input nodes are supposed to be synthesized into new variables, they just let the model fitting optimize for it."
  },
  {
    "objectID": "lectures/neural_networks/index.html#building-a-neural-net-classifier",
    "href": "lectures/neural_networks/index.html#building-a-neural-net-classifier",
    "title": "Neural Nets",
    "section": "Building a neural net classifier",
    "text": "Building a neural net classifier\nIn the next few steps, we’ll build up a neural net classifier, and walk through the kind of decisions that are commonly made in constructing these models.\n\nThe Data\nWe’ll be working with vowel formant data, the following code blocks\n\nLoad pandas, which will read in the data (the output of FAVE-extract on speaker s03 from the Buckeye corpus).\nLoad plotly, which I’ll use for making graphs\nReads in the data\nMakes a plot\n\n\n# Loads libraries\nimport pandas as pd\nimport plotly.express as px\n\n\n# reads in the tab delimited data\nvowels = pd.read_csv(\"data/s03.txt\", sep = \"\\t\")\nvowels = vowels[[\"plt_vclass\", \"ipa_vclass\", \"word\", \"F1\", \"F2\", \"F3\", \"dur\"]].dropna()\n\n\n# makes an F1 by F2 plot\nfig1 = px.scatter(vowels, \n                 x = \"F2\", \n                 y = \"F1\", \n                 color = \"ipa_vclass\",\n                 hover_data = [\"word\"])\nfoo = fig1.update_xaxes(autorange = \"reversed\")\nfoo = fig1.update_yaxes(autorange = \"reversed\")\nfig1.show()\n\n\n                        \n                                            \n\n\nTo keep things simpler, let’s try building a classifier that tries to identify which front vowel we’re looking at. Here’s how each vowel category appears in the dataset, along with its lexical set label, just to clear what vowels we’re looking at/\n\nVowels To Classify\n\n\nplt_vclass\nipa_vclass\nLexical Set\n\n\n\n\niy\ni\nFleece\n\n\ni\nɪ\nKit\n\n\ney\nej\nFace\n\n\ne\nɛ\nFace\n\n\nae\næ\nTrap\n\n\n\n\nfront_tf = vowels[\"plt_vclass\"].isin([\"iy\", \"i\", \"ey\", \"e\", \"ae\"])\nfront_vowels = vowels[front_tf]\n\n\n\nWhy a Neural Network?\nOne of the ideas behind a neural network model is that while these 5 categories may each be overlapping on each individual dimension, maybe through some combination of dimensions, they might be separable.\n\n\nCode\nhist_f1 = px.histogram(front_vowels,\n                       x = \"F1\",\n                       color = \"plt_vclass\",\n                       marginal = \"violin\",\n                       category_orders={\"plt_vclass\": [\"iy\", \"i\", \"ey\", \"e\", \"ae\"]},\n                       title = \"Vowels, along F1\")\nhist_f1.show()\n\n\n\n                        \n                                            \n\n\n\n\nCode\nhist_f2 = px.histogram(front_vowels,\n                       x = \"F2\",\n                       color = \"plt_vclass\",\n                       marginal = \"violin\",\n                       category_orders={\"plt_vclass\": [\"iy\", \"i\", \"ey\", \"e\", \"ae\"]},\n                       title = \"Vowels, along F2\")\nhist_f2.show()\n\n\n\n                        \n                                            \n\n\n\n\nCode\nhist_f3 = px.histogram(front_vowels,\n                       x = \"F3\",\n                       color = \"plt_vclass\",\n                       marginal = \"violin\",\n                       category_orders={\"plt_vclass\": [\"iy\", \"i\", \"ey\", \"e\", \"ae\"]},\n                       title = \"Vowels, along F3\")\nhist_f3.show()\n\n\n\n                        \n                                            \n\n\n\n\nCode\nhist_dur = px.histogram(front_vowels,\n                       x = \"dur\",\n                       color = \"plt_vclass\",\n                       marginal = \"violin\",\n                       category_orders={\"plt_vclass\": [\"iy\", \"i\", \"ey\", \"e\", \"ae\"]},\n                       title = \"Vowels, along duration\")\nhist_dur.show()\n\n\n\n                        \n                                            \n\n\n\n\nData Preparation\nBefore trying to train a neural network model, it’s common to “scale” or “normalize” the data. Just the formant data themselves have different ranges they cover, but the duration is on a much smaller scale.\n\nfront_vowels.agg(\n  {\"F1\" : [\"min\", \"mean\", \"max\", \"std\"],\n   \"F2\" : [\"min\", \"mean\", \"max\", \"std\"],\n   \"F3\" : [\"min\", \"mean\", \"max\", \"std\"],\n   \"dur\" : [\"min\", \"mean\", \"max\", \"std\"]}\n)\n\n              F1           F2           F3       dur\nmin   213.500000   754.800000  1473.300000  0.050000\nmean  471.265951  1679.375168  2550.123948  0.121439\nmax   949.400000  2646.900000  4362.500000  0.580000\nstd   103.716981   247.387841   321.782707  0.067917\n\n\nThis becomes even more obvious if we plot F3 and duration in the same plot, but constrain the axes so that they’re on similar scales.\n\n\nCode\nfig2 = px.scatter(front_vowels, \n                  x = \"F3\", \n                  y = \"dur\", \n                  color = \"plt_vclass\",\n                  category_orders={\"plt_vclass\": [\"iy\", \"i\", \"ey\", \"e\", \"ae\"]},\n                  hover_data = [\"word\"])\nfoo = fig2.update_xaxes(\n  constrain=\"domain\",  # meanwhile compresses the xaxis by decreasing its \"domain\"\n  )                 \nfoo= fig2.update_yaxes(\n    scaleanchor = \"x\",\n    scaleratio = 1,\n    constrain=\"domain\"\n)\nfig2.show()\n\n\n\n                        \n                                            \n\n\nThe neural network will need to estimate dramatically different weights to scale F3 and duration onto the same hidden layer. That’ll make it difficult, and maybe impossible, for it to get to optimal weights via gradient descent.\n\nScalers\n\nZ-scoring\nThe most common kind of scaling (so common it’s sometimes just called “standardization”) subtracts out the mean value and divides by the standard deviation. It’s also sometimes called “z-scoring”, and sometimes even “Lobanov normalization” by phoneticians.\n\nfrom sklearn.preprocessing import scale\n\n\nz_score_features = scale(front_vowels[[\"F1\", \"F2\", \"F3\", \"dur\"]], axis = 0)\n\n\n\nCode\nfig3 = px.scatter(x = z_score_features[:, 2], \n                  y = z_score_features[:, 3], \n                  color = front_vowels[\"plt_vclass\"],\n                  category_orders={\"plt_vclass\": [\"iy\", \"i\", \"ey\", \"e\", \"ae\"]})\nfoo = fig3.update_xaxes(\n  title = \"F3, zscored\",\n  constrain=\"domain\",  # meanwhile compresses the xaxis by decreasing its \"domain\"\n  )                 \nfoo = fig3.update_yaxes(\n    title = \"duration, zscored\",\n    scaleanchor = \"x\",\n    scaleratio = 1,\n    constrain=\"domain\"\n)\nfig3.show()\n\n\n\n                        \n                                            \n\n\n\n\nMinMax Scaling\nAnother kind of scaling you can do is “MinMax” scaling, which subtracts away the minimum value and divides by the maximum value, effectively converting everything to a proportion.\n\nfrom sklearn.preprocessing import minmax_scale\n\n\nminmax_score_features = minmax_scale(front_vowels[[\"F1\", \"F2\", \"F3\", \"dur\"]], axis = 0)\n\n\n\nCode\nfig4 = px.scatter(x = minmax_score_features[:, 2], \n                  y = minmax_score_features[:, 3], \n                  color = front_vowels[\"plt_vclass\"],\n                  category_orders={\"plt_vclass\": [\"iy\", \"i\", \"ey\", \"e\", \"ae\"]})\nfoo = fig4.update_xaxes(\n  title = \"F3, mimmax\",\n  constrain=\"domain\",  # meanwhile compresses the xaxis by decreasing its \"domain\"\n  )                 \nfoo = fig4.update_yaxes(\n    title = \"duration, minmax\",\n    scaleanchor = \"x\",\n    scaleratio = 1,\n    constrain=\"domain\"\n)\nfig4.show()\n\n\n\n                        \n                                            \n\n\n\n\n\n\nLabel Coding\nThe output layer of the neural network is going to have 5 nodes, one for each vowel, and the values in each node will be the probability that the vowel it’s classifying belongs to the specific vowel class. Let’s say the specific token was Face vowel. The output and the desired output might look like this:\n                      iy      i   ey      e     ae\nmodel output:    [ 0.025  0.025  0.9  0.025  0.025]\ndesired output   [ 0      0      1    0      0    ]\nIn fact, for every token, we need a different array of 0 and 1 values to compare against the output of the model.\niy   [ 1  0  0  0  0 ]\ni    [ 0  1  0  0  0 ]\ney   [ 0  0  1  0  0 ]\ne    [ 0  0  0  1  0 ]\nae   [ 0  0  0  0  1 ]\nThis kind of re-encoding of labels into arrays with 1 in just one position is called “One Hot Encoding”.\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n# The .reshape here converts an 1d array into a column vector.\nfront_vowel_labels = np.array(front_vowels[\"plt_vclass\"]).reshape(-1, 1)\n\n\none_hot = OneHotEncoder(sparse = False)\none_hot.fit_transform(front_vowel_labels)\n\narray([[1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       ...,\n       [0., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 1., 0., 0., 0.]])\n\n\nIn the example neural net model we’re going to fit, the library automatically does this one-hot encoding, but I’ve included the code here just to be explicit that it is happening.\nWe do have to convert these character labels to numeric codes, though.\n\nfrom sklearn.preprocessing import LabelEncoder\nv_to_int = LabelEncoder()\nvowel_integers = v_to_int.fit_transform(front_vowels[\"plt_vclass\"])\nvowel_integers\n\narray([0, 1, 1, ..., 3, 1, 1])\n\n\n\n\nTrain/test split\nWe’re going to split the data we have into a training and a test set. This step is especially important for neural networks, since they’re so good at remixing the data. On a data set like this, depending on how many states and layers your model has, it could effectively recode the 4 data dimensions into a unique value for each token, and then all it learns is “Token 1 is an iy. Token 2 an ae.” Having a heldout test set is important for making sure the model isn’t overfitting in this way.\nIn fact, some of the biggest pitfalls you can land in with these neural network models is by not constructing your test and train data carefully enough. In this example, we’re building a classifier based on just one speaker, but if we were using many speakers’ data for the training, we’d want to be careful to make sure that if a speaker’s data was in the training set, their data was not also in the test set. The model could get really good at categorizing vowel classes for speakers it was trained on, but then be very poor on speakers who weren’t in the training data. By allowing speakers’ data to be in both training and test, we would get an artificially high accuracy rate. This phenomenon is sometimes called “data leackage” or “leaky validation”.\n\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(z_score_features, \n                                                    vowel_integers,\n                                                    test_size = 0.2, \n                                                    random_state = 517)\n\n\n\nActivation Functions\nWe’re going to build this classifier model using tensorflow, but the code for that requires specifying “activation functions”, so we should cover that first.\nLet’s grab the first observation from the training set, and make up some random weights to be the weights to map the data onto one hidden node.\n\none_data = X_train[0, : ]\nfake_weights = np.random.random(4)\nfake_bias = np.random.random(1)\n\nfake_node_value = (one_data @ fake_weights) + fake_bias\nfake_node_value\n\narray([0.20705318])\n\n\nIn keeping with the “neuron” analogy of “neural” networks, we need to convert this number into some representation of whether or not this “neuron” is going to “fire”. This conversion process is called the “activation function,” and there are two that are commonly used in neural networks. The first is the sigmoid function that we’ve already discussed.\n\n\n\n\n\n\n\n\nIf the outcome of taking the dot product of input data and weights and adding the bias was 0, a node using the sigmoid activation function would pass 0.5 onto the next layer. It was 6, the node would pass on a value close to 1 to the next layer.\nAnother commonly used activation function is the “Rectified Linear Unit” or ReLU. This pass on whatever value comes in if it is greater than 0, and pass on 0 otherwise.\n\n\n\n\n\nOne reason why the ReLU has been preferred over the sigmoid for some purposes is that it can be more useful for gradient descent, for reasons that involve talking about more calculus than we can get into here.\n\n\nDeciding on the model\nLet’s build a model that has two hidden layers, each with 10 nodes that use the ReLU activation function\n\nimport tensorflow as tf\n\nHere’s how that looks in Tensorflow\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(10, input_shape=(4,), activation = \"relu\"),\n  tf.keras.layers.Dense(10, activation='relu'),\n  tf.keras.layers.Dense(5)\n])\n\nThe model is initialized with random weights, and we can preview the kind of outputs we get\n\nraw_output = model(X_train[0:3])\nraw_output\n\n<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\narray([[-0.07931734,  0.1286157 , -0.08446631, -0.36783975,  0.23257047],\n       [ 0.33786553,  0.45055553,  0.4057316 , -0.36589253,  0.48780265],\n       [-0.16296549,  0.36262435,  0.17686045, -0.05237741,  0.07849739]],\n      dtype=float32)>\n\n\nEach row is the number that winds up in the the output node. We can convert them to probabilities with softmax:\n\ntf.nn.softmax(raw_output)\n\n<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\narray([[0.18721801, 0.23048985, 0.18625651, 0.14029557, 0.25574008],\n       [0.20635614, 0.23097134, 0.22084688, 0.10208905, 0.23973657],\n       [0.15417339, 0.26077753, 0.21656783, 0.1722016 , 0.1962797 ]],\n      dtype=float32)>\n\n\nWe can also get the predicted category label with argmax/\n\npred = tf.math.argmax(raw_output, axis = 1)\npred.numpy()\n\narray([4, 4, 1])\n\n\nThese predicted category labels aren’t good predictions for the actual category labels.\n\ny_train[0:3]\n\narray([1, 4, 3])\n\n\n\n\nLoss Function\nNext, we need to decide on the loss function, or how we’re going to measure how badly our predictions match the data. Since we’re giving the model integer labels and getting back a list of un-scaled weights for each possible category, the appropriate loss function here is Sparse Categorical Cross Entropy\n\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n\nloss_fn(y_true=y_train[0:3], y_pred=raw_output)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=1.5516175>\n\n\n\n\nOptimizer\nNext, we need to decide on the “optimizer” we’re going to use. On every iteration, the model is going to estimate the gradient of the loss for every model parameter. The optimizer controls how that gradient is used. A popular one is the Adam optimizer.\n\noptim_fn = tf.keras.optimizers.Adam()\n\n\n\nTraining the Model!\n\nmodel.compile(optimizer=optim_fn,\n              loss=loss_fn,\n              metrics=['accuracy'])\n\n\nhistory = model.fit(X_train, y_train, \n                    epochs=500, verbose = 0,\n                    validation_split = 0.2)\n\n\ntraining_history = pd.DataFrame.from_dict(history.history)\n\n\nnrow = training_history.shape[0]\ntraining_history[\"epoch\"] = list(range(nrow))\n\n\nloss_plot = px.line(training_history, x = \"epoch\", y = [\"loss\", \"val_loss\"])\nloss_plot.show()\n\n\n                        \n                                            \n\n\n\nacc_plot = px.line(training_history, x = \"epoch\", y = [\"accuracy\", \"val_accuracy\"])\nacc_plot.show()\n\n\n                        \n                                            \n\n\n\ntest_pred = model(X_test)\ntest_pred_int = tf.math.argmax(test_pred, axis = 1).numpy()\n(test_pred_int == y_test).mean()\n\n0.6134453781512605"
  },
  {
    "objectID": "lectures/neural_networks/index.html#more-nodes",
    "href": "lectures/neural_networks/index.html#more-nodes",
    "title": "Neural Nets",
    "section": "More Nodes?",
    "text": "More Nodes?\n\nmodel2 = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(10, input_shape=(4,), activation = \"relu\"),\n  tf.keras.layers.Dense(10, activation='relu'),\n  tf.keras.layers.Dense(10, activation='relu'),\n  tf.keras.layers.Dense(5)\n])\n\n\nmodel2.compile(optimizer=optim_fn,\n              loss=loss_fn,\n              metrics=['accuracy'])\n\n\nhistory2 = model2.fit(X_train, y_train, \n                    epochs=500, verbose = 0,\n                    validation_split = 0.2)\n\n\ntraining_history2 = pd.DataFrame.from_dict(history2.history)\ntraining_history2[\"epoch\"] = range(500)\n\n\nacc_plot2 = px.line(training_history2, x = \"epoch\", y = [\"accuracy\", \"val_accuracy\"])\nacc_plot2.show()"
  },
  {
    "objectID": "lectures/gradient_descent/01_gradient_descent.html",
    "href": "lectures/gradient_descent/01_gradient_descent.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "What if wanted to convert inches to centimeters, but didn’t know that the formula is inches * 2.54? But what we did have was the following table of belt sizes from the Gap!\n\n\n\nWaist Size\nBelt Length (in)\nBelt Length (cm)\n\n\n\n\n28\n30.5\n77\n\n\n30\n32.5\n83\n\n\n32\n34.5\n88\n\n\n34\n36.5\n93\n\n\n36\n38.5\n98\n\n\n38\n40.5\n103\n\n\n40\n42.5\n108\n\n\n42\n44.5\n113\n\n\n44\n46.5\n118\n\n\n46\n48.5\n123\n\n\n\nWhat we could do is guess the multiplier, and see how wrong it is.\n\nimport numpy as np\n\n\nbelt_in = np.array([30.5, 32.5, 34.5, 36.5, 38.5, 40.5, 42.5, 44.5, 46.5, 48.5])\nbelt_cm = np.array([77, 83, 88, 93, 98, 103, 108, 113, 118, 123])\n\n\nmultiplier_guess = 1.5\ncm_guess = belt_in * multiplier_guess\n\n\n# If our guess was right, this should all be 0\ncm_guess - belt_cm\n\narray([-31.25, -34.25, -36.25, -38.25, -40.25, -42.25, -44.25, -46.25,\n       -48.25, -50.25])\n\n\nOur guess wasn’t a great guess. With this multiplier, our guesses are all too small. Let’s describe how bad our guess was with one number, and call it the “loss.” The usual loss function for data like this is the Mean Squared Error.\n\ndef mse(actual, guess):\n  \"\"\"\n    Given the actual target outcomes and the outcomes we guessed,\n    calculate the mean squared error.\n  \"\"\"\n  error = actual-guess\n  squared_error = np.power(error, 2)\n  mean_squared_error = np.mean(squared_error)\n  return(mean_squared_error)\n\n\nmse(belt_cm, cm_guess)\n\n1728.2125\n\n\nIf we made our multiplier guess a little closer to what it ought to be, though, our mean squared error, or loss, should get smaller.\n\nmultiplier_guess += 0.2\ncm_guess = belt_in * multiplier_guess\nmse(belt_cm, cm_guess)\n\n1128.2125\n\n\nOne thing we could try doing is make a long list of possible multipliers, and try them all to see which one has the smallest loss. This is also known as a “grid search”. I’ll have to re-write the loss function to calculate the loss for specific multipliers\n\n# This gives us 50 evenly spaced numbers between 0 and 50\npossible_mults = np.linspace(start = 0., stop = 5., num = 50)\n\ndef mse_loss(multiplier, inches, cm):\n  \"\"\"\n    given a multiplier, and a set of traning data,\n    (inches and their equivalent centimeters), return the \n    mean squared error obtained by using the given multiplier\n  \"\"\"\n  cm_guess = inches * multiplier\n  loss = mse(cm_guess, cm)\n  return(loss)\n\n\nlosses = np.array([mse_loss(m, belt_in, belt_cm) for m in possible_mults])\n\nIt’s probably best to visualize the relationship between the multiplier and the loss in a graph.\n\n\n\n\n\nIf we get the index of the smallest loss and get the associated multiplier, we can see that we’re not too far off!\n\npossible_mults[losses.argmin()]\n\n2.5510204081632653\n\n\n\n\nOne thing that is going to remain the same no matter how complicated the models get is the measure of how well they’ve done, or the loss, is going to get boiled down to one number. But in real modelling situations, or neural networks, the number of parameters is going to get huge. Here we have only one parameter, but if we had even just 5 parameters, and tried doing a grid search over 50 evenly spaced values of each parameter, the number of possible combinations of parameter values will get intractable.\n\nf\"{(5 ** 50):,}\"\n\n'88,817,841,970,012,523,233,890,533,447,265,625'\n\n\n\n\n\nLet’s look at the plot of our parameter vs the loss again:\n\n\n\n\n\nThere are a few really important features of this loss function:\n\nAs the estimate gets further away from the ideal value in either direction, the loss increases.\nThe increase is “monotonic”, meaning it’s not bumpy or sometime going up, sometimes going down.\nThe further away the guess gets from the optimal value, the steeper the “walls” of the curve get.\n\nLet’s say we were just these two point here, and we couldn’t “see” the whole curve, but we knew features 1 through 3 were true. With that in hand, and information about how the loss function is calculated, we can get the slope of the function at each point (indicated by the arrows).\n\n\n\n\n\nIf we were able to to update our parameter in a way that is proportional to the slope of the loss, then we would gradually get closer and closer to the optimal value. The updates would be very large at first, while the parameter values are far away from the optimal value, and then would start updating by smaller and smaller amounts as we home in on the optimal value because the slopes get shallower and shallower the closer we get.\nThe slope of the loss function at any given point is the gradient, and this process of gradually descending downwards is called gradient descent."
  },
  {
    "objectID": "lectures/gradient_descent/01_gradient_descent.html#gradient-descent",
    "href": "lectures/gradient_descent/01_gradient_descent.html#gradient-descent",
    "title": "Gradient Descent",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n“But Joe!” you exclaim, “How do you calculate the slope of the loss for a single point without seeing the whole distribution?”\nThe answer to that question used to be “with calculus.” But nowadays, people do it with “autograd” or “autodiff”, which basically means “we let the computer figure it out.” There isn’t autograd functionality in numpy, but there is in a closely related library called Jax, which is being developed by Google. Jax has a module called numpy which is designed to operate exactly the same way as numpy.\n\nimport jax.numpy as jnp\nfrom jax import grad\n\nI’m going to rewrite the inches to centimeter functions over again, this time making sure to use jax functions to ensure everything runs smoothly.\n\ndef inch_to_cm_jax(multiplier, inches):\n  \"\"\"\n    a function that converts inches to cm\n  \"\"\"\n  cm = jnp.dot(inches, multiplier)\n  return(cm)\n\ndef cm_loss_jax(multiplier, inches, cm):\n  \"\"\"\n    estimate the mismatch between the\n  \"\"\"\n  est = inch_to_cm_jax(multiplier, inches)\n  diff = est - cm\n  sq_err = jnp.power(diff, 2)\n  mean_sq_err = jnp.mean(sq_err)\n  return(mean_sq_err)\n\nThen we pass the new loss function to a jax function called grad() to create a new gradient function.\n\ncm_loss_grad_jax = grad(cm_loss_jax, argnums=0)\n\nWhere cm_loss_jax() will give use the mean-squared error for a specific multiplier, cm_loss_grad_jax() will give us the slope for that multiplier, automatically.\n\nprint(multiplier_guess)\n\n1.7\n\n\n\n# This is the mean-squared-error\nprint(cm_loss_jax(multiplier_guess, belt_in, belt_cm))\n\n1128.2124\n\n\n\n# This is the slope\nprint(cm_loss_grad_jax(multiplier_guess, belt_in, belt_cm))\n\n-2681.3499"
  },
  {
    "objectID": "lectures/gradient_descent/01_gradient_descent.html#learning-rates-and-epochs",
    "href": "lectures/gradient_descent/01_gradient_descent.html#learning-rates-and-epochs",
    "title": "Gradient Descent",
    "section": "Learning Rates and “Epochs”",
    "text": "Learning Rates and “Epochs”\nNow we can write a for-loop to iteratively update out multiplier guess, changing it just a little bit proportional to the gradient. There are two “hyper parameters” we need to choose here.\n\nThe “learning rate”. We can’t go adding the gradient itself to the multiplier. The gradient right now is in the thousands, and we’re trying to nudge 1.7 to 2.54. So, we pick a “learning rate”, which is just a very small decimal to multiply the gradient by before we add it to the parameter. I’ll say let’s start at 1/100,000\nThe number of “epochs.” We need to decide how many for loops we’re going to go through before we decide to call it and check on how the learning has gone. I’ll say let’s go for 1000.\n\n\nlearning_rate = 1/100_000\nepochs = 1000\n\n\n# I want to be able to plot everything after, so I'm going to create collectors.\nepoch_list    = []\nparam_list    = []\nloss_list     = []\ngradient_list = []\n\n\nmultiplier_guess = 0.\nfor i in range(epochs):\n  # append the current epoch\n  epoch_list.append(i)\n  # append the current guess\n  param_list.append(multiplier_guess)\n  \n  loss = cm_loss_jax(multiplier_guess, belt_in, belt_cm)\n  loss_list.append(loss)\n  gradient = cm_loss_grad_jax(multiplier_guess, belt_in, belt_cm)\n  gradient_list.append(gradient)\n  \n  multiplier_guess += -(gradient * learning_rate)\n\nprint(f\"The final guess was {multiplier_guess:.3f}\")\n\nThe final guess was 2.541"
  },
  {
    "objectID": "lectures/gradient_descent/01_gradient_descent.html#this-will-all-work-with-more-parameters",
    "href": "lectures/gradient_descent/01_gradient_descent.html#this-will-all-work-with-more-parameters",
    "title": "Gradient Descent",
    "section": "This will all work with more parameters",
    "text": "This will all work with more parameters\nLet’s try estimating the body mass of penguins from their bill length again.\n\nimport pandas as pd\nfrom palmerpenguins import load_penguins\n\n\npenguins = load_penguins()\n\nHere, I grab columns for the bill length and body mass as numpy arrays.\n\nbill_length = np.array(penguins.dropna()[\"bill_length_mm\"])\nbody_mass = np.array(penguins.dropna()[\"body_mass_g\"])\n\nAnd then I “normalize” the data, by subtracting the mean and dividing by the standard deviation. Understanding this part isn’t crucial. It’ll just make the parameter estimation go more smoothly.\n\nbill_length_z = (bill_length - bill_length.mean())/bill_length.std()\nbody_mass_z = (body_mass - body_mass.mean())/body_mass.std()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI’ll set up a prediction matrix with one row for each penguin and colum full of just 1s, and a column with the bill length data.\n\nbill_length_X = np.stack([np.ones(bill_length_z.size), bill_length_z], axis = 1)\nbill_length_X[0:10, ]\n\narray([[ 1.        , -0.89604189],\n       [ 1.        , -0.82278787],\n       [ 1.        , -0.67627982],\n       [ 1.        , -1.33556603],\n       [ 1.        , -0.85941488],\n       [ 1.        , -0.9326689 ],\n       [ 1.        , -0.87772838],\n       [ 1.        , -0.52977177],\n       [ 1.        , -0.98760942],\n       [ 1.        , -1.72014965]])\n\n\n\nThe return of Dot Products\nI’ve added this column of 1s so that we can have just one vector of parameters, the first value being the slope and the second being the intercept.\n\nfake_param = np.array([2, 4])\n\nNow we can do element-wise multiplication for the data for any given penguin…\n\nbill_length_X[1, ] * fake_param\n\narray([ 2.        , -3.29115147])\n\n\n…and then sum it up to get the estimated body mass for the penguin\n\n(bill_length_X[1,] * fake_param).sum()\n\n-1.2911514669828215\n\n\nA.K.A a Dot Product\n\nnp.dot(bill_length_X[1,], fake_param)\n\n-1.2911514669828215\n\n\n\nDot product with the whole matrix\nIn fact, we can get the estimated body mass for all penguins with just a sigle dot product.\n\nbody_mass_est = np.dot(bill_length_X, fake_param)\nbody_mass_est\n\narray([-1.58416756, -1.29115147, -0.70511928, -3.34226411, -1.43765951,\n       -1.73067561, -1.51091354, -0.1190871 , -1.95043767, -4.8805986 ,\n       -3.41551813, -1.87718365,  0.90646922, -5.02710664,  3.47036003,\n       -2.53646986, -2.60972388, -3.9282963 , -2.24345377, -1.80392963,\n       -4.36782043, -0.48535721, -0.55861124, -2.46321584, -0.55861124,\n       -1.29115147, -2.975994  , -1.29115147, -0.26559514, -3.56202618,\n       -1.51091354, -1.80392963,  0.68670715, -2.6829779 , -1.0713894 ,\n       -3.48877216, -0.33884917, -3.85504227,  2.07853359, -3.12250204,\n       -1.21789744, -0.1190871 , -3.85504227,  0.75996118, -1.21789744,\n       -0.85162733, -4.5875825 ,  0.54019911, -4.95385262,  0.10067497,\n       -1.65742158, -0.48535721, -3.48877216, -2.6829779 , -4.07480434,\n        0.02742095, -2.6829779 , -0.1190871 , -3.56202618,  0.24718302,\n       -4.22131239, -0.1190871 , -3.9282963 ,  0.39369106, -5.68639285,\n       -1.14464342, -1.21789744,  3.32385198, -4.22131239,  1.12623129,\n       -0.26559514, -2.975994  , -3.70853423,  0.61345313, -4.8805986 ,\n        1.19948532, -3.34226411, -4.51432848, -2.90273997,  0.02742095,\n       -3.6352802 , -3.19575607, -2.17019974, -1.73067561, -4.07480434,\n       -0.1190871 , -5.32012273, -1.21789744, -3.70853423, -0.33884917,\n       -2.31670779, -0.70511928, -5.97940894,  1.41924739, -4.5875825 ,\n       -0.19234112, -2.60972388, -2.53646986, -2.46321584, -1.14464342,\n       -1.95043767, -2.24345377, -2.31670779,  1.41924739, -2.31670779,\n        3.17734394, -1.14464342,  0.68670715, -1.21789744,  1.05297727,\n       -1.95043767, -2.90273997, -4.07480434, -0.1190871 , -3.70853423,\n       -2.60972388, -0.77837331,  0.10067497, -4.44107446, -0.48535721,\n       -1.80392963,  0.17392899, -1.65742158,  2.07853359, -2.0236917 ,\n        1.34599336, -3.26901009, -2.75623193, -2.31670779, -0.1190871 ,\n       -4.14805837, -0.77837331, -3.12250204, -1.14464342, -0.77837331,\n       -0.48535721, -6.71194917, -0.41210319, -2.90273997, -1.65742158,\n       -1.51091354, -3.41551813, -3.85504227, -2.53646986, -3.85504227,\n        0.17392899,  3.54361405,  6.40052095,  5.44821865,  6.40052095,\n        4.6424244 ,  3.83663014,  3.03083589,  3.98313819,  1.49250141,\n        4.05639221, -0.26559514,  5.66798072,  3.10408991,  5.22845658,\n        3.32385198,  5.88774279,  0.54019911,  5.81448877,  3.61686808,\n        5.44821865,  6.547029  ,  2.81107382,  3.83663014,  3.6901221 ,\n        1.19948532,  3.54361405,  4.78893244,  5.08194854,  6.40052095,\n        4.42266233,  1.12623129,  2.81107382, 13.43290716,  5.74123474,\n        5.22845658,  0.97972325,  2.29829566,  2.00527957,  5.44821865,\n        1.05297727,  6.10750486,  2.95758187,  6.10750486,  6.76679107,\n        1.71226348,  3.10408991,  6.76679107,  2.66456578,  2.88432785,\n        3.90988417,  5.30171061,  2.81107382,  6.47377497,  3.83663014,\n        2.7378198 ,  1.85877152,  3.10408991,  1.41924739,  6.69353704,\n        2.95758187,  3.61686808,  3.25059796,  9.55044394,  3.32385198,\n        6.2540129 ,  6.03425084,  1.63900945,  6.91329911,  4.71567842,\n        3.76337612,  5.08194854,  3.83663014,  3.76337612,  5.37496463,\n        4.56917038,  7.2063152 ,  2.88432785,  2.88432785,  5.74123474,\n        8.23187153,  4.49591635,  6.40052095,  2.66456578,  6.98655314,\n        1.56575543,  7.35282325,  4.56917038,  7.93885543,  4.56917038,\n        8.01210946,  3.10408991,  6.03425084,  2.37154968,  6.98655314,\n        5.96099681,  4.12964624,  5.22845658,  7.2063152 ,  5.30171061,\n       10.72250831,  4.34940831,  5.74123474,  4.05639221,  0.32043704,\n        8.89115773,  1.49250141,  5.00869451,  6.76679107,  6.2540129 ,\n        1.63900945,  7.4993313 ,  3.61686808, 10.13647613,  5.52147267,\n        4.34940831,  4.05639221,  6.69353704,  2.88432785,  6.32726693,\n        3.83663014,  6.40052095,  7.35282325,  3.03083589,  8.37837957,\n        2.88432785,  3.54361405,  7.35282325,  3.47036003,  7.35282325,\n        3.90988417,  7.64583934,  4.20290026,  7.86560141,  3.39710601,\n        6.76679107,  6.62028302, 12.26084279,  3.76337612,  5.81448877,\n        0.8332152 ,  5.30171061,  1.41924739,  6.84004509,  3.98313819,\n        7.86560141,  6.76679107,  6.03425084,  3.76337612,  8.4516336 ,\n       -0.26559514,  9.47718992,  0.90646922,  7.13306118,  6.18075888,\n        4.56917038,  4.6424244 ,  7.86560141,  4.12964624,  8.96441176,\n        5.66798072,  3.61686808,  7.05980716,  3.10408991,  7.05980716,\n        6.98655314,  6.47377497,  5.66798072,  7.4993313 ,  6.2540129 ,\n        5.00869451,  7.42607727,  3.25059796,  6.91329911,  0.90646922,\n        8.01210946,  2.88432785,  5.88774279,  6.547029  ,  3.17734394,\n        7.79234739,  4.05639221,  3.25059796, 10.64925429,  1.63900945,\n        6.10750486,  6.98655314,  6.547029  ])\n\n\nThis is our first foray into matrix multiplication.\n\n\n\nDoing linear regression with gradient descent\nIf we start off with some (bad) guesses for the slope and intercept, we can get the estimated body mass for every penguin:\n\nparam_guess = np.array([-2., 0.])\nmass_guess = np.dot(bill_length_X, param_guess)\n\nAnd then we can again get the mean squared error, or the loss, which is a single value describing how bad we’re doing at predicting body mass with this intercept and slope.\n\nnp.mean(np.power(body_mass_z - mass_guess, 2))\n\n5.0\n\n\n\nLoss function, now in two dimensions\nJust like we plotted the loss as it related to the single multiplier above, we can plot the loss as it relates to these two parameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd again, we’ve got a shape with a kind of curvature, and we can update both the intercept and the slope values incrementally with the negative of the slope of the curvature, to gradually arrive close to the best values.\nThese functions are basically the same as the single parameter case from above:\n\ndef fit_mass(params, X):\n  \"\"\"\n    Given some values and parameters\n    guess the outcome \n  \"\"\"\n  est = jnp.dot(X, params)\n  return(est)\n\ndef fit_loss(params, X, actual):\n  \"\"\"\n    Return the loss of the params\n  \"\"\"\n  est = fit_mass(params, X)\n  err = est - actual\n  sq_err = jnp.power(err, 2)\n  mse = jnp.mean(sq_err)\n  return(mse)\n\nfit_grad = grad(fit_loss, argnums=0)\n\nAnd this is the setup to run the for-loop that gradually updates our parameters.\n\nepoch_list    = []\nparam_list    = []\nloss_list     = []\ngradient_list = []\n\nparam_guess = np.array([-2., 0])\nlearning_rate = 0.01\n#vt = np.array([0., 0.])\n\nfor i in range(1000):\n  # append the current epoch\n  epoch_list.append(i)\n  param_list.append(param_guess)\n  loss = fit_loss(param_guess, bill_length_X, body_mass_z)\n  loss_list.append(loss)\n  gradient = fit_grad(param_guess,  bill_length_X, body_mass_z)\n  gradient_list.append(gradient)\n \n  param_guess += -(gradient * learning_rate)\n \nprint(f\"Final param guess was {param_guess}\")\n\nFinal param guess was [4.4573799e-08 5.8944976e-01]\n\n\nHere’s an animation of how the line we treat as the best estimate changes over training epochs"
  },
  {
    "objectID": "lectures/gradient_descent/01_gradient_descent.html#more-advanced-options-still-operate-on-the-same-principles",
    "href": "lectures/gradient_descent/01_gradient_descent.html#more-advanced-options-still-operate-on-the-same-principles",
    "title": "Gradient Descent",
    "section": "More advanced options still operate on the same principles",
    "text": "More advanced options still operate on the same principles\nIn all of the examples we’ve looked at here, we’ve done two things:\n\nCalculated the loss for all data points all at once.\nUpdated the parameters by multiplying the negative gradient by some small “learning parameter” number.\n\nThere are more complicated and nuanced ways to go about this process, but they all operate on the same principles.\n\n“Stochastic Gradient Descent”\nSometimes it’s not possible or is too computationally intensive to calculate the loss and its gradient for all data points in one go. There are a few ways of dealing with this, one of which is to chunk the data up unto randomized batches, and get the loss & gradient one batch at a time. This is called “Stochastic Gradient Descent”.\n\n\n“Optimizers”\nThere are also a whole array of gradient descent “optimizers”. Some of them gradually change the learning rate parameter. Others introduce the concept of “momentum” into the process. One of the most popular one I see people use when I’m reading blogs from neural network people is called Adam."
  },
  {
    "objectID": "lectures/matrix_multiplication/index.html",
    "href": "lectures/matrix_multiplication/index.html",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "In the lecture notes on gradient descent, we already saw the use of matrix multiplication used to do linear regression, and I framed it as a kind of extension of dot products. Let’s expand upon that."
  },
  {
    "objectID": "lectures/matrix_multiplication/index.html#what-is-matrix-multiplication-used-for",
    "href": "lectures/matrix_multiplication/index.html#what-is-matrix-multiplication-used-for",
    "title": "Matrix Multiplication",
    "section": "What is Matrix Multiplication used for?",
    "text": "What is Matrix Multiplication used for?\nMatrix multiplication is what makes almost everything work.\nfor thing in set_of_all_things:\n   if matrix_multiplication in thing:\n      works = True\n    else:\n      works = Maybe?\nIt’s so fundamental to so many areas of mathematics, statistics, machine learning, etc this is how mathematicians talk about trying to discover new matrix multiplication algorithms:\n\n“It’s hard to distinguish scientific thinking from wishful thinking,” said Chris Umans of the California Institute of Technology. “I want the exponent to be two because it’s beautiful.”\n“Exponent two” refers to the ideal speed — in terms of number of steps required — of performing one of the most fundamental operations in math: matrix multiplication. If exponent two is achievable, then it’s possible to carry out matrix multiplication as fast as physically possible. If it’s not, then we’re stuck in a world misfit to our dreams. —Quanta Magazine\n\nAny regression model, principle components analysis, neural network, word2vec model, automatic speech recognition, self driving car, and even this rotating map of Kentucky, all use matrix multiplication for their basic operation."
  },
  {
    "objectID": "lectures/matrix_multiplication/index.html#how-does-it-work",
    "href": "lectures/matrix_multiplication/index.html#how-does-it-work",
    "title": "Matrix Multiplication",
    "section": "How does it work?",
    "text": "How does it work?\nWe’ll be using numpy to do matrix multiplication, and we’ll revisit the task of converting heights in feet and inches to centimeters. To convert feet to centimeters, we need to multiply it by 30.48, and to convert inches to centimeters, we need to multiply them by 2.54. Let’s set up a column vector with these values\n\nimport numpy as np\n\n\nfoot_in_tocm = np.array([[30.48], \n                         [ 2.54]])\nfoot_in_tocm                         \n\narray([[30.48],\n       [ 2.54]])\n\n\nI’ve called it a “column” vector, because it a actually has two rows, and one column.\n\nfoot_in_tocm.shape\n\n(2, 1)\n\n\nAnd let’s set up a matrix of heights in feet and inches for an away mission with Jen Luc Picard, Lt. Data, and Deanna Troi. We’ll have one row per away team member, and one column for the foot component of their height and another for the inches component of their height.\n\nheights_ft = np.array([[5, 10],\n                       [5, 11],\n                       [5,  5]])\n\nIn order to convert each away team member’s height into centimeters, we need to\n\nMultiply their feet component by 30.48\nMultiply their inches component by 2.54\nSum the result.\n\na.k.a a dot product. We’ve got heights as rows…\n\nheights_ft[0, ]\n\narray([ 5, 10])\n\n\n… and the conversion factors as columns…\n\nfoot_in_tocm\n\narray([[30.48],\n       [ 2.54]])\n\n\nBut this is the usual organization of values for doing a dot product. You take the values across columns of matrix A, multiply them by the values across rows in matrix B, and sum up the result.\n\nnp.dot(heights_ft[0, ], foot_in_tocm)\n\narray([177.8])\n\n\nSo, Jean Luc Picard is 177.8 centimeters tall. If we take the dot product of the whole heights_ft matrix, without doing any indexing, and foot_in_tocm we’ll get back a column vector of every away team member’s height.\n\nnp.dot(heights_ft, foot_in_tocm)\n\narray([[177.8 ],\n       [180.34],\n       [165.1 ]])\n\n\nThere’s also a convenience operator built in to do matrix multiplication: @\n\nheights_ft @ foot_in_tocm\n\narray([[177.8 ],\n       [180.34],\n       [165.1 ]])\n\n\n\nWhat just happened.\nMatrix multiplication goes rowwise down the first matrix, and pulls out a row. Then it takes the dot product of that row and the first column of the second matrix. Then, the result gets saved as the first row, first column of the result matrix.\n\n\n\n\n\n\n\nSome rules for matrix multiplication\nMatrix multiplication isn’t “commutative”. That is to say, while heights_ft @ foot_in_tocm works, if we flipped the arguments to foot_in_tocm @ heights_ft, we’ll get an error.\n\nfoot_in_tocm @ heights_ft\n\nError in py_call_impl(callable, dots$args, dots$keywords): ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)\n\n\nThis is because the number of columns in the first matrix always need to match the number of rows in the second matrix. You can kind of visualize this requirement by lining up the matrix .shapes like this:\n✅ Good!\nMatrix A shape    (3,   2)\nMatrix B Shape         (2,   1)\n\n🚫 Bad!\nMatrix B shape   (2,   1)\nMatrix A shpe         (2,   3)\nThe resulting matrix will have the same number of rows from matrix A and the same number of columns as matrix B, which we can also visualize like this:\nMatrix A shape    (3,   2)\nMatrix B Shape         (2,   1)\nResult Shape      (3,        1)\n\n\nA more interesting conversion matrix\nTo convert feet and inches in to centimeters, we created a matrix with a two rows, and one column. Let’s say we wanted to simultaneously convert the crew’s heights into meters and into yards. We just need to add columns to the conversion matrix with what we need to multiply feet and inches by for these other measurement units.\n\n##                          cm      m      yd                         \nfoot_in_other = np.array([[30.48, 0.3048, 1/3],   # foot multiplier\n                          [2.54,  0.0254, 1/36]]) # inch multiplier\n\nThen, we matrix multiply the\n\nheights_ft @ foot_in_other\n\narray([[177.8       ,   1.778     ,   1.94444444],\n       [180.34      ,   1.8034    ,   1.97222222],\n       [165.1       ,   1.651     ,   1.80555556]])"
  },
  {
    "objectID": "lectures/matrix_multiplication/index.html#the-way-this-relates-to-neural-networks",
    "href": "lectures/matrix_multiplication/index.html#the-way-this-relates-to-neural-networks",
    "title": "Matrix Multiplication",
    "section": "The way this relates to neural networks",
    "text": "The way this relates to neural networks\nLet’s start with an imaginary task of trying to guess which species a penguin is, based on one of 4 body measurements. This code loads the penguins data set and pulls out a matrix of the body length measures, and a vector of species.\n\n\n\n\n\n\nNote\n\n\n\nThe peng dataset is in a format called a pandas dataframe that we haven’t gotten a chance to learn about yet.\n\n\n\nfrom palmerpenguins import load_penguins\npeng = load_penguins()\n\n\nbody_meas = peng[[\"bill_length_mm\", \"bill_depth_mm\", \n                  \"flipper_length_mm\", \"body_mass_g\"]].\\\n                  dropna()\nspecies = peng.dropna()[\"species\"]\n\nLet’s pull out one row of body measurements for an individual penguin\n\none_penguin = np.array(body_meas)[0,:]\none_penguin\n\narray([  39.1,   18.7,  181. , 3750. ])\n\n\nWith our neural network, we’ll want to use these 4 Input values to try to predict which one of 3 species this individual penguin is, ad we’ll try to do that with a “hidden” layer of 10 nodes. That kind of model is conventionally represented like so:\n\n\n\n\n\nTo get the value that’s supposed to be in the first hidden layer node, we multiply each of the 4 body measurement by some number, then sum them together.\n\n\n\n\n\nThat should be starting to sound like a familiar process! To do that for a single node, we just do a dot product, but to repeat it simultaneously for all hidden layer nodes, we need a matrix of weights to do matrix multiplication with. To work out the shape of the matrix we need, we can go back to thinking about how the shapes of the matrices we multiply relate to the outputs\nInput shape      (1,     4)\nweights shape?          (?,     ?)\nhidden shape     (1,           10)\nIt looks like we need a 4 by 10 matrix to convert the 4 input data dimensions into 10 hidden dimensions.\n\nto_hidden_layer = np.random.random((4, 10))\nhidden_layer = one_penguin @ to_hidden_layer\nhidden_layer\n\narray([1680.05479008, 1044.57772039,  844.75218032,  173.76123438,\n       3648.58188585, 3714.92000402,  240.29638976,  418.92577036,\n       1545.41450822, 2795.51178638])\n\n\nThen, we need translate the values for these 10 hidden nodes into 3 values for the output nodes.\nhidden shape      (1,   10)\nweights shape?         ( ?    ?)\noutput shape      (1,         3)\nLooks like we’ll need a 10 by 3 matrix.\n\nto_output = np.random.random((10, 3))\noutput = hidden_layer @ to_output\noutput\n\narray([7768.61330139, 8201.82309701, 7851.91440961])\n\n\nAnd then, we’d probably convert the output to probabilities using softmax, and whichever node has the largest probability we choose as the species.\n\nThings to note\nFirst, a nifty thing is that the matrices we created to convert one penguin’s data to 3 output nodes will work just as well converting an arbitrary number of penguin’s data to output nodes.\ninput            (N,  4)\nhidden weights       (4, 10)\nhidden nodes     (N,     10)\noutput weights          (10, 3)\noutput           (N,          3)\n\nnp.array(body_meas)[0:6, ] @ to_hidden_layer @ to_output\n\narray([[7768.61330139, 8201.82309701, 7851.91440961],\n       [7874.20887775, 8312.9173626 , 7958.17552609],\n       [6826.68067025, 7191.24211381, 6881.39217186],\n       [7204.24658538, 7597.0022168 , 7269.85071435],\n       [7595.77456845, 8013.83171594, 7670.35667202],\n       [7523.75929824, 7940.82374454, 7601.66414793]])\n\n\nThe second thing to note is that these numbers we just got, based on random weights in the matrices, are obviously bad. But, they can be improved with gradient descent!"
  }
]
[
  {
    "objectID": "resources/reading/index.html",
    "href": "resources/reading/index.html",
    "title": "Reading a Technical Paper",
    "section": "",
    "text": "There may never be a point in your academic life where you will read a paper and understand everything in it (or if there is, I haven’t gotten there). Instead, you have to develop methods for getting whatever information you can out of a paper itself, and then draw up a list of terms and concepts to do further background research on.\nLet’s work through a sample paragraph from Bender et al. (2021) to see some of these strategies in action."
  },
  {
    "objectID": "resources/reading/index.html#a-tricky-paragraph",
    "href": "resources/reading/index.html#a-tricky-paragraph",
    "title": "Reading a Technical Paper",
    "section": "A Tricky Paragraph",
    "text": "A Tricky Paragraph\nBender et al. (2021) is an important paper about ethics and safety concerns in natural language processing. However, it could be hard to follow some of the discussion without a background in the NLP literature. Here’s a sample paragraph where they discuss specific advancements made in NLP models.\n\nThe next big step was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g. 82].\n\nMy own approach to trying to understand paragraphs (and whole papers) like this, where I might not be familiar with all of the concepts involved, is ask the following questions:\n\nCan I figure out the upshot? What message is this paragraph trying to communicate?\nCan I pick out any themes? What keeps getting repeated?\nWhat search terms can I pull out of the paper to do more background research."
  },
  {
    "objectID": "resources/reading/index.html#can-we-figure-out-the-upshot",
    "href": "resources/reading/index.html#can-we-figure-out-the-upshot",
    "title": "Reading a Technical Paper",
    "section": "Can we figure out the upshot?",
    "text": "Can we figure out the upshot?\nThis paragraph is trying to tell us something. I’ve color coded the pieces of the paragraph which I think are most useful for figuring out the point of this paragraph even if you don’t know what all the specifics mean.\n\nThe next big step1 was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art2 performance on question answering, textual entailment, semaantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well3. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary4 for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed5 to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score6 in 10 epochs as opposed to 4867 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data8. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g. 82].1 “Big steps”, improvement2 “state of the art”, (SOTA) means the best it can be3 “at first … and later” from limited applications, it expanded4 less “labeled data” needed.5 Less “training data” needed.6 Whatever the “F1 score” is, this got the best one7 Whatever an “epoch” is, this needed less of them8 The same score, but less data.\n\n\nThe upshot\n~Things~ got better with less."
  },
  {
    "objectID": "resources/reading/index.html#can-we-figure-out-themes-from-repetition",
    "href": "resources/reading/index.html#can-we-figure-out-themes-from-repetition",
    "title": "Reading a Technical Paper",
    "section": "Can we figure out themes from repetition?",
    "text": "Can we figure out themes from repetition?\nI’ve highlighted the words whose repetition struck me as being important to the theme of this paragraph.\n\nThe next big step was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g. 82].\n\nIt seems clear “training” is a big deal, and it involves data."
  },
  {
    "objectID": "resources/reading/index.html#any-search-terms-for-background-research",
    "href": "resources/reading/index.html#any-search-terms-for-background-research",
    "title": "Reading a Technical Paper",
    "section": "Any search terms for background research?",
    "text": "Any search terms for background research?\nThere are a lot of technical terms in this paragraph, but it seems like they can be classified under a few main categories.\n\nThe next big step was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While training the word embeddingsrequired a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g. 82].\n\nThere’s at least three kinds of things we could try doing some background research on here:\n\n“Models”\nThere are a few models named here. Some seem like generic names, and others seem more specific. The most generic “word embeddings” is actually defined in the paragraph\n\nrepresentations of the distribution of words\n\nThere’s probably a lot more to investigate here.\nAfter defining “word embeddings,” they say “these word vectors”, which seems to suggest that these are synonymous or at least highly interchangeable concepts.\nSpecific “systems” that generate “word vectors” are\n\nword2vec\nGloVe\n\nSome good search terms to find information about these would probably be “word2vec word vectors” or “GloVe word vectors”.\nNext they name another class of model, “LSTM models,” that generate “word vectors”, and name some specific LSTM modelsYou can also get an idea of naming conventions in this field. The name formats seem to either be thing2thing or an acronym that is a pronounceable word… and hard to search for all on its own.\n\ncontext2vec\nELMo\n\nSome good search terms here would probably be “LSTM context2vec” or “LSTM ELMo”.\n\n\n“NLP Tasks”\nAfter saying that these models are used on “NLP tasks”, they name a few specific ones. Some of them just have names, while others also have an acronym associated with them.\n\nquestion answering\ntextual entailment\nsemantic role labeling (SRL)\ncoreference resolution\nnamed entity recognition (NER)\nsentiment analysis\n\nSome of these tasks wouldn’t make for great search terms on their own, like “question answering,” but appending “NLP” to the beginning for “NLP question answering” would probably work.\n\n\nScores\nIn this paragraph, only one kind of score, the “F1 score” is mentioned. But in the context of the rest of the paper, there are a number of other “Scores” that could be important to investigate."
  },
  {
    "objectID": "resources/reading/index.html#wrapping-up",
    "href": "resources/reading/index.html#wrapping-up",
    "title": "Reading a Technical Paper",
    "section": "Wrapping up",
    "text": "Wrapping up\nNormally, the process isn’t as elaborate as it appears in this demo. I don’t usually color code all of the words in a paragraph, much less a whole paper, like this. But I do often try to mentally summarize paragraphs and sections with what the upshot is. Some authors are better than others in getting across their point in among the technical aspects, but ideally they are always trying to communicate some message that you can at least approximate even if you don’t understand everything in detail."
  },
  {
    "objectID": "resources/notation/index.html",
    "href": "resources/notation/index.html",
    "title": "Mathematical Notation",
    "section": "",
    "text": "The most common variables we’re going to be seeing:\n\n\\(x, y\\)\n\nStand-ins for numbers, usually a list, or vector, of numbers\n\\(y\\) is often some kind of “outcome” variable\n\\(x\\) is often some kind of input, or predictor variables\ne.g. “We want to predict how many ice cream cones, \\(y\\), we’ll sell if it’s a specific temperature, \\(x\\).\n\n\\(X, Y\\)\n\nStand ins for categorical variables\n\n\\(w\\)\n\nA special variable for a word\n\n\\(c\\)\n\nA “count” of something\n\n\\(p\\)\n\nA probability\n\n\\(N\\)\n\nUsually, the total number of something\n\n\\(n\\)\n\nA contextual number. E.g. “In \\(n+1\\) days (that is, tomorrow)…”\n\n\\(n,m\\)\n\nWhen \\(n\\) and \\(m\\) are used together, it’s often to describe the number of rows and columns of a matrix.\n\n\\(A, B\\)\n\nThese are almost always used for matrices\nCapital roman letters are often a clue we’re looking at a matrix (but not always)\n\n\\(\\lambda\\)\n\n“lambda”\nOften used for an arbitrary value you multiply things by.\n\n\\(k\\)\n\nOften used for an arbitrary value you add things to.\n\n\\(\\delta\\)\n\n“delta”\nOften used to describe a difference of some kind\n\n\\(\\alpha, \\beta, \\gamma\\)\n\n“alpha”, “beta”, “gamma”\nUsed for model parameters\n\n\\(\\theta\\)\n\n“theta”\nAlso used as a model parameter.\nOr, to describe an angle (in radians)\n\n\n\n\n\n\\(\\hat{y}\\)\n\n“y hat”\nA predicted value for \\(y\\)\ne.g. “I predicted \\(\\hat{y}\\) ice cream cones to be sold, but they actually sold \\(y\\).\n\n\\(\\bar{y}\\)\n\n“y bar”\nThe average value of \\(y\\)\n\n\\(y^*\\)\n\n“y star”\nA modified value of \\(y\\)"
  },
  {
    "objectID": "resources/notation/index.html#one-dimensional",
    "href": "resources/notation/index.html#one-dimensional",
    "title": "Mathematical Notation",
    "section": "One Dimensional",
    "text": "One Dimensional\nWhen we have a variable that contains a list of values, each individual value will be described with an “index”. For example, if we had a variable \\(X\\) that contained the names of the week.\n\\[\nX = (\\text{Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday})\n\\]\nThen the first value in the list would be \\(X_1\\), and we could say \\(X_1 = \\text{Monday}\\). You’d usually pronounce \\(X_1\\) as “X sub 1”.\nSometimes we want to be able to refer to a generic value in the list \\(X\\) and for that we’d use an index variable like \\(X_i\\) (pronounced “X sub i”). The most common letters used to indicate generic indices are \\(i, j, k\\).\nWe can do math on the indices also. So, if the name of today is \\(X_i\\), then the name of tomorrow is \\(X_{i+1}\\). The name of yesterday is \\(X_{i-1}\\). The day after tomorrow would be \\(X_{i+2}\\).\nWe can also include a range of numbers in the indices. So, the names of the first three days of the week are \\(X_{1:3} = (\\text{Monday, Tuesday, Wednesday})\\). The names of yesterday, today, and tomorrow are \\(X_{i-1:i+1}\\)."
  },
  {
    "objectID": "resources/notation/index.html#two-dimensional",
    "href": "resources/notation/index.html#two-dimensional",
    "title": "Mathematical Notation",
    "section": "Two Dimensional",
    "text": "Two Dimensional\nWe could imagine a Month as being a matrix \\(M\\) made up of 4 weeks, with each week being 7 days.\n\\[\nM = \\left[ \\begin{array}{lllllll}\n\\text{Monday} & \\text{Tuesday} & \\text{Wednesday} & \\text{Thursday} &  \\text{Friday} &     \\text{Saturday} & \\text{Sunday} \\\\\n\\text{Monday} & & & \\dots & & & \\text{Sunday}\\\\\n\\vdots & & & \\ddots\\\\\n\\text{Monday} & & & \\dots & & &\\text{Sunday}\\\\\n\\end{array} \\right]\n\\]\nIf we wanted to get the name of the third day during the first week, it would be \\(M_{1, 3} = \\text{Wednesday}\\). When giving numeric indices of a matrix, the rows (the parts going across) come first, and the columns (the parts going up and down) come second. So, we’d describe the matrix \\(M\\) as being a \\(4\\times7\\) “four by seven” matrix.\nTo refer to a day of the month generically, we’d use \\(M_{i,j}\\) (pronounced “M sub i, j”). We can also use ranges in these indices. So, to refer to the second week of the month, we’d use \\(M_{2,1:7}\\). Or, to refer to all of the Saturdays in the month, we’d use \\(M_{1:4,6}\\).\nAnd, we can also use math in these indices. So, we could refer to “a week from today” with \\(M_{i+1,j}\\)."
  },
  {
    "objectID": "resources/notation/index.html#matrix-transposition",
    "href": "resources/notation/index.html#matrix-transposition",
    "title": "Mathematical Notation",
    "section": "Matrix Transposition",
    "text": "Matrix Transposition\nThe one special notation related to matrices is “transposition”, which basically takes a matrix and flips it.\n\\[\nA = \\left[\\begin{array}{ccc}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{array}\n\\right]\n\\]\n\\[\nA^\\intercal = \\left[\\begin{array}{cc}1 & 4\\\\\n2 & 5 \\\\\n3 & 6 \\end{array}\\right]\n\\]\nThe little \\(^\\intercal\\) indicates that we’re transposing the matrix. You’d pronounce \\(A^\\intercal\\) as “A transpose”.\nThe transpose operation can apply to single lists too. This will eventually be important.\n\\[\nx = [1, 2, 3, 4]\n\\]\n\\[\nx^\\intercal = \\left[\\begin{array}{c} 1\\\\2\\\\3\\\\4 \\end{array}\\right]\n\\]\nAgain, \\(x^\\intercal\\) is pronounced “x transpose.”"
  },
  {
    "objectID": "resources/notation/index.html#generic",
    "href": "resources/notation/index.html#generic",
    "title": "Mathematical Notation",
    "section": "Generic",
    "text": "Generic\n\n\\(f(), g()\\)\n\nThese are the most common “functions” you’ll see. Usually in a whole formula like \\(y = f(x)\\) (pronounced “y equals f of x”).\nIt means “we stick the value \\(x\\) in and \\(y\\) comes out.\nWhat \\(f()\\) or \\(g()\\) (or whatever name we give the function) is needs to be defined. They don’t have a fixed meaning."
  },
  {
    "objectID": "resources/notation/index.html#specialized",
    "href": "resources/notation/index.html#specialized",
    "title": "Mathematical Notation",
    "section": "Specialized",
    "text": "Specialized\n\n\\(P()\\)\n\nThe function \\(P()\\) refers to the probability of whatever we put in.\n\\(P(X_i)\\) returns the probability of a specific \\(X_i\\) value.\nWe’d refer to the specific value the \\(P()\\) returns with the variable \\(p\\), usually with the same index. So \\(p_i = P(X_i)\\).\n\n\\(C()\\)\n\nThe function \\(C()\\) returns the count of whatever we put in.\nIn the matrix above, \\(C(\\text{Monday}) = 4\\)\nWe’d refer to the specific count value of something with the variable \\(c\\), usually with the same index. So \\(c_{i} = C(X_i)\\)."
  },
  {
    "objectID": "resources/notation/index.html#summation",
    "href": "resources/notation/index.html#summation",
    "title": "Mathematical Notation",
    "section": "Summation",
    "text": "Summation\n\\(\\sum\\)\nThis operator indicates that we are adding together numbers in a list. Let’s look at the table of the height of actors, in cm who played Spider-Man in Spider-Man: No Way Home.\n\n\n\n\nActor\nHeight (cm)\n\n\n\n\nTobey Maguire\n172\n\n\nAndrew Garfield\n179\n\n\nTom Holland\n169\n\n\n\n\nWe would say that there are \\(N=3\\) actors who played Spider-Man in the movie. And we could represent their heights in a variable \\(y\\), and say\n\\[\ny = (172, 179, 169)\n\\]\nThe height of the first actor would be \\(y_1\\), which equals \\(172\\), and the way to refer to any given height on the list would be \\(y_i\\).\nTo get the total height of the actors (like if one stood on the head of the other), we would have to sum it up, which we could represent like this:\n\\[\nh = y_1 + y_2 + y_3\n\\]\nOr, we could use summation notation\n\\[\nh = \\sum_{i=1}^Ny_i\n\\]\nThe way to read this out loud is “h equals the sum of y sub i from i equals 1 to N”. The \\(i=1\\) part underneath the \\(\\sum\\) means “start getting values out of \\(y\\) starting with 1”. The \\(N\\) at the top means “keep adding 1 to \\(i\\) until \\(i = N\\).”\nWhat the whole notation is going to do is pull out every value of \\(y\\) and add them together.\nIf you know how to do some coding, sometimes it’s easier to understand the mathematical notation to see it in code.\n\n```{python}\ny = [172, 179, 169]\nN = 3 # = len(y)\nh = 0 \n\nfor i in range(N):\n  h = h + y[i]\n\nprint(h)\n```\n\n520"
  },
  {
    "objectID": "resources/notation/index.html#product",
    "href": "resources/notation/index.html#product",
    "title": "Mathematical Notation",
    "section": "Product",
    "text": "Product\n\\(\\prod\\)\nThe product operator, \\(\\prod\\), works a lot like the the summation operator, except instead of adding numbers together, it multiplies them. For example, let’s say we’re keeping track of the day-to-day changes in the number of visitors to a website.\n\n\n\nDay\nPercent Change\nMultiplier\n\n\n\n\n1\nup 1%\n1.01\n\n\n2\nno change\n1.00\n\n\n3\ndown 5%\n0.95\n\n\n\nWe can get the total proportional change over these three days by multiplying the proportions together. Writing it out the long way, it’s\n\\[\nN = 3\n\\]\n\\[\ny = (1.01, 1, 0.95)\\\\\n\\]\n\\[\nt = y_1\\cdot y_2\\cdot y_3\n\\]\nIn product notation, though, it looks like this.\n\\[\nt = \\prod_{i=1}^Ny_i\n\\]\nAgain, if you’re more comfortable with programming code, it’s equivalent to this.\n\n```{python}\ny = [1.01, 1, 0.95]\nN = 3\nt = 1\n\nfor i in range(N):\n  t = t * y[i]\n\nprint(f\"{t:.4}\")\n```\n\n0.9595"
  },
  {
    "objectID": "resources/notation/index.html#conditional-probability",
    "href": "resources/notation/index.html#conditional-probability",
    "title": "Mathematical Notation",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nThe “conditional probability” is the probability of some value or event \\(Y\\), holding constant some other value or event \\(X\\) . For example, maybe\n\\[\nY = \\text{I am teaching}\n\\]\nand\n\\[\nX_1 = \\text{It is a Monday}\n\\]\nand\n\\[\nX_6=\\text{It is a Saturday}\n\\]\nThe probability that I am teaching a class is is a lot lower on a Saturday than on a Monday. We can express these like so.\n\\[\np_1 = P(Y | X_1)\n\\]\na.k.a “p sub 1 equals the probability I am teaching, given that it is a Monday”.\n\\[\np_6 = P(Y|X_6)\n\\]\na.k.a. “p sub 6 equals the probability I am teaching given that it is a Saturday”\nand\n\\[\np_6 < p_1\n\\]\nThe key piece of notation in the expressions above is the \\(|\\) (the “pipe”) inside of the \\(P()\\) function."
  },
  {
    "objectID": "resources/notation/index.html#joint-probability",
    "href": "resources/notation/index.html#joint-probability",
    "title": "Mathematical Notation",
    "section": "Joint Probability",
    "text": "Joint Probability\nThe joint probability is the probability of two values or events happening together. It’s not the same as a conditional probability of one event given the other, but explaining why requires more time and space.\nIf we stick with the same events as above (“I am teaching” and “It is a Monday”) the joint probability of “I am teaching and it is a Monday” would be notated as\n\\[\nq_1 = P(Y,X_1)\n\\]\nThe comma inside the \\(P()\\) function means “and.”\n\n\npdf\n\n\n\n\nCC BY-SA 4.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lin517: Natural Language Processing",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n\n\n\nAug 20, 2022\n\n\nMathematical Notation\n\n\n\n\n\n\n\n \n\n\n\nAug 22, 2022\n\n\nReading a Technical Paper\n\n\n\n\n\n\n\n\n\nAug 24, 2022\n\n\nWhat is NLP?(for this course)\n\n\n\n\n\n\n\n\n\nAug 31, 2022\n\n\nData Sparsity\n\n\n\n\n\n\n\n \n\n\n\nSep 2, 2022\n\n\nStarting Python\n\n\npython\n\n\n\n\n\n\n\nSep 6, 2022\n\n\nData Processing\n\n\n\n\n\n\n\n\n\nSep 9, 2022\n\n\nLists and Dictionaries\n\n\npython\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "python_sessions/01_session2.html",
    "href": "python_sessions/01_session2.html",
    "title": "Lists and Dictionaries",
    "section": "",
    "text": "In the previous module, we learned about assignment and the kinds of values we can have in python. Just a quick recap:"
  },
  {
    "objectID": "python_sessions/01_session2.html#lists",
    "href": "python_sessions/01_session2.html#lists",
    "title": "Lists and Dictionaries",
    "section": "Lists",
    "text": "Lists\nWe’ll rarely want to work with just one of string values. More often we’ll want to work with a collection of values. For this we’ll use lists. For example, here is a list of Mary Shelly’s published novels.\nnovels = [\"Frankenstein\",\n          \"Valperga\",\n          \"The Last Man\", \n          \"The Fortunes of Perkin Warbeck, A Romance\", \n          \"Lodore\", \n          \"Falkner. A Novel\"]\nAssignment works the same way as before. We can use any text, without commas, as a variable name, then assign a value to it with the = operator.\n\n💡 TASK 1\nAssign the names of people in your group (first and last) to a list called our_group.\n\nLists don’t just have to contain strings, but can actually contain any mixture of data types.\npublished_dates = [1818, 1823, 1826, 1830, 1835, 1837]\n\nLists can get complex\nYou can have lists that contain other lists.\nmy_signs = [\"Joe\", [\"Capricorn\", \"Gemini\", \"Capricorn\"]]\n\n💡 TASK 2\nEach person in the group create a list of lists, with your name as the first value, and your big three in a list as the second value. Assign this value to a variable named with your initials. (If you don’t know your big three or don’t want to share, just say “Capricorn” across the board.)\n\n\n\nMaking changes to lists\n\nAdding items\nThere are a few ways you can add additional values to lists. One way is with the + operator. For example, one of Mary Shelley’s books, Mathida was published posthumously.\nposthumous = [\"Mathilda\"]\nWe could add this to the list of novels like so.\nprint(novels + posthumous)\nOne thing you should note is that this doen’t change the list novels. If you run print(novels) now, it will have just the original books in it.\nTo change the actual list stored in novels, we need to use the .append() “method”. We’ll talk more about “methods” vs functions in later lessons\nnovels.append(\"Mathilda\")\nprint(novels)\nA thing to remember about this is .append() changes the variable novels “in place.” That is, without doing any additional assignment, or anything, you’ve changed its value.\n\n💡 TASK 3\nCreate a variable called group_sign which is a list of each person’s star sign list from Task 2.\n\n\n\nSorting\nTo sort a list, alphabetically for strings or numerically for numbers, you can use the .sort() method. Again, this changes the list in place.\n\n💡 TASK 4\nAlphabetically sort the titles of Mary Shelly’s novels.\n\n\n\n\nIndexing Lists (and other “iterables”)\nIn order to pull values out of a list, we need to “index” it. Here’s a really important thing to remember:\nIndexing Starts at 0!\nTo get the fist value out of a list we use the index value 0, and pass it in-between brackets, like so:\nprint(novels[0])\nIn the alphabetically ordered list, this should return Falkner, A Novel\n\n💡 TASK 5\nPrint the name of the second member in our_group.\n\nNumeric indexing works with any “iterable” in python. For example, if we assigned the first novel to a variable, and started indexing that, it would start printing individual letters.\nfirst_novel = novels[0]\nprint(first_novel[2])\nThis will have printed out the third letter of the first novel’s title.\n🚨We can do this better by stacking up indices.\nInstead of assigning the first valye to a variable, we can get the same result by just placing these indexing brackets one after another.\nprint(novels[0][2])\n\n💡 TASK 6\nPrint the second letter from the first value in the list our_group.\n\n\n💡 TASK 7\nPrint the moon sign of the third member of your group, as stored in our_signs from task 3.\n\n\n\nSlicing\nIf we want to get a range of value out of the list, we can use a “slice”. For example, to get the first three books out of the list novels, we can do\nprint(novels[0:3])\nNow… this should strike you as weird, because to get the third value from the list, you use novels[2]. Why does the slice end in 3?\nWe should think about the relationship between values and indices like this:\n\nRather than being set one on top of eachother, the indices come between the values. When you give just one numeric index, python returns the first value to the right. When you pass python a slice with a starting index and and ending index, it returns everything between those indices.\n\n\nReverse indexing.\nThere’s a quick way to get the final value from a list or iterable as well: index with -1.\n\n💡 TASK 8\nPrint the last letter from the last value in the list our_group\n\n\n💡 TASK 9\nPrint the rising sign of the first person as stored in our_signs"
  },
  {
    "objectID": "python_sessions/01_session2.html#dictionaries",
    "href": "python_sessions/01_session2.html#dictionaries",
    "title": "Lists and Dictionaries",
    "section": "Dictionaries",
    "text": "Dictionaries\nWhile lists can be highly complex, and can even capture the relationships between data, they can be a bit limiting. For example, here’s a list representing the relationship between the title and the year of publication of Mary Shelley’s first and last novels.\nnovel_dates = [[\"Frankenstein\", 1818],\n               [\"Mathilda\",     1959]]\nWhile this does the job, if I came along later, and all I knew was the title “Frankenstein” and wanted to quickly get the date, it would take a bit of work with these nested lists.\nWhat would speed up the job are “dictionaries”, which store key:value pairs.\nnovel_dict = {\"Frankenstein\"     : 1818,\n              \"Valperga\"         : 1823,\n              \"The Last Man\"     : 1826, \n              \"The Fortunes of Perkin Warbeck, A Romance\" : 1830,\n              \"Lodore\"           : 1835,\n              \"Falkner. A Novel\" : 1837}\nThese dictionaries are created with opening and closing curly brackets { }, and have a sequence of key : value pairs. The “key” is called the “key”, because instead of indexing dictionaries with numeric values, we index them with whatever the keys are.\nSo to get the publication date of Frankenstein, we do\nnovel_dict[\"Frankenstein\"]\nTo add a previously absent key : value pair to a dictionary, we pass the new key we want to use to [ ], and then assign the new value.\nnovel_dict[\"Mathilda\"] = 1959\n\n💡 TASK 10\nCreate a dictionary where the keys are the names (first and last) of your group members, and the values are your heights, in inches (as a numeric value).\n(Hint, if you’re 5’10, the math would be (5*12) + 10)\n\n\n💡 TASK 11\nPrint the height of the alphebetically first group member. (Don’t just type in their name, get it from python).\n\n\nChecking for keys\nTo check if a key is already in the dictionary, we can use the in operator.\n`python \"Mary: A Fiction\" in novel_dict # False"
  },
  {
    "objectID": "python_sessions/00_session1.html",
    "href": "python_sessions/00_session1.html",
    "title": "Starting Python",
    "section": "",
    "text": "Welcome to Python!\nIf you have never done programming before, all python is is a program that reads a text file, and executes commands contained in the text file."
  },
  {
    "objectID": "python_sessions/00_session1.html#its-a-text-file",
    "href": "python_sessions/00_session1.html#its-a-text-file",
    "title": "Starting Python",
    "section": "It’s a text file",
    "text": "It’s a text file\nGo ahead and open the text file fake.txt. It contains text that is all valid python code. If you pop over to the shell and run\npython3 fake.txt\nPython will happily run the code in fake.txt. However, I recommend we always save our python scripts with the .py file extension for a few reasons.\n\nEveryone does it, so it’ll match human expectations for people looking at your code.\nAny worthwhile code editor (replit included) will decide what the text in the file is supposed to be based on the file extension, and trys to help you accordingly with things like\n\nSyntax highlighting, making your code easier to read.\nAutocomplete assistance (e.g. if you type an open (, it’ll automatically insert the closing ).\nCode suggestions. Some text editors will try to clue you with the possible names of functions your trying to type if you just type in the first few letters, and then may even try to clue you with the names of arguments to the functions."
  },
  {
    "objectID": "python_sessions/00_session1.html#interacting-python",
    "href": "python_sessions/00_session1.html#interacting-python",
    "title": "Starting Python",
    "section": "Interacting Python",
    "text": "Interacting Python\nThere are are a bunch of different ways you can interact with python to get it to interpret your code.\n\nInteractive Session\nIf you go to the Shell and just run python3 without any other arguments, it will launch the python shell, which inteprets the text you type in as python code. If you copy-paste this code into the python shell, it should print hello world back at you.\nprint(\"hello world\")\nTo quit the python shell, run\nquit()\n\n\n“Notebooks” (not in replit)\nThere are a few “notebook” options out there which allow you to interleave text, like notes to yourself, or descriptions of the code, with python code chunks.\nYou can experiment with free online notebooks by setting up an account at Kaggle or Google Colab.\nFor your local set up, I’d recommend either configuring VS Code to run Jupyter notebooks, or use a Quarto document in RStudio (it’s not just for R!)\n\n\nScripts\nThe primary way we’ll be interacting with python inside of replit is with python “scripts,” which are just text documents of code python should run, line by line."
  },
  {
    "objectID": "python_sessions/00_session1.html#getting-started",
    "href": "python_sessions/00_session1.html#getting-started",
    "title": "Starting Python",
    "section": "Getting Started",
    "text": "Getting Started\nThe python script main.py already has some code in it.\nimport numpy as np\n\n# This is a comment\n# Python won't interpret or run a line starting with #\n\nages = np.array([40, 42, 25])\nyear = np.array([2016, 2001, 2019])\n\n💡 TASK 1\nBelow this line (it doesn’t matter how many new lines you add) enter this line of code crucially without any spaces or tabs preceding it.\nprint(\"hello world\")\nOnce you’ve done that, hit the big green run button, and hello world should print out in the console.\n\n\nHow to see what python is doing\nWe’re going to be using the print() function a lot. The primary way we’ll be interacting with python is via python scripts, and the only way to see what our code has done is to explicitly tell python to print the output."
  },
  {
    "objectID": "python_sessions/00_session1.html#values-variables-and-assignment",
    "href": "python_sessions/00_session1.html#values-variables-and-assignment",
    "title": "Starting Python",
    "section": "Values, Variables, and Assignment",
    "text": "Values, Variables, and Assignment\nBefore we get into what the stuff at the very top of the script means, we’re going to first cover the basics of values, variable, and assignment.\n\nValues\nThe main python value types are\n\nNumeric\nCharacter\nBoolean (True/False)\n\n\nNumeric Practice\n\n💡 TASK 2\nCalculate how many seconds and print the output by adding this line to your script.\n print(f\"There are {365 * 24 * 60} minutes in a year.\")\n\nWhat python has done is multiplied 365 (for the number of days) by 24 (for the number of hours in a day) by 60 (for the number of minutes in an hour) to produce the number of minutes in a day.\nPython can do any kind of arithmetic you ask of it. For example, we can caclute what percent of a whole day one minute is by adding this line of code to our script.\n\n💡 TASK 3\nCalculate how many seconds and print the output by adding this line to your script.\n print(f\"One minute is {(1/(24 * 60)) * 100}% of a day.\")\n\n\n💡 TASK 4\nChapter 1 of Frankenstein has 1,780 words, and 75 of them were the word “of”. Calculate what percent of words were “of” by adding this line of code to your script.\nprint(f\"{}% of words in Chapter 1 of Franenstein were 'of'\")\nFill in the correct mathematical formula inside the {}\n\n\n\n\nAssignment\nWe don’t usually want to just do some calculations and then just let the values disappear when we print them, though. We’ll usually want to save some values for future use. We can do that by “assigning” values to “variables.”\nThe assignment operator in python is =. For example we can assign the name of this class to a variable called this_class like so:\nthis_class = \"Lin517\"\n\n💡 TASK 5\nAssign the value \"Lin517\" to this_class\n\n\n💡 TASK 6\nPrint the variable this_class\n\n\nImportant Things to Note!\n\nThe variable this_class did not exist before we did assignment! If we had asked python to print this_class before we did the assignment, it would have given us an error.\nVariable names are case sensitive! If we tried to print This_class or this_Class or This_Class they would all return an error.\nYou can start variable names with any letter or underscore, but that’s all (no numbers at the start).\nAfter the first character, you can use any letter, number, or underscore.\nNo &, ., * or ? are allowed.\nAny text that isn’t enclosed inside \" \" will be interpreted as a variable name."
  },
  {
    "objectID": "python_sessions/00_session1.html#doing-things-with-variables.",
    "href": "python_sessions/00_session1.html#doing-things-with-variables.",
    "title": "Starting Python",
    "section": "Doing things with variables.",
    "text": "Doing things with variables.\nOnce you assign a value to a variable, it can stand in as if it was that variable. For example.\ndays_in_year    = 365\nhours_in_day    = 24\nminutes_in_hour = 60\n\nprint(f\"There are {days_in_year * hours_in_day * minutes_in_hour} minutes in a year\")\n\n💡 TASK 7\n\nAssign the current year to a variable called this_year.\nAssign one of your ages to a variable called my_age\nCalculate your year of birth by subtracting my_age from this_year\nPrint the result.\n\n\n\n💡 TASK 8\nCalculate how old you’ll be in 2040 and print the result.\n\nYou can overwrite the value you’ve assigned to any variable by just assigning a new value to it."
  },
  {
    "objectID": "python_sessions/00_session1.html#numbers",
    "href": "python_sessions/00_session1.html#numbers",
    "title": "Starting Python",
    "section": "Numbers",
    "text": "Numbers\nTechnically, there are two kinds of numbers in Python: integers (numbers without decimals places) and floats (numbers with decimal places). This used to be a bigger deal in python2, but python3 converts as necessary. We’ve already done some work with numbers above. The built in arithmetic in python that we can use on numbers is:\n\nx + y addition\nx - y subtraction\nx * y multiplication\nx / y division\nx ** y exponentiation (that is, xy)\nx % y modulus (this gives you the remainder of doing division)\nx // y floor division (this gives you the largest whole number that y can go into x)\n\n\n💡 TASK 9\nMary Shelly has written 1,780 words for Frankenstein Chapter 1, and her publisher has told her there is a strict word limit of 300 words per page. 1. Calculate how many full pages chapter 1 is going to be. Assign this value to the variable full_pages. 2. There’s going to be some words left over. Calculate how many words are going to go onto the overflow page. Assign this value to a variable called overflow."
  },
  {
    "objectID": "python_sessions/00_session1.html#strings",
    "href": "python_sessions/00_session1.html#strings",
    "title": "Starting Python",
    "section": "Strings",
    "text": "Strings\nWe’ve already been doing a lot with strings in these print() statements. But just to be explicit, everything that comes inside \" \" is interpreted as a string, even numbers. If you tried to do\n1 + \"1\"\nYou would get an error, because the first value is a number and the second value is a string.\nWe can use some math-looking-things on strings, though.\n\n💡 TASK 10\nDo the following assignments.\nroot    = \"Lingu\"\naffixes = \"istics\"\nNow, print what happens when you do root + affixes\n\n\n💡 TASK 11\nAgain, do the following assignments.\nframe = \"It's a \"\nword  = \"salad \"\nredup = word * 2\nNow, print the result of what happens when you do frame + redup"
  },
  {
    "objectID": "lectures/data_sparsity/data_sparsity.html",
    "href": "lectures/data_sparsity/data_sparsity.html",
    "title": "Data Sparsity",
    "section": "",
    "text": "Let’s say we’re biologists, working in a rain forest, and put out a bug net to survey the biodiversity of the forest. We catch 10 bugs, and each species is a different color:\n[\\(_1\\), \\(_2\\), \\(_3\\), \\(_4\\), \\(_5\\), \\(_6\\), \\(_7\\), \\(_8\\), \\(_9\\), \\(_{10}\\)]\nWe have 10 bugs in total, so we’ll say \\(N=10\\). This is our “token count.” We’ll use the \\(i\\) subscript to refer to each individual bug (or token).\nIf we made a table of each bug species, it would look like:\n\n\n\nspecies\nindex \\(j\\)\ncount\n\n\n\n\n\n1\n5\n\n\n\n2\n2\n\n\n\n3\n1\n\n\n\n4\n1\n\n\n\n5\n1\n\n\n\nLet’s use \\(M\\) to represent the total number of species, so \\(M=5\\) here. This is our type count, and we’ll the subscript \\(j\\) to represent the index of specific types.\nWe can mathematically represent the count of each species like so.\n\\[\nc_j = C(\\class{fa fa-bug}{}_j)\n\\]\nHere, the function \\(C()\\) takes a specific species representation \\(\\class{fa fa-bug}{}_j\\) as input, and returns the specific count \\(c_j\\) for how many times that species showed up in our net. So when \\(j = {\\color{#785EF0}{1}}\\), \\(\\color{#785EF0}{c_1}=5\\), and when \\(j = {\\color{#FFB000}{4}}\\), \\(\\color{#FFB000}{c_4}=1\\).\nHere’s a plot, with the species id \\(j\\) on the x-axis, and the number of times that species appeared in the net \\(c_j\\) on the y-axis.\n\n\n\n\n\n\n\nWhat is the probability that tomorrow, when we put the net out again, that the first bug we catch will be from species ? Usually in these cases, we’ll use past experience to predict the future. Today, of the \\(N=10\\) bugs we caught, \\(\\color{#785EF0}{c_1}=5\\) of them were species . We can represent this as a fraction like so:\n\\[\n\\frac{{\\color{#785EF0}{\\class{fa fa-bug}{}}}_1,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_2,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_3,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_4,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_5}\n{{\\color{#785EF0}{\\class{fa fa-bug}{}}}_1,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_2,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_3,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_4,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_5,\n      {\\color{#DC267F}{\\class{fa fa-bug}{}}}_6,\n      {\\color{#DC267F}{\\class{fa fa-bug}{}}}_7,\n      {\\color{#FE6100}{\\class{fa fa-bug}{}}}_8,\n      {\\color{#FFB000}{\\class{fa fa-bug}{}}}_9,\n      {\\color{#4C8C05}{\\class{fa fa-bug}{}}}_{10}}\n\\]\nOr, we can simplify it a little bit. The top part (the numerator) is equal to \\(\\color{#785EF0}{c_1}=5\\), and the bottom part (the denominator) is equal to the total number of bugs, \\(N\\). Simplifying then:\n\\[\n\\frac{\\color{#785EF0}{c_1}}{N} = \\frac{5}{10} = 0.5\n\\]\nWe’ll use this as our guesstimate of the probability that the very next bug we catch will be from species . Let’s use the function \\(\\hat{P}()\\) to mean “our method for guessing the probability”, and \\(\\hat{p}\\) to represent the guess we came to. We could express “our guess that the first bug we catch will be ” like so.\n\\[\n{\\color{#785EF0}{\\hat{p}_1}} = \\hat{P}({\\color{#785EF0}{\\class{fa fa-bug}{}}}) = \\frac{\\color{#785EF0}{c_1}}{N} = \\frac{5}{10} = 0.5\n\\]\nWe can then generalize our method to any bug like so:\n\\[\n\\hat{p}_j = \\hat{P}(\\class{fa fa-bug}{}_j) = \\frac{c_j}{N}\n\\]\n\n\n\nLet’s say we set out the net again, and the first bug we catch is actually . This is a new species of bug that wasn’t in the net the first time. Makes enough sense, the forest is very large. However, what probability would we have given catching this new species?\nWell, \\(\\color{#35F448}{c_6} = C({\\color{#35F448}{\\class{fa fa-bug}{}}}) = 0\\). So our estimate of the probability would have been \\({\\color{#35F448}{\\hat{p}_6}} = \\hat{P}({\\color{#35F448}{\\class{fa fa-bug}{}}}) = \\frac{\\color{#35F448}{c_6}}{N} = \\frac{0}{10} = 0\\).\nWell obviously, the probability that we would catch a bug from species  wasn’t 0, because events with 0 probability don’t happen, and we did catch the bug. Admittedly, \\(N=10\\) is a small sample to try and base a probability estimate on, so how large would we need the sample to be before we could make probabity estimates for all possible bug species, assuming we stick with the probability estimating function \\(\\hat{P}(\\class{fa fa-bug}{}_j) = \\frac{c_j}{N}\\)?"
  },
  {
    "objectID": "lectures/data_sparsity/data_sparsity.html#youd-need-fa-infinity",
    "href": "lectures/data_sparsity/data_sparsity.html#youd-need-fa-infinity",
    "title": "Data Sparsity",
    "section": "You’d need ",
    "text": "You’d need \nThis kind of data problem does arise for counting species, but this is really a tortured analogy for language data.1 For example, let’s take all of the words from Chapter 1 of Mary Shelly’s Frankenstein, downloaded from Project Gutenberg. I’ll count how often each word occurred, and assign it a rank, with 1 being given to the word that occurred the most.1 For me, I used this analogy to include colorful images of bugs in the lecture notes. For Good (1953), they had to use a tortured analogy since the methods for fixing probability estimates were still classified after being used to crack the Nazi Enigma Code in WWII.\n\n\n\n\n\n\n\n\n\nJust to draw the parallels between the two analogies:\n\n\n\n\n\n\n\n\nvariable\nin the analogy\nin Frankenstein Chapter 1\n\n\n\n\n\\(N\\)\nThe total number of bugs caught in the net. (\\(N=10\\))\nThe total number of words in the first chapter. (\\(N=1,780\\)).\n\n\n\\(x_i\\)\nAn individual bug. e.g. \\(_1\\)\nAn individual word token. In chapter 1, \\(x_1\\) = “i”\n\n\n\\(w_j\\)\nA bug species. \nA word type. The indices are frequency ordered, so for chapter 1 \\(w_1\\) = “of”\n\n\n\\(c_j\\)\nThe count of how many individuals there are of a species.\nThe count of how many tokens there are of a type.\n\n\n\nHere’s a table of the top 10 most frequent word types.\n\n\n\n\n\n\\(w_j\\)\n\\(c_j\\)\n\\(j\\)\n\n\n\n\nof\n75\n1\n\n\nthe\n75\n2\n\n\nand\n70\n3\n\n\nto\n61\n4\n\n\na\n52\n5\n\n\nher\n52\n6\n\n\nwas\n40\n7\n\n\nmy\n33\n8\n\n\nin\n32\n9\n\n\nhis\n29\n10\n\n\n\n\n\nIf we plot out all of the word types with the rank (\\(j\\)) on the x-axis and the count of each word type (\\(c_j\\)) on the y-axis, we get a pattern that if you’re not already familiar with it, you will be.\n\n\n\n\n\nThis is a “Zipfian Distribution” a.k.a. a “Pareto Distribution” a.k.a. a “Power law,” and it has a few features which make it ~problematic~ for all sorts of analyses.\nFor example, let’s come back to the issue of predicting the probability of the next word we’re going to see. Language Models are “string prediction models,” after all, and in order to get a prediction for a specific string, you need to have seen the string in the training data. Remember how our bug prediction method had no way of predicting that we’d see a  because it had never seen one before?\nThere are a lot of possible string types of “English” that we have not observed in Chapter 1 of Frankenstein. Good & Turing proposed that you could guesstimate that the probability of seeing a never before seen “species” was about equal to the proportion of “species” you’d only seen once. With just Chapter 1, that’s a pretty high probability that there are words you haven’t seen yet.\n\n\n\n\n\nseen once?\ntotal\nproportion\n\n\n\n\nno\n1216\n0.683\n\n\nyes\n564\n0.317\n\n\n\n\n\nSo, let’s increase our sample size. Here’s the same plot of rank by count for chapters 1 through 5.\n\n\n\n\n\n\n\n\n\n\nseen once?\ntotal\nproportion\n\n\n\n\nno\n9928\n0.858\n\n\nyes\n1649\n0.142\n\n\n\n\n\nWe increased the size of the whole corpus by a factor of 10, but we’ve still got a pretty high probability of encountering an unseen word.\nLet’s expand it out to the whole book now.\n\n\n\n\n\n\n\n\n\n\nseen once?\ntotal\nproportion\n\n\n\n\nno\n72122\n0.96\n\n\nyes\n3021\n0.04\n\n\n\n\n\n\n🎵 Ain’t no corpus large enough 🎵\nAs it turns out, there’s no corpus large enough to guarantee observing every possible word at least once, for a few reasons.\n\nThe infinite generative capacity of language! The set of all possible words is, in principle infinitely large.\nThese power law distributions will always have the a lot of tokens with a frequency of 1, and even just those tokens are going to have their probabilities poorly estimated.\n\nTo illustrate this, I downloaded the 1-grams of just words beginning with [Aa] from the Google Ngrams data set. This is an ngram dataset based on all of the books scanned by the Google Books project. It’s 4 columns wide, 86,618,505 rows long, and 1.8G large, and even then I think it’s a truncated version of the data set, because the fewest number of years any given word appears is exactly 40.\nIf we take just all of the words that start with [Aa] published in the year 2000, the most common frequency for a word to be is still just 1, even if it is a small proportion of all tokens.\n\n\n\nFrequencies of frequencies in words starting with [Aa] from the year 2000 in google ngrams \n\n\n\n\n\n\n\nword frequency\nnumber of types with frequency\nproportion of all tokens\n\n\n\n\n1\n205141\n4.77e-10\n\n\n2\n152142\n9.55e-10\n\n\n3\n107350\n1.43e-09\n\n\n4\n80215\n1.91e-09\n\n\n5\n60634\n2.39e-09\n\n\n6\n47862\n2.86e-09\n\n\n\n\n\n\n\nAn aside\nI’ll be plotting the rank vs the frequency with logarithmic axes from here on. Linear axes give equal visual space for every incremental change in the x and y values, while lograrithmic axes put more space between smaller numbers than larger numbers.\n\n\n\n\n\n\nrank by frequency on linear scales\n\n\n\n\n\n\n\nrank by frequency on logarithmic scales\n\n\n\n\n\n\n\nIt gets worse\nWe can maybe get very far with our data sparsity for how often we’ll see each individual word by increasing the size of our corpus size, but 1gram word counts are rarely as far as we’ll want to go.\nTo come back to our bugs example, let’s say that bug species  actually hunts bug species . If we just caught a  in our net, it’s a lot more likely that we’ll catch a  next, coming after the helpless  than it would be if we hadn’t just caught a . To know what exactly the probability catching  and then a  is, we’d need to count up every 2 bug sequence we’ve seen.\nBringing this back to words, 2 word sequences are called “bigrams” and 3 word sequences are called “trigrams,” and they are also distributed according to a Power Law, and each larger string of words has a worse data sparsity one than the one before. But each larger string of words means more context, which makes for better predictions."
  },
  {
    "objectID": "lectures/data_sparsity/data_sparsity.html#some-notes-on-power-laws",
    "href": "lectures/data_sparsity/data_sparsity.html#some-notes-on-power-laws",
    "title": "Data Sparsity",
    "section": "Some Notes on Power Laws",
    "text": "Some Notes on Power Laws\nThe power law distribution is pervasive in linguistic data, in almost every domain where we might count how often something happens or is observed. This is absolutely a fact that must be taken into account when we develop our theories or build our models. Some people also think it is an important fact to be explained about language, but I’m deeply skeptical.\nA lot of things follow power law distributions. The general property of these distributions is that the second most frequent thing will have a frequency about as half as the most frequent thing, the third most frequent thing will have a frequency about a third of the most frequent thing, etc. We could put that mathematically as:\n\\[\nc_j = \\frac{c_1}{j}\n\\]\nFor example, here’s the log-log plot of baby name rank by baby name frequency in the US between 1880 and 2017.22 Data from the babynames R package, which in turn got the data from the Social Security Administration.\n\n\n\n\n\nrank by frequency of baby names\n\n\n\n\nThe log-log plot isn’t perfectly straight (it’s common enough for data like this to have two “regimes”).\nHere’s the number of ratings each movie on IMDB has received.\n\n\n\n\n\nIf we break down the movies by their genre, we get the same kind of result.\n\n\n\n\n\nOther things that have been shown to exhibit power law distributions (Newman 2005; Jiang and Jia 2011) are\n\nUS city populations\nnumber of citations academic papers get\nwebsite traffic\nnumber of copies books sell\nearthquake magnitudes\n\nThese are all possibly examples of “preferential attachment”, but we can also create an example that doesn’t involve preferential attachment, and still wind up with a power-law. Let’s take the first 12 words from Frankenstein:\n\n\n\n\"to\"\"mrs\"\"saville\"\"england\"\"st\"\"petersburgh\"\"dec\"\"11th\"\"17\"\"you\"\"will\"\"rejoice\"\n\n\n\nNow, let’s paste them all together into one long string with spaces.\n\n\n\n\"to mrs saville england st petersburgh dec 11th 17 you will rejoice\"\n\n\nAnd now, let’s choose another arbitrary symbol to split up words besides \" \". I’ll go with e.\n\n\n\n\"to mrs savill\"\" \"\"ngland st p\"\"t\"\"rsburgh d\"\"c 11th 17 you will r\"\"joic\"\"\"\n\n\n\nThe results aren’t words. They’re hardly useful substrings. But, if we do this to the entire novel and plot out the rank and count of thes substrings like they were words, we still get a power law distribution.\n\n\n\n\n\nIn fact, if I take the top 4 most frequent letters, besides spaces, that occur in the text and use them as substring delimiters, the resulting substring distributions are all power-law distributed.\n\n\n\n\n\nThey even have other similar properties often associated with power law distributions in language. For example, it’s often been noted that more frequent words tend to be shorter. These weird substrings exhibit that pattern even more strongly than actual words do!\n\n\n\n\n\nThis is all to say, be cautious about explanations for power-law distributions that are"
  },
  {
    "objectID": "lectures/data_sparsity/data_sparsity.html#extra",
    "href": "lectures/data_sparsity/data_sparsity.html#extra",
    "title": "Data Sparsity",
    "section": "Extra",
    "text": "Extra\nTo work out just how accurate the Good-Turing estimate is, I did the following experiment.\nStarting from the beginning of the book, I coded each word \\(w_i\\) for whether or not it had already appeared in the book, 1 if yes, 0 if no. This is my best shot at writing that out in mathematical notation.\n\\[\na_i = \\left\\{\\begin{array}{ll}1,& x_i\\in x_{1:i-1}\\\\\n                             0,& x_1 \\notin x_{1:i-1}\\end{array}\\right\\}\n\\]\nThen for every position in the book, I made a table of counts of all the words up to that point in the book so far, and got the proportion of word tokens that had appeared only once. Again, here’s my best stab at writing that out mathematically.\n\\[\nc_{ji} = C(w_j), w_j \\in x_{i:i-1}\n\\]\n\\[\nr_i = \\sum_{j=1}\\left\\{\\begin{array}{ll}1,&c_{ji}=1\\\\0,& c_{ji} >1 \\end{array}\\right\\}\n\\]\n\\[\ng_i = \\frac{r_i}{i-1}\n\\]\n\nfrank_words$first_appearance <- NA\nfrank_words$first_appearance[1] <- 1\n\nfrank_words$gt_est <- NA\nfrank_words$gt_est[1] <- 1\nfor(i in 2:nrow(frank_words)){\n  i_minus <- i-1\n  prev_corp <- frank_words$word[1:i_minus]\n  this_word <- frank_words$word[i]\n  \n  frank_words$first_appearance[i] <- ifelse(this_word %in% prev_corp, 0, 1)\n  frank_words$gt_est[i] <- sum(table(prev_corp) == 1)/i_minus\n}\n\n\n\n\nThen, I plotted the Good-Turing estimate for every position as well as a non-linear logistic regression smooth."
  },
  {
    "objectID": "lectures/data_processing/index.html",
    "href": "lectures/data_processing/index.html",
    "title": "Data Processing",
    "section": "",
    "text": "Before we even get to substantive issues of “text normalization” and “tokenization”, we need to also deal with basic data wrangling. For example, let’s say I wanted to download 4 works from Mary Shelly from Project Gutenberg and calculate what the most common 4 word sequences in her work are, I might quickly write some code like this.\n\n\n\n\n# python\n\n# urllib.request will download the books\nimport urllib.request\n\n# using a dictionary just to show the title of books here in the code.\nshelley_dict = {\"Tales and stories\": \"https://www.gutenberg.org/cache/epub/56665/pg56665.txt\",\n                \"Frankenstein\" : \"https://www.gutenberg.org/files/84/84-0.txt\",\n                \"The Last Man\" : \"https://www.gutenberg.org/cache/epub/18247/pg18247.txt\",\n                \"Mathilda\" : \"https://www.gutenberg.org/cache/epub/15238/pg15238.txt\"}\n\n# A collector list for all of the 4 word sequences\nall_grams4 = []\n\n# Loop over every url\nfor url in shelley_dict.values():\n  book_dat = urllib.request.urlopen(url)\n  \n  # this deals with the \n  # 1. character encoding\n  # 2. trailing whitespace\n  # 3. simplistic tokenization on spaces\n  book_lines = [line.decode(\"utf-8-sig\").strip().split(\" \") \n                for line in book_dat]\n  \n  # This flattens the list above into one long list of words\n  book_words = [word \n                for line in book_lines \n                  for word in line \n                    if len(word) > 0]\n  \n  # Collector list of 4grams from just this book\n  grams4 = []\n  \n  # loop over every index postion up to 4 words short of the end.\n  for i in range(len(book_words)-4):\n    \n    # glue together 4 word sequences with \"_\"\n    grams4.append(\"_\".join(book_words[i:(i+4)]))\n    \n  # Add this book's 4grams to all of the books' 4grams\n  all_grams4 += grams4\n\nThe list all_grams4 contains a list of every token of 4grams in these books. Let’s count them up and look at the top 10 most frequent 4 word phrases Mary Shelley used in her writing!\n\n\n\nTable 1: Top 10 4grams from Mary Shelley’s Work\n\n\n4gram\ncount\n\n\n\n\nProject_Gutenberg_Literary_Archive\n52\n\n\nthe_Project_Gutenberg_Literary\n44\n\n\nGutenberg_Literary_Archive_Foundation\n34\n\n\nthe_terms_of_this\n32\n\n\nProject_Gutenberg-tm_electronic_works\n30\n\n\nat_the_same_time\n25\n\n\nto_the_Project_Gutenberg\n24\n\n\n*_*_*_*\n22\n\n\nin_the_United_States\n21\n\n\nfor_the_sake_of\n21\n\n\n\n\n\nSo, either Mary Shelly was obsessed with the Project Gutenberg Literary Archive, and the terms of this and for the sake of, or something else is going on.\nAs it turns out, every plain text Project Gutenberg book has header information with a short version of the users’ rights and other metadata information, and then at the end has the entirety of the Project Gutenberg License, which is written in legal language.\nIn any corpus building project, decisions need to be made about how header, footer, and general boilerplate data like this will be treated. There are handy packages for python and R that make stripping out the legal language easy\n\npython: gutenbergpy\nR: gutenbergr\n\nOr, you might decide to leave it all in. It seems pretty clear this is the approach to the dataset they trained GPT-3 on, because if you prompt it with the first few lines of the Project Gutenberg license, it will continue it.\n\nFigure 1: The original Project Gutenberg License vs what GPT3 reproduces\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting aside the issue of headers and footers, we also need to deal with the fact that “markup” is everywhere. Even in the relatively plain text of Project Gutenberg books, they use underscores _ to indicate italics or emphasized text.\n\n# python\nunderscore_lines = [line \n                      for line in book_lines \n                        if any([\"_\" in word \n                                  for word in line])]\nfor i in range(4):\n  print(\" \".join(underscore_lines[i]))\n\nMathilda _is being published\nof_ Studies in Philology.\nnovelette _Mathilda_ together with the opening pages of its rough\ndraft, _The Fields of Fancy_. They are transcribed from the microfilm\n\n\nThis, again, is something we need to decide whether or not we want to include in our corpora. For these massive language models that focus on text generation, they may want the model to generate markup along with the text, so they might leave it in. Some text markup that’s intended to indicate prosodic patterns could be useful to leave in from a linguistic theory perspective.\nEither way, though, it is still a decision that needs to be made about the data."
  },
  {
    "objectID": "lectures/data_processing/index.html#text-normalization",
    "href": "lectures/data_processing/index.html#text-normalization",
    "title": "Data Processing",
    "section": "Text Normalization",
    "text": "Text Normalization\nI called the issues above “data wrangling”, since it’s mostly about identifying the content we want to be including in our modelling. But once you’ve done that, there are still questions about how we process data for analysis which fall under “text normalization”.\nConsider the following sentences\n\nThe 2019 film Cats is a movie about cats. Cats appear in every scene. A cat can always be seen.\n\nLet’s split this sentence up along whitespace1, and count how many times “cats” appears.\n\nimport re\nphrase = \"\"\"The 2019 film Cats is a movie about cats. \nCats appear in every scene. \nA cat can always be seen\"\"\"\n\nwords = re.split(\"\\s\", phrase)\ncat_c = Counter(words)\n\n\n\n\nTable 2: frequency of “cats”\n\n\n\n\n\n\nword\ncount\n\n\n\n\nCats\n2\n\n\ncats.\n1\n\n\ncat\n1\n\n\n\n\n\nA very important thing to keep in mind is that our language models will treat the words in these rows as three completely separate word types.2 That even includes the period . in the second row. Some typical steps involve\n\nseparating punctuation from words\n“case folding” or converting everything to lowercase.\n\n\nwords2 = re.split(r\"\\s\", phrase)\nwords2 = [re.sub(r\"\\W\", '', word) for word in words2]\nwords2 = [word.lower() for word in words2]\nwords2\n\n['the', '2019', 'film', 'cats', 'is', 'a', 'movie', 'about', 'cats', '', 'cats', 'appear', 'in', 'every', 'scene', '', 'a', 'cat', 'can', 'always', 'be', 'seen']\n\n\n\n\n\nTable 3: frequency of tokenized “cats”\n\n\nword\ncount\n\n\n\n\ncats\n3\n\n\ncat\n1\n\n\n\n\n\nWe’ve now got a slightly better set of counts. With the punctuation stripped and everything pushed to lowercase, there’s now just two word forms: cats and cat.\nOne downside, though, is we’ve also collapsed together the title Cats, which refers to either a Broadway musical or a 2019 film, and the word “cats” which refers to furry felines. Merging these two together could be sub-optimal for later tasks, like, say, sentiment analysis of movie reviews.\n\n‘Cats’ is both a horror and an endurance test, a dispatch from some neon-drenched netherworld where the ghastly is inextricable from the tedious. – LA Times"
  },
  {
    "objectID": "lectures/data_processing/index.html#tokenization-or-text-is-complex",
    "href": "lectures/data_processing/index.html#tokenization-or-text-is-complex",
    "title": "Data Processing",
    "section": "Tokenization (or, text is complex)",
    "text": "Tokenization (or, text is complex)\nSetting aside semantic issues, there are a lot of things that happen inside of text, especially if it is transcribed speech, that makes normalizing text and tokenizing it way more challenging than just splitting up on white space and stripping out punctuation, even just for English.\n\nPlaces to leave in punctuation\nSome examples given by Jurafsky & Martin for where you might want to leave in punctuation are:\n\nYou don’t want to eliminate punctuation from inside Ph.D, or m.p.h.. You also don’t want to eliminate it from some proper names, like ampersands in Procter & Gamble, Texas A&M, A&W, m&m's.\nYou’ll want to keep formatting in numerals, and not split them into separate words. These are all possible numeric formats cross culturally for the same quantity\n\n1,000.55\n1.000,55\n1 000,55\n\nCurrency symbols should probably be kept together with their numerals, and depending on the culture & denomination.\n\n$0.99\n99¢\n€0,99\n\nDates: There are so many different permutations on how dates can be formatted that I shouldn’t list them all here, but here are some.3\n\nyyyy-mm-dd 2022-09-12, yyyy/mm/dd 2022/09/12\nyyyy-m-dd 2022-9-12, yyyy/m/dd 2022/9/12\ndd-mm-yyyy 12-09-2022, dd/mm/yyyy 12/09/2022\ndd-m-yyyy 12-9-2022, dd/m/yyyy 12/9/2022\ndd-mm-yy 12-09-22, dd/mm/yy 12/09/2022\nmm-dd-yyyy 09-12-2022, mm/dd/yyyy 09/12/2022\nm-dd-yyyy 9-12-2022, m/dd/yyyy 9/12/2022\nmm-dd-yy 09-12-22, mm/dd/yy 09/12/22\nm-dd-yy 9-12-22, m/dd/yy 9/12/22\n\nEmoticons,4 where the token is entirely punctuation :), >.<.\n\n\n\nPlaces to split up words\nSometimes the tokens you get back from whitespace tokenization ought to be split up even further. One example might be hyphenated words, like hard-won.\n\nhard-won ➔ hard, won or hard, -, won.\n\nAnother example involves clitics, like n't or 're in English.\n\nisn't ➔ is, n't\ncan't ➔ ca, n't\nwhat're ➔ what, 're\n\n\n\nPlaces to glue words together\nYou might want to also glue together tokens from whitespace tokenization.\n\nNew, York, City ➔ New York City\nSuper, Smash, Brothers ➔ Super Smash Brothers.\n\n\n\nChallenges with speech and text\n\n: $1500\n\n: “one thousand five hundred dollars”\n: “fifteen hundred dollars”\n: “one and a half thousand dollars”\n: “one point five thousand dollars”"
  },
  {
    "objectID": "lectures/data_processing/index.html#tokenizers",
    "href": "lectures/data_processing/index.html#tokenizers",
    "title": "Data Processing",
    "section": "Tokenizers",
    "text": "Tokenizers\nThere seem to be broadly two kinds of tokenizers people use, depending on their goals.\n\nTokenizers that try to hew to linguistic structure, and can generate relatively large vocabulary sizes (number of tokens).\nTokenizers that try to keep the vocabulary size relatively small, to make neural network training possible.\n\n\nWord/language piece based tokenizers\nThere are a number of tokenizers available through the nltk (Natural Language Took Kit) (Bird, Klein, and Loper 2009) python package. They all have slightly different settings and outcomes. Here I’ll compare the PennTreeBank tokenizer, a simpler punctuation-based tokenizer, and a “casual” tokenizer.\n\nPennTreeBank\nThe PennTreeBank tokenizer is built up out of regular expressions (more on that soon). It\n\nseparates out punctuation and non-alphanumeric characters as their own tokens\nSeparates off contractions as their own tokens, using a fixed list\n\n\nfrom nltk.tokenize import sent_tokenize, TreebankWordTokenizer\n\nphrase2 = \"\"\"CATS had a budget of $100,000,000, most of which went into the so-called 'digital fur technology'.\nIt's a little hard to believe, but it only made $75.5 million at the box office. \n#badmovie :-P\n\"\"\"\n\nsentences = sent_tokenize(phrase2)\ntokens = [TreebankWordTokenizer().tokenize(s) for s in sentences]\n\n\n\n\nCATS\nhad\na\nbudget\nof\n$\n\n\n100,000,000\n,\nmost\nof\nwhich\nwent\n\n\ninto\nthe\nso-called\n'digital\nfur\ntechnology\n\n\n'\n.\nIt\n's\na\nlittle\n\n\nhard\nto\nbelieve\n,\nbut\nit\n\n\nonly\nmade\n$\n75.5\nmillion\nat\n\n\nthe\nbox\noffice\n.\n#\nbadmovie\n\n\n:\n-P\n\n\n\n\n\n\n\n\n\nSimple whitespace + punctuation tokenizer\nSplits strings based on whitespace & non-alphanum\n\nfrom nltk.tokenize import wordpunct_tokenize\ntokens = [wordpunct_tokenize(s) for s in sentences]\n\n\n\n\nCATS\nhad\na\nbudget\nof\n$\n\n\n100\n,\n000\n,\n000\n,\n\n\nmost\nof\nwhich\nwent\ninto\nthe\n\n\nso\n-\ncalled\n'\ndigital\nfur\n\n\ntechnology\n'.\nIt\n'\ns\na\n\n\nlittle\nhard\nto\nbelieve\n,\nbut\n\n\nit\nonly\nmade\n$\n75\n.\n\n\n5\nmillion\nat\nthe\nbox\noffice\n\n\n.\n#\nbadmovie\n:-\nP\n\n\n\n\n\n\nTweet Tokenizer\nIntended to be more apt for tokenizing tweets.\n\nfrom nltk.tokenize import TweetTokenizer\ntokens = [TweetTokenizer().tokenize(s) for s in sentences]\n\n\n\n\nCATS\nhad\na\nbudget\nof\n$\n\n\n100,000\n,\n000\n,\nmost\nof\n\n\nwhich\nwent\ninto\nthe\nso-called\n'\n\n\ndigital\nfur\ntechnology\n'\n.\nIt's\n\n\na\nlittle\nhard\nto\nbelieve\n,\n\n\nbut\nit\nonly\nmade\n$\n75.5\n\n\nmillion\nat\nthe\nbox\noffice\n.\n\n\n#badmovie\n:-P\n\n\n\n\n\n\n\n\n\n\nFixed Vocab Tokenizing\nThe downside of tokenizers like the three above is that you can’t pre-specify how many types you will get out. That is, you can’t pre-specify your vocabulary size. That isn’t ideal for neural-network based models, which need to use matrices of finite and pre-specified size. So there are also tokenizers that keep a fixed cap on the vocabulary size, even if they result in tokens that aren’t really linguistically meaningful.\n\nByte Pair Encoding\n\n\n\nWords\n\n\nc a t s _\nc a n ' t _\nc a n t e r _\n\n\n\n\nVocabulary\n\n\n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", ' '}\n\n\n\n\n\n\n\n\n\nWords\n\n\nca t s _\nca n ' t _\nca n t e r _\n\n\n\n\nVocabulary\n\n\n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", ' ', 'ca'}\n\n\n\n\n\n\nWords\n\n\nca t s _\ncan ' t _\ncan t e r _\n\n\n\n\nVocabulary\n\n\n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", ' ', 'ca', 'can'}\n\n\n\n\n\nTraining\n\nimport gutenbergpy.textget\nimport sentencepiece as spm\n\nraw_book = gutenbergpy.textget.get_text_by_id(84)\nclean_book  = gutenbergpy.textget.strip_headers(raw_book)\nwith open(\"texts/frank.txt\", 'wb') as file:\n  x = file.write(clean_book)\n  file.close()\n\n\nspm.SentencePieceTrainer.train(input = \"texts/frank.txt\", \n                               model_prefix = \"m\",\n                               vocab_size = 5000, \n                               model_type = \"bpe\")\n\n\nsp = spm.SentencePieceProcessor(model_file='m.model')\n\n\npara = \"\"\"\nYou will rejoice to hear that no disaster has accompanied the\ncommencement of an enterprise which you have regarded with such evil\nforebodings. I arrived here yesterday, and my first task is to assure\nmy dear sister of my welfare and increasing confidence in the success\nof my undertaking\n\"\"\"\n\nsp.encode_as_pieces(para)\n\n['▁You', '▁will', '▁rejo', 'ice', '▁to', '▁hear', '▁that', '▁no', '▁disaster', '▁has', '▁accompanied', '▁the', '▁commencement', '▁of', '▁an', '▁enterprise', '▁which', '▁you', '▁have', '▁regarded', '▁with', '▁such', '▁evil', '▁fore', 'bod', 'ings', '.', '▁I', '▁arrived', '▁here', '▁y', 'esterday', ',', '▁and', '▁my', '▁first', '▁task', '▁is', '▁to', '▁assure', '▁my', '▁dear', '▁sister', '▁of', '▁my', '▁we', 'lf', 'are', '▁and', '▁incre', 'asing', '▁confidence', '▁in', '▁the', '▁success', '▁of', '▁my', '▁undertaking']\n\n\n\n\nThe Benefit?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmethod\nonce\ntotal\nproportion once\n\n\n\n\nbpe\n262\n4652\n0.056\n\n\nptb\n3533\n7710\n0.458"
  },
  {
    "objectID": "lectures/what_is_nlp/index.html#nlp-in-computational-linguistics",
    "href": "lectures/what_is_nlp/index.html#nlp-in-computational-linguistics",
    "title": "What is NLP?(for this course)",
    "section": "NLP \\(\\in\\) Computational Linguistics",
    "text": "NLP \\(\\in\\) Computational Linguistics\nIn set notation, \\(\\in\\) means “is an element of”. That is, there’s a large set of things called “Computational Linguistics”, and NLP is a part of that larger set.\n“Computational Linguistics” covers a very broad range of topics. Natural Language Processing is currently an area of research and application that receives a lot of attention & money, but Computational Linguistics is a much broader umbrella term. The Association for Computational Linguistics defines it as\n\nComputational linguistics is the scientific study of language from a computational perspective. Computational linguists are interested in providing computational models of various kinds of linguistic phenomena. These models may be “knowledge-based” (“hand-crafted”) or “data-driven” (“statistical” or “empirical”). Work in computational linguistics is in some cases motivated from a scientific perspective in that one is trying to provide a computational explanation for a particular linguistic or psycholinguistic phenomenon; and in other cases the motivation may be more purely technological in that one wants to provide a working component of a speech or natural language system. Indeed, the work of computational linguists is incorporated into many working systems today, including speech recognition systems, text-to-speech synthesizers, automated voice response systems, web search engines, text editors, language instruction materials, to name just a few."
  },
  {
    "objectID": "lectures/what_is_nlp/index.html#some-examples-of-x-x-in-textcltextandx-ne-textnlp",
    "href": "lectures/what_is_nlp/index.html#some-examples-of-x-x-in-textcltextandx-ne-textnlp",
    "title": "What is NLP?(for this course)",
    "section": "Some examples of \\(\\{x | x \\in \\text{CL}~\\text{and}~x \\ne \\text{NLP}\\}\\)",
    "text": "Some examples of \\(\\{x | x \\in \\text{CL}~\\text{and}~x \\ne \\text{NLP}\\}\\)\nThis is set notation that means “the set of things, such that each thing is in Computational Linguistics, and the thing is not Natural Language Processing\n\nFormalizing Theory\nOne use of computational linguistics is to formalize linguistic theories into a computational framework. This might seem weird, since a lot of linguistic theory already looks very formal. But giving mathy looking definitions in a verbal description of a theory is a very different thing from implementing that theory in code that will run.\nSome examples are\n\nMinimalist Parsers (Berwick and Stabler 2019) implementing parsers for Minimalist Syntax\nThe Gradual Learning Algorithm (Boersma and Hayes 2001) implementing constraint re-ranking in Optimality Theory\nThe Tolerance Principle (Charles. Yang 2016), formalizing how learners might acquire rules that have exceptions.\n\nThe interesting thing with formalizing verbal theories, computationally, is that things that might seem like big differences in the verbal theories could turn out to be computationally identical, and some things that might not seem like a big difference can turn out to be massively different computationally.\n\n\nConceptual Experiments\nYou can use general computational principles to flesh out what you would expect to happen given under specific theories, or to use specific computational implementations of linguistic theory to explore their consequences.\nHere’s a little example from dialectology. We have two proposed principles:\n\nGarde’s Principle: Mergers are irreversible by linguistic means\n\nonce a community gets merger, like the cot/caught merger, it cannot get back the distinction\n\nHerzog’s Corollary: Mergers expand at the expense of distinctions.\n\nonce a community develops a merger, like the cot/caught merger, it will inevitably spread geographically to other communities\n\n\nWe can translate these two principles into a “finite state automaton” below.\n\n\n\n\n\n\n\nfinite_state_machine\n\n  \n\nd\n\n ɔ/ɑ distinction   \n\nd->d\n\n  0.90   \n\nm\n\n ɔ/ɑ merger   \n\nd->m\n\n  0.10   \n\nm->d\n\n  0.01   \n\nm->m\n\n  0.99   \n\ninit\n\n   \n\ninit->d\n\n   \n\n\n\n\n\nA verbal translation of this diagram would be\n\nWe start out in a state of distinguishing between /ɔ/ and /ɑ/. With each step in time (“generation”), we probably keep distinguishing between /ɔ/ and /ɑ/ with a 0.9 probability, but there’s some chance we become a merged community. Once we become a merged community, we are overwhelmingly likely to remain a merged community with a 0.99 probability. But there is a very little probability that we might go back to being merged at 0.01 probability.\n\nUnder these circumstances, are we inevitably going to become a merged community? How long until we reach the maximum probability of becoming a merged community? We can answer these questions with a conceptual experiment, converting the description and diagram above into a transition probability matrix, and then just doing a bunch of matrix multiplications.\n\n# python\nimport numpy as np\n\nd_change = np.array([0.90, 0.10])\nm_change = np.array([0.01, 0.99])\n\nchange_mat = np.row_stack((d_change, m_change))\nprint(change_mat)\n\n[[0.9  0.1 ]\n [0.01 0.99]]\n\n\n\n# python\ninitial_state = np.array((1,0))\nn_generations = 100\ncollector = [initial_state]\n\ncurrent_state = initial_state\nfor i in range(n_generations):\n  new_state = current_state @ change_mat\n  collector.append(new_state)\n  current_state = new_state\n  \nresults_mat = np.row_stack(collector)\n\n\n\n\n\n\nLooks like with the probabilities set up this way, we’re not guaranteed to become a merged community. The probability is very high (about 0.91), but not for certain. We might say, seeing this, that unless the Garde’s Principle is absolute (it’s impossible to undo a merger by any means) then Herzog’s Corollary won’t necessarily hold.\nOther examples of conceptual experiments are\n\nC. D. Yang (2000) used a model of variable grammar learning to see if he could predict which grammars (e.g. V2 vs no-V2) would win over time.\nSneller, Fruehwald, and Yang (2019) used the tolerance principle to see if a specific phonological change in Philadelphia could plausibly develop on its own, or if it had to be due to dialect contact.\nLinzen and Baroni (2021) used RNNs (a kind of neural network) to see if “garden path” sentences (e.g. “The horse raced past the barn fell.”1) were difficult just because the word at the pivot point was especially unlikely.\n\n\n\nAgent Based Modelling\nThis doesn’t always fall under the rubric of “computational linguistics,” but agent-based modelling involves programming virtual “agents” that then “interact” with each other. Part of what you program into the simulation is rules for how agents interact with each other, and what information they exchange or adopt when they do. It’s often used to model the effect of social network structure.\n\nDe Boer (2001) models vowel system acquisition and development over time.\nStanford and Kenny (2013) explore models of geographic spread of variation.\nKauhanen (2017) explores whether any linguistic variant needs to have an advantage over another in order to become the dominant form.\n\n\n\nBuilding and using computational tools and data\nOf course, there is a massive amount of effort that goes into constructing linguistic corpora, and developing computational tools to analyze those corpora."
  },
  {
    "objectID": "lectures/what_is_nlp/index.html#nlp",
    "href": "lectures/what_is_nlp/index.html#nlp",
    "title": "What is NLP?(for this course)",
    "section": "NLP",
    "text": "NLP\nFor this class, we’ll be mostly focusing on the “Language Modeling” component of NLP, and we’ll be following the definition of “Language Model” from Bender and Koller (2020) as a model trained to predict what string or word is most likely in the context of other words. For example, from the following sentence, can you guess the missing word?\n\nI could tell he was mad from the tone of his [____]\n\n\nUsing the predictions\nLanguage model predictions are really useful for many applications. For example, let’s say you built an autocaptioning system that took audio and processed it into a transcription. You might have a situation where the following sentence gets transcribed.\n\nPoker and blackjack are both  games people play at casinos.\n\nThe digital signal, , in this sentence is consistent with two possible words here\n\ncar\ncard\n\nUs humans here know that in the context of “poker”, “blackjack”, “games” and “casinos”, the more likely word is “card”, not “car.” But a simple model that’s just processing acoustics doesn’t know that. So to improve your captioning, you’d probably want to incorporate a language model that takes the context into account and boosts the probability of “card”.\nThis is just one example, but there are many other kinds of string prediction tasks, such as:\n\nGiven a string in language A, predict the string in language B (a.k.a. machine translation).\nGiven a whole paragraph, predict a summary of the paragraph (summarization).\nGiven a question, predict an answer (question answering).\nGiven a prompt, continue the text in the same style (text generation).\n\n\n\nUsing the representations\nIn the process of training models to do text generation, they develop internal representations of strings of text that can be useful for other purposes. For example, a common NLP task is “sentiment analysis,” that could be used to analyze, say, reviews of products online.2\nOne really very simplistic approach would be to get a dictionary of words that have been scored for their “positivity” and “negativity.” Then, every one of those words or a tweet or what ever has one of those words in it, you add its score as a total sentiment score.\n\n# R\nlibrary(tidytext)\nset.seed(101)\nget_sentiments(\"afinn\") %>%\n  sample_n(10) %>%\n  kable()\n\n\n\n\nword\nvalue\n\n\n\n\nlobby\n-2\n\n\nstricken\n-2\n\n\nloser\n-3\n\n\njealous\n-2\n\n\nbreakthrough\n3\n\n\ninability\n-2\n\n\nharshest\n-2\n\n\nranter\n-3\n\n\ncried\n-2\n\n\nwarfare\n-2\n\n\n\n\n\nHere’s an example with a notable tweet.\n\n# R\ntweet <- \"IF THE ZOO BANS ME FOR HOLLERING AT THE ANIMALS I WILL FACE GOD AND WALK BACKWARDS INTO HELL\"\ntweet_df <- tibble(word = tweet %>%\n                     tolower() %>%\n                     str_split(\" \") %>%\n                     simplify()) %>%\n  left_join(get_sentiments(\"afinn\")) %>%\n  replace_na(replace = list(value = 0))\n\n\nfull tweetsum\n\n\n\n# R\ntweet_df\n\n# A tibble: 19 × 2\n   word      value\n   <chr>     <dbl>\n 1 if            0\n 2 the           0\n 3 zoo           0\n 4 bans          0\n 5 me            0\n 6 for           0\n 7 hollering     0\n 8 at            0\n 9 the           0\n10 animals       0\n11 i             0\n12 will          0\n13 face          0\n14 god           1\n15 and           0\n16 walk          0\n17 backwards     0\n18 into          0\n19 hell         -4\n\n\n\n\n\n# R\ntweet_df %>%\n  summarise(sentiment = sum(value))\n\n# A tibble: 1 × 1\n  sentiment\n      <dbl>\n1        -3\n\n\n\n\n\nHowever, this is a kind of lackluster approach to sentiment analysis nowadays. Many language models now now, as a by product of their string prediction training, have more complex representations of words than just a score between -5 and 5, and have representations of whole strings that can be used (so it won’t give the same score to “good” and “not good”).\n\n# python\nfrom transformers import pipeline\n\n# warning, this will download approx\n# 1.3G of data.\nsentiment_analysis = pipeline(\"sentiment-analysis\",\n                              model=\"siebert/sentiment-roberta-large-english\")\n\n\n# python\nprint(sentiment_analysis(\"IF THE ZOO BANS ME FOR HOLLERING AT THE ANIMALS I WILL FACE GOD AND WALK BACKWARDS INTO HELL\"))\nprint(sentiment_analysis(\"This ain't bad!\"))\n\n[{'label': 'NEGATIVE', 'score': 0.9990140199661255}]\n[{'label': 'POSITIVE', 'score': 0.9944340586662292}]"
  }
]
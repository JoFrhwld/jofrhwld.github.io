[
  {
    "objectID": "resources/reading/index.html",
    "href": "resources/reading/index.html",
    "title": "Reading a Technical Paper",
    "section": "",
    "text": "There may never be a point in your academic life where you will read a paper and understand everything in it (or if there is, I haven’t gotten there). Instead, you have to develop methods for getting whatever information you can out of a paper itself, and then draw up a list of terms and concepts to do further background research on.\nLet’s work through a sample paragraph from Bender et al. (2021) to see some of these strategies in action."
  },
  {
    "objectID": "resources/reading/index.html#a-tricky-paragraph",
    "href": "resources/reading/index.html#a-tricky-paragraph",
    "title": "Reading a Technical Paper",
    "section": "A Tricky Paragraph",
    "text": "A Tricky Paragraph\nBender et al. (2021) is an important paper about ethics and safety concerns in natural language processing. However, it could be hard to follow some of the discussion without a background in the NLP literature. Here’s a sample paragraph where they discuss specific advancements made in NLP models.\n\nThe next big step was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g. 82].\n\nMy own approach to trying to understand paragraphs (and whole papers) like this, where I might not be familiar with all of the concepts involved, is ask the following questions:\n\nCan I figure out the upshot? What message is this paragraph trying to communicate?\nCan I pick out any themes? What keeps getting repeated?\nWhat search terms can I pull out of the paper to do more background research."
  },
  {
    "objectID": "resources/reading/index.html#can-we-figure-out-the-upshot",
    "href": "resources/reading/index.html#can-we-figure-out-the-upshot",
    "title": "Reading a Technical Paper",
    "section": "Can we figure out the upshot?",
    "text": "Can we figure out the upshot?\nThis paragraph is trying to tell us something. I’ve color coded the pieces of the paragraph which I think are most useful for figuring out the point of this paragraph even if you don’t know what all the specifics mean.\n\nThe next big step1 was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art2 performance on question answering, textual entailment, semaantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well3. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary4 for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed5 to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score6 in 10 epochs as opposed to 4867 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data8. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g. 82].1 “Big steps”, improvement2 “state of the art”, (SOTA) means the best it can be3 “at first … and later” from limited applications, it expanded4 less “labeled data” needed.5 Less “training data” needed.6 Whatever the “F1 score” is, this got the best one7 Whatever an “epoch” is, this needed less of them8 The same score, but less data.\n\n\nThe upshot\n~Things~ got better with less."
  },
  {
    "objectID": "resources/reading/index.html#can-we-figure-out-themes-from-repetition",
    "href": "resources/reading/index.html#can-we-figure-out-themes-from-repetition",
    "title": "Reading a Technical Paper",
    "section": "Can we figure out themes from repetition?",
    "text": "Can we figure out themes from repetition?\nI’ve highlighted the words whose repetition struck me as being important to the theme of this paragraph.\n\nThe next big step was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g. 82].\n\nIt seems clear “training” is a big deal, and it involves data."
  },
  {
    "objectID": "resources/reading/index.html#any-search-terms-for-background-research",
    "href": "resources/reading/index.html#any-search-terms-for-background-research",
    "title": "Reading a Technical Paper",
    "section": "Any search terms for background research?",
    "text": "Any search terms for background research?\nThere are a lot of technical terms in this paragraph, but it seems like they can be classified under a few main categories.\n\nThe next big step was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While training the word embeddingsrequired a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g. 82].\n\nThere’s at least three kinds of things we could try doing some background research on here:\n\n“Models”\nThere are a few models named here. Some seem like generic names, and others seem more specific. The most generic “word embeddings” is actually defined in the paragraph\n\nrepresentations of the distribution of words\n\nThere’s probably a lot more to investigate here.\nAfter defining “word embeddings,” they say “these word vectors”, which seems to suggest that these are synonymous or at least highly interchangeable concepts.\nSpecific “systems” that generate “word vectors” are\n\nword2vec\nGloVe\n\nSome good search terms to find information about these would probably be “word2vec word vectors” or “GloVe word vectors”.\nNext they name another class of model, “LSTM models,” that generate “word vectors”, and name some specific LSTM modelsYou can also get an idea of naming conventions in this field. The name formats seem to either be thing2thing or an acronym that is a pronounceable word… and hard to search for all on its own.\n\ncontext2vec\nELMo\n\nSome good search terms here would probably be “LSTM context2vec” or “LSTM ELMo”.\n\n\n“NLP Tasks”\nAfter saying that these models are used on “NLP tasks”, they name a few specific ones. Some of them just have names, while others also have an acronym associated with them.\n\nquestion answering\ntextual entailment\nsemantic role labeling (SRL)\ncoreference resolution\nnamed entity recognition (NER)\nsentiment analysis\n\nSome of these tasks wouldn’t make for great search terms on their own, like “question answering,” but appending “NLP” to the beginning for “NLP question answering” would probably work.\n\n\nScores\nIn this paragraph, only one kind of score, the “F1 score” is mentioned. But in the context of the rest of the paper, there are a number of other “Scores” that could be important to investigate."
  },
  {
    "objectID": "resources/reading/index.html#wrapping-up",
    "href": "resources/reading/index.html#wrapping-up",
    "title": "Reading a Technical Paper",
    "section": "Wrapping up",
    "text": "Wrapping up\nNormally, the process isn’t as elaborate as it appears in this demo. I don’t usually color code all of the words in a paragraph, much less a whole paper, like this. But I do often try to mentally summarize paragraphs and sections with what the upshot is. Some authors are better than others in getting across their point in among the technical aspects, but ideally they are always trying to communicate some message that you can at least approximate even if you don’t understand everything in detail."
  },
  {
    "objectID": "resources/notation/index.html",
    "href": "resources/notation/index.html",
    "title": "Mathematical Notation",
    "section": "",
    "text": "The most common variables we’re going to be seeing:\n\n\\(x, y\\)\n\nStand-ins for numbers, usually a list, or vector, of numbers\n\\(y\\) is often some kind of “outcome” variable\n\\(x\\) is often some kind of input, or predictor variables\ne.g. “We want to predict how many ice cream cones, \\(y\\), we’ll sell if it’s a specific temperature, \\(x\\).\n\n\\(X, Y\\)\n\nStand ins for categorical variables\n\n\\(w\\)\n\nA special variable for a word\n\n\\(c\\)\n\nA “count” of something\n\n\\(p\\)\n\nA probability\n\n\\(N\\)\n\nUsually, the total number of something\n\n\\(n\\)\n\nA contextual number. E.g. “In \\(n+1\\) days (that is, tomorrow)…”\n\n\\(n,m\\)\n\nWhen \\(n\\) and \\(m\\) are used together, it’s often to describe the number of rows and columns of a matrix.\n\n\\(A, B\\)\n\nThese are almost always used for matrices\nCapital roman letters are often a clue we’re looking at a matrix (but not always)\n\n\\(\\lambda\\)\n\n“lambda”\nOften used for an arbitrary value you multiply things by.\n\n\\(k\\)\n\nOften used for an arbitrary value you add things to.\n\n\\(\\delta\\)\n\n“delta”\nOften used to describe a difference of some kind\n\n\\(\\alpha, \\beta, \\gamma\\)\n\n“alpha”, “beta”, “gamma”\nUsed for model parameters\n\n\\(\\theta\\)\n\n“theta”\nAlso used as a model parameter.\nOr, to describe an angle (in radians)\n\n\n\n\n\n\\(\\hat{y}\\)\n\n“y hat”\nA predicted value for \\(y\\)\ne.g. “I predicted \\(\\hat{y}\\) ice cream cones to be sold, but they actually sold \\(y\\).\n\n\\(\\bar{y}\\)\n\n“y bar”\nThe average value of \\(y\\)\n\n\\(y^*\\)\n\n“y star”\nA modified value of \\(y\\)"
  },
  {
    "objectID": "resources/notation/index.html#one-dimensional",
    "href": "resources/notation/index.html#one-dimensional",
    "title": "Mathematical Notation",
    "section": "One Dimensional",
    "text": "One Dimensional\nWhen we have a variable that contains a list of values, each individual value will be described with an “index”. For example, if we had a variable \\(X\\) that contained the names of the week.\n\\[\nX = (\\text{Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday})\n\\]\nThen the first value in the list would be \\(X_1\\), and we could say \\(X_1 = \\text{Monday}\\). You’d usually pronounce \\(X_1\\) as “X sub 1”.\nSometimes we want to be able to refer to a generic value in the list \\(X\\) and for that we’d use an index variable like \\(X_i\\) (pronounced “X sub i”). The most common letters used to indicate generic indices are \\(i, j, k\\).\nWe can do math on the indices also. So, if the name of today is \\(X_i\\), then the name of tomorrow is \\(X_{i+1}\\). The name of yesterday is \\(X_{i-1}\\). The day after tomorrow would be \\(X_{i+2}\\).\nWe can also include a range of numbers in the indices. So, the names of the first three days of the week are \\(X_{1:3} = (\\text{Monday, Tuesday, Wednesday})\\). The names of yesterday, today, and tomorrow are \\(X_{i-1:i+1}\\)."
  },
  {
    "objectID": "resources/notation/index.html#two-dimensional",
    "href": "resources/notation/index.html#two-dimensional",
    "title": "Mathematical Notation",
    "section": "Two Dimensional",
    "text": "Two Dimensional\nWe could imagine a Month as being a matrix \\(M\\) made up of 4 weeks, with each week being 7 days.\n\\[\nM = \\left[ \\begin{array}{lllllll}\n\\text{Monday} & \\text{Tuesday} & \\text{Wednesday} & \\text{Thursday} &  \\text{Friday} &     \\text{Saturday} & \\text{Sunday} \\\\\n\\text{Monday} & & & \\dots & & & \\text{Sunday}\\\\\n\\vdots & & & \\ddots\\\\\n\\text{Monday} & & & \\dots & & &\\text{Sunday}\\\\\n\\end{array} \\right]\n\\]\nIf we wanted to get the name of the third day during the first week, it would be \\(M_{1, 3} = \\text{Wednesday}\\). When giving numeric indices of a matrix, the rows (the parts going across) come first, and the columns (the parts going up and down) come second. So, we’d describe the matrix \\(M\\) as being a \\(4\\times7\\) “four by seven” matrix.\nTo refer to a day of the month generically, we’d use \\(M_{i,j}\\) (pronounced “M sub i, j”). We can also use ranges in these indices. So, to refer to the second week of the month, we’d use \\(M_{2,1:7}\\). Or, to refer to all of the Saturdays in the month, we’d use \\(M_{1:4,6}\\).\nAnd, we can also use math in these indices. So, we could refer to “a week from today” with \\(M_{i+1,j}\\)."
  },
  {
    "objectID": "resources/notation/index.html#matrix-transposition",
    "href": "resources/notation/index.html#matrix-transposition",
    "title": "Mathematical Notation",
    "section": "Matrix Transposition",
    "text": "Matrix Transposition\nThe one special notation related to matrices is “transposition”, which basically takes a matrix and flips it.\n\\[\nA = \\left[\\begin{array}{ccc}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{array}\n\\right]\n\\]\n\\[\nA^\\intercal = \\left[\\begin{array}{cc}1 & 4\\\\\n2 & 5 \\\\\n3 & 6 \\end{array}\\right]\n\\]\nThe little \\(^\\intercal\\) indicates that we’re transposing the matrix. You’d pronounce \\(A^\\intercal\\) as “A transpose”.\nThe transpose operation can apply to single lists too. This will eventually be important.\n\\[\nx = [1, 2, 3, 4]\n\\]\n\\[\nx^\\intercal = \\left[\\begin{array}{c} 1\\\\2\\\\3\\\\4 \\end{array}\\right]\n\\]\nAgain, \\(x^\\intercal\\) is pronounced “x transpose.”"
  },
  {
    "objectID": "resources/notation/index.html#generic",
    "href": "resources/notation/index.html#generic",
    "title": "Mathematical Notation",
    "section": "Generic",
    "text": "Generic\n\n\\(f(), g()\\)\n\nThese are the most common “functions” you’ll see. Usually in a whole formula like \\(y = f(x)\\) (pronounced “y equals f of x”).\nIt means “we stick the value \\(x\\) in and \\(y\\) comes out.\nWhat \\(f()\\) or \\(g()\\) (or whatever name we give the function) is needs to be defined. They don’t have a fixed meaning."
  },
  {
    "objectID": "resources/notation/index.html#specialized",
    "href": "resources/notation/index.html#specialized",
    "title": "Mathematical Notation",
    "section": "Specialized",
    "text": "Specialized\n\n\\(P()\\)\n\nThe function \\(P()\\) refers to the probability of whatever we put in.\n\\(P(X_i)\\) returns the probability of a specific \\(X_i\\) value.\nWe’d refer to the specific value the \\(P()\\) returns with the variable \\(p\\), usually with the same index. So \\(p_i = P(X_i)\\).\n\n\\(C()\\)\n\nThe function \\(C()\\) returns the count of whatever we put in.\nIn the matrix above, \\(C(\\text{Monday}) = 4\\)\nWe’d refer to the specific count value of something with the variable \\(c\\), usually with the same index. So \\(c_{i} = C(X_i)\\)."
  },
  {
    "objectID": "resources/notation/index.html#summation",
    "href": "resources/notation/index.html#summation",
    "title": "Mathematical Notation",
    "section": "Summation",
    "text": "Summation\n\\(\\sum\\)\nThis operator indicates that we are adding together numbers in a list. Let’s look at the table of the height of actors, in cm who played Spider-Man in Spider-Man: No Way Home.\n\n\n\n\nActor\nHeight (cm)\n\n\n\n\nTobey Maguire\n172\n\n\nAndrew Garfield\n179\n\n\nTom Holland\n169\n\n\n\n\nWe would say that there are \\(N=3\\) actors who played Spider-Man in the movie. And we could represent their heights in a variable \\(y\\), and say\n\\[\ny = (172, 179, 169)\n\\]\nThe height of the first actor would be \\(y_1\\), which equals \\(172\\), and the way to refer to any given height on the list would be \\(y_i\\).\nTo get the total height of the actors (like if one stood on the head of the other), we would have to sum it up, which we could represent like this:\n\\[\nh = y_1 + y_2 + y_3\n\\]\nOr, we could use summation notation\n\\[\nh = \\sum_{i=1}^Ny_i\n\\]\nThe way to read this out loud is “h equals the sum of y sub i from i equals 1 to N”. The \\(i=1\\) part underneath the \\(\\sum\\) means “start getting values out of \\(y\\) starting with 1”. The \\(N\\) at the top means “keep adding 1 to \\(i\\) until \\(i = N\\).”\nWhat the whole notation is going to do is pull out every value of \\(y\\) and add them together.\nIf you know how to do some coding, sometimes it’s easier to understand the mathematical notation to see it in code.\n\n```{python}\ny = [172, 179, 169]\nN = 3 # = len(y)\nh = 0 \n\nfor i in range(N):\n  h = h + y[i]\n\nprint(h)\n```\n\n520"
  },
  {
    "objectID": "resources/notation/index.html#product",
    "href": "resources/notation/index.html#product",
    "title": "Mathematical Notation",
    "section": "Product",
    "text": "Product\n\\(\\prod\\)\nThe product operator, \\(\\prod\\), works a lot like the the summation operator, except instead of adding numbers together, it multiplies them. For example, let’s say we’re keeping track of the day-to-day changes in the number of visitors to a website.\n\n\n\nDay\nPercent Change\nMultiplier\n\n\n\n\n1\nup 1%\n1.01\n\n\n2\nno change\n1.00\n\n\n3\ndown 5%\n0.95\n\n\n\nWe can get the total proportional change over these three days by multiplying the proportions together. Writing it out the long way, it’s\n\\[\nN = 3\n\\]\n\\[\ny = (1.01, 1, 0.95)\\\\\n\\]\n\\[\nt = y_1\\cdot y_2\\cdot y_3\n\\]\nIn product notation, though, it looks like this.\n\\[\nt = \\prod_{i=1}^Ny_i\n\\]\nAgain, if you’re more comfortable with programming code, it’s equivalent to this.\n\n```{python}\ny = [1.01, 1, 0.95]\nN = 3\nt = 1\n\nfor i in range(N):\n  t = t * y[i]\n\nprint(f\"{t:.4}\")\n```\n\n0.9595"
  },
  {
    "objectID": "resources/notation/index.html#conditional-probability",
    "href": "resources/notation/index.html#conditional-probability",
    "title": "Mathematical Notation",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nThe “conditional probability” is the probability of some value or event \\(Y\\), holding constant some other value or event \\(X\\) . For example, maybe\n\\[\nY = \\text{I am teaching}\n\\]\nand\n\\[\nX_1 = \\text{It is a Monday}\n\\]\nand\n\\[\nX_6=\\text{It is a Saturday}\n\\]\nThe probability that I am teaching a class is is a lot lower on a Saturday than on a Monday. We can express these like so.\n\\[\np_1 = P(Y | X_1)\n\\]\na.k.a “p sub 1 equals the probability I am teaching, given that it is a Monday”.\n\\[\np_6 = P(Y|X_6)\n\\]\na.k.a. “p sub 6 equals the probability I am teaching given that it is a Saturday”\nand\n\\[\np_6 < p_1\n\\]\nThe key piece of notation in the expressions above is the \\(|\\) (the “pipe”) inside of the \\(P()\\) function."
  },
  {
    "objectID": "resources/notation/index.html#joint-probability",
    "href": "resources/notation/index.html#joint-probability",
    "title": "Mathematical Notation",
    "section": "Joint Probability",
    "text": "Joint Probability\nThe joint probability is the probability of two values or events happening together. It’s not the same as a conditional probability of one event given the other, but explaining why requires more time and space.\nIf we stick with the same events as above (“I am teaching” and “It is a Monday”) the joint probability of “I am teaching and it is a Monday” would be notated as\n\\[\nq_1 = P(Y,X_1)\n\\]\nThe comma inside the \\(P()\\) function means “and.”\n\n\npdf\n\n\n\n\nCC BY-SA 4.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lin517: Natural Language Processing",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nModified\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n2022-08-20\n\n\n2022-10-02\n\n\nMathematical Notation\n\n\n\n\n\n\n2022-08-22\n\n\n2022-09-06\n\n\nReading a Technical Paper\n\n\n\n\n\n\n2022-08-24\n\n\n2022-09-08\n\n\nWhat is NLP?(for this course)\n\n\n\n\n\n\n2022-08-31\n\n\n2022-09-11\n\n\nData Sparsity\n\n\n\n\n\n\n2022-09-02\n\n\n2022-09-08\n\n\nStarting Python\n\n\npython\n\n\n\n\n2022-09-06\n\n\n2022-10-02\n\n\nData Processing\n\n\n\n\n\n\n2022-09-09\n\n\n2022-10-03\n\n\nLists and Dictionaries\n\n\npython\n\n\n\n\n2022-09-13\n\n\n2022-09-13\n\n\nAddendum\n\n\n\n\n\n\n2022-09-13\n\n\n2022-10-02\n\n\nLemmatizing and Stemming\n\n\n\n\n\n\n2022-09-16\n\n\n2022-09-15\n\n\nLoops Etc.\n\n\npython\n\n\n\n\n2022-09-23\n\n\n2022-10-09\n\n\nComprehensions and Useful Things\n\n\npython\n\n\n\n\n2022-09-28\n\n\n2022-10-02\n\n\nngram Language Models\n\n\n\n\n\n\n2022-09-30\n\n\n2022-10-09\n\n\nMaking and Counting Bigrams\n\n\npython\n\n\n\n\n2022-10-03\n\n\n2022-10-02\n\n\nEvaluating models\n\n\n\n\n\n\n2022-10-10\n\n\n2022-10-09\n\n\nngrams - Perplexity\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "python_sessions/04_session5.html",
    "href": "python_sessions/04_session5.html",
    "title": "Making and Counting Bigrams",
    "section": "",
    "text": "The goal is, for a given book, find\n\nThe token most likely to follow “the”. What is its conditional probability?\nWhat is that token’s overall probability in the book?\nHow much does knowing the preceding word is “the” boost that token’s probability vs not know what the preceding word is?\n\n\n\nI’ve written the getbook.py script for you to be able to quickly download a book from project gutenberg with the header and license info stripped out. You can use it like this, in the shell:\n# bash\npython getbook.py 84 frankenstein.txt\n\n\n\n\n\nAfter reading in a book (and potentially .strip()ing off leading and trailing whitespace), you’ll need to glue all of the lines together into one big megastring for tokenizing. You can do that like so:\n```{python}\nmegastring = \" \".join(book_lines)\n```\n\n\n\nThere’s a convenient function called collections.Counter() that counts how many things are in a list, and returns a dictionary keyed by the values it counted, with its values as the dictionary values.\n\n#python\nfrom collections import Counter\n\nletters = [\"a\", \"a\", \"b\", \"b\", \"b\"]\nletters_c = Counter(letters)\n\nprint(letters_c)\n\nprint(letters_c[\"b\"])\n\nCounter({'b': 3, 'a': 2})\n3\n\n\nYou can also get the most common value from the counting dictionary with .most_common(1). This returns a list of “tuples”\n\n# python\nprint(letters_c.most_common(1))\n\n[('b', 3)]\n\n\n\n\n\n\nnltk has a few functions that will make this go easier.\n\n\nYou might need to run nltk.download('punkt')\n\n\n\nIn a long paragraph or a “megastring”, if we want bigram counts that are sensitive to sentence boundaries, that means we need to first split it up into sentences. We can do that with ntlk.sent_tokenize()\n\nimport pprint\npp = pprint.PrettyPrinter(indent = 2)\n\n\n# python\nfrom nltk import sent_tokenize\n\n\npara = \"This is a sentence. This is a sentence too. Is this?\"\nsentences = sent_tokenize(para)\npp.pprint(sentences)\n\n['This is a sentence.', 'This is a sentence too.', 'Is this?']\n\n\n\n\n\nDon’t forget to tokenize sentences into words\n\n# python\nfrom nltk import word_tokenize\n\nsentence_words = [word_tokenize(sent) for sent in sentences]\npp.pprint(sentence_words)\n\n[ ['This', 'is', 'a', 'sentence', '.'],\n  ['This', 'is', 'a', 'sentence', 'too', '.'],\n  ['Is', 'this', '?']]\n\n\n\n\n\nWe’ll also want to put start-of-sentence and end-of-sentence padding on each sentence, which we can do with nltk.lm.preprocessing.pad_both_ends()\n\n# python\n\nfrom nltk.lm.preprocessing import pad_both_ends\n\n# n = 2 because we're *going* to do bigrams\n# pad_both_ends returns a special object we're\n# converting to a list, just to see what's happening\nsentence_padded = [list(pad_both_ends(sent, n = 2)) \n                     for sent in sentence_words]\npp.pprint(sentence_padded)\n\n[ ['<s>', 'This', 'is', 'a', 'sentence', '.', '</s>'],\n  ['<s>', 'This', 'is', 'a', 'sentence', 'too', '.', '</s>'],\n  ['<s>', 'Is', 'this', '?', '</s>']]\n\n\n\n\n\nWe (finally!) get the bigrams in each sentence nltk.bigrams().\n\n# python\nfrom nltk import bigrams\n\n# Again, bigrams() returns a special object we're\n# converting to a list\nsent_bg = [list(bigrams(sent)) \n             for sent in sentence_padded]\npp.pprint(sent_bg)\n\n[ [ ('<s>', 'This'),\n    ('This', 'is'),\n    ('is', 'a'),\n    ('a', 'sentence'),\n    ('sentence', '.'),\n    ('.', '</s>')],\n  [ ('<s>', 'This'),\n    ('This', 'is'),\n    ('is', 'a'),\n    ('a', 'sentence'),\n    ('sentence', 'too'),\n    ('too', '.'),\n    ('.', '</s>')],\n  [('<s>', 'Is'), ('Is', 'this'), ('this', '?'), ('?', '</s>')]]\n\n\n\n\n\nBefore you try counting anything, you’re going to need to “flatten” this list of lists into just one flat list of all of the bigrams.\nleft as an exercise to the reader.\n\n\n\n\nWhen I find the “conditional probability” of the most common word following “the”, what I mean is “What is the probability of the word w, given that we just had ‘the’?”. Or, to put it in math terms \\(P(w | \\text{the})\\).\nThe conditional probability \\(P(w | \\text{the})\\) is equal to the joint probability of P(the, w) (a.k.a. the probability of that bigram out of all bigrams) divided by the probability of just “the”, \\(P(\\text{the})\\).\n\\[\nP(w|\\text{the}) = \\frac{P(\\text{the}~w)}{P(\\text{the})}\n\\]\nTo get the probablity of \\(P(\\text{the}~w)\\), you’ll need to divide the count of “the w” by the count of all bigram tokens (hint: this is just how long the list of bigrams is.)\nTo get the probability of just “the”, you’ll actually have to get a separate count of just all individual tokens, count how frequent “the” is, and divide that by the number of total tokens.\n\n\n\nTake a moment or two to list out each piece of code or information you’re going to need to get to do this project, at a high level. It doesn’t need to be complete, and you’ll probably come back to this list and revise it. But having a list like this will help guide you to what the next step in the process is."
  },
  {
    "objectID": "python_sessions/02_session3.html",
    "href": "python_sessions/02_session3.html",
    "title": "Loops Etc.",
    "section": "",
    "text": "Python gets really useful when we stop typing our data into the script directly, and start reading it in programmatically. There are lots of kinds of files you might want to read into python, and sometimes specialized libraries are involved.\n\n\n\n\n\n\nNote\n\n\n\nWe’re not going to be dealing with many of these file formats, but just to keep your imaginations open about the kind of data you can read into python:\n\n\n# comes with python\nimport json\nwith open(\"file.json\", \"r\") as f:\n  data = json.load(f)\n\n\n\n# requires pip install pyyaml\nimport yaml\nwith open(\"file.yml\", \"r\") as f:\n  data = yaml.safe_load(f)\n\n\n\n# requires pip install pandas\nimport pandas as pd\ndata = pd.read_csv(\"file.csv\")\n\n\n\n# requires pip install librosa\nwav, sr = librosa.load(\"file.wav\")\n\n\n\n# requires pip install imageio\nimport imageio\ndata = imageio.imread(\"file.png\")\n\n\n\nTo read in text data into python, we need to\n\nTell python where the file is (i.e. give it a path).\nTell python to open the file, and how to open the file.\nRead in the file.\n\n\n\nAdd this to your main.py script (the indentation matters!)\nfile_path = \"frankenstein\"\nwith open(file_path, \"r\") as f:\n   lines = f.readlines()\nThe object lines is now a list of all of the lines in Frankenstein.\nAssign the 55th line to a variable called line55 and print it."
  },
  {
    "objectID": "python_sessions/02_session3.html#scripting-next-steps",
    "href": "python_sessions/02_session3.html#scripting-next-steps",
    "title": "Loops Etc.",
    "section": "Scripting : next steps",
    "text": "Scripting : next steps\n\nImports\nThere is a lot of great functionality including in “base” python. However, we will usually want to access some functionality written by other people that is not included in the in basic python, in which case we’ll need to import it using import followed by the name of the module we’re importing.\nIt is usually best practice to put all imports at the TOP of your scripts\n\n💡 TASK 2\nImport both nltk and re in your main.py script nltk also requires a (one time) download, so add that as well\nimport nltk\nimport re\nnltk.download('punkt')\n\n\nAccessing functionality from the imports\nTo access the functions of the modules we’ve just imported, we type out the name of the module, a dot, and the function we want to access. For example, the re module has functions for doing things with regular expressions, including one called findall(). To access this function, we would type re.findall().\n\n💡 TASK 3\nuse the word_tokenize() function from nltk to tokenize the 55th line from Frankenstein. Assign the result to a variable called tokens55.\n\n\n\n\n\n\n\nOther import possibilities\n\n\n\nIn order to save yourself some typing, you can tell python to import a module, and to call it by a different name with import <> as <>. For example, by social convention, people tend to import the pandas package as pd.\nimport pandas as pd\ndata = pd.read_csv(\"file.csv\")\nYou can also import one off functions from a module with from <> import <>. We could replace the pandas example with\nfrom pandas import read_csv\ndata = read_csv(\"file.csv\")\nNote, this only gives you access to the read_csv() function and nothing else from the pandas module."
  },
  {
    "objectID": "python_sessions/02_session3.html#functions-vs-methods",
    "href": "python_sessions/02_session3.html#functions-vs-methods",
    "title": "Loops Etc.",
    "section": "Functions vs Methods",
    "text": "Functions vs Methods\nThere are (at least) two ways to interact with an object in python. The first is by passing the object to a function. For example, if we pass a string or a list to the len() function, it will tell us how long it is.\n\n💡 TASK 4\nGet the length of tokens55 with len() and assign it to the variable len55. Then add this print statement.\nprint(f\"There are {len55} tokens in line 55\")\n\nMost objects in python also have associated methods. Methods are like functions that are bundled into objects, and get applied to those methods. We’ve already worked with methods like .append() to add a value to a list, or .sort() to sort a list.\n\n💡 TASK 5\nWe can all of the methods associated with an object with dir(). Print out the the results of\ndir(tokens55)\n\nThe methods you’ll most often want to use don’t have __ before and after their names.\nYou can get help on how to use any method or function with help(). For example, to get help on nltk.word_tokenize() we would just add this to our script\nhelp(nltk.word_tokenize)\n\n💡 TASK 6\nUsing a mixture of dir() and help(), figure out how to use a method associated with tokens55 to get the index of \"regarded\". Assign this index to the variable regard_idx and add this print statement to your script.\nprint(f\"The index of 'regarded' is {regard_idx}\")"
  },
  {
    "objectID": "python_sessions/02_session3.html#loops",
    "href": "python_sessions/02_session3.html#loops",
    "title": "Loops Etc.",
    "section": "Loops",
    "text": "Loops\nSo far we’ve printed out an individual line from lines. If we wante to print out every line from Frankenstein, it would be a bad use of a compter to start listing\nprint(lines[0])\nprint(lines[1])\nprint(lines[2])\nprint(lines[3])\n...\nInstead, we can leverage computers’ ability to do repetitive and boring tasks very fast with a for loop.\n\n💡 TASK 7\nPut the following for loop in your script\nfor character in line55:\n    print(character)\nWhat happens?\n\nLet’s break down what’s happening here, step-by-step.\n\nFirst, python knows it’s going to be “looping over” the object line55.\nIt starts by taking the first value in line55, which is \"c\".\nIt assigns this value to the variable character.\nIt runs whatever code is inside of the loop, in this case, print(character).\nIt goes back to step 2), and gets the next value in line55, which is now \"o\".\nIt assigns this value to the variable character …\n\nAnd it will continue doing this until there are no more values to get out of line55.\n\nCollecting results\nWe can “collect” values from for loops by declaring a collector variable before the for loop, and then modifying it inside the loop.\nFor example, if we wanted to get the total length, in characters, in the book, we wouldn’t want to write it out this way:\ntotal_len = 0\ntotal_len += len(lines[0])\ntotal_len += len(lines[1])\ntotal_len += len(lines[2])\n...\nInstead, we’d want to write a for loop\n\n💡 TASK 8\nUsing a for loop, and looping over lines, tally up how many total characters there are in a variable called total_len.\n\n\n💡 TASK 9\nUsing a for loop, and looping over lines, collect all of the tokens you get from nltk.word_tokenize() in a single, flat list called all_tok.\n(hint: you’ll have to initialize an empty list like this: all_tok = [])\n\n\n\nConditionals\nWe can control the behavior of for loops a with if statements. Outside of the for loop context, we can see how if statements work.\ndoctor = \"Frankenstein\"\nmonster = \"Frankenstein's monster\"\nif monster == \"Frankenstein\":\n  print(\"The monster is named Frankenstein\")\nelse:\n  print(\"Actually, it was the *doctor* who was named Frankenstein\")\nThe comparison statement monster == \"Frankenstein” returns a True or False value. When an if statement gets a True value, it runs the code inside its block. Otherwise, it just passes onto the rest of the script, or it runs code in an else block.\n\n💡 TASK 10\nInitialize an empty list called five_character. Loop over the list of all tokens in all_tok. If a token has five characters, append it to five_character."
  },
  {
    "objectID": "python_sessions/03_session4.html",
    "href": "python_sessions/03_session4.html",
    "title": "Comprehensions and Useful Things",
    "section": "",
    "text": "We’re going to be exploring the way the spaCy package does tokenization.\nIf you get an error at the very beginning when hitting Run, run this code to download the spaCy model in the shell.\n# bash\npython -m spacy download en_core_web_sm\nCurrently, the code in main.py\n\nloads the spaCy English model\nreads in Frankenstein\nStrips leading whitespace from the beginning and end of each line\nConcatenates all of the lines into one megastring\nUses the spaCy analyzer to (among other things) tokenize the book.\n\n\nimport spacy\nfrom collections import Counter\nfrom collections import defaultdict\n\n# Load the spaCy english model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# open and read in Frankenstein\nwith open(\"gen/texts/frank.txt\", 'r') as f:\n  lines = f.readlines()\n\n# Remove leading and trailing whitespaces\nlines = [line.strip() for line in lines]\n\n# concatenate frankenstein into one huge string\nfrank_one_string = \" \".join(lines)\n\n# Tokenize all of frankenstein\nfrank_doc = nlp(frank_one_string)\n\nprint(frank_doc[500:600])\n\nriver. But supposing all these conjectures to be false, you cannot contest the inestimable benefit which I shall confer on all mankind, to the last generation, by discovering a passage near the pole to those countries, to reach which at present so many months are requisite; or by ascertaining the secret of the magnet, which, if at all possible, can only be effected by an undertaking such as mine.  These reflections have dispelled the agitation with which I began my letter, and I feel my heart glow\n\n\n\n\n\nWe can treat frank_doc like a list, but it’s actually a special data structure. The same goes for each token inside frank_doc. If you just say\n\nprint(frank_doc[506])\n\nconjectures\n\n\nIt will print conjectures. But if you say\n\nprint(\n  dir(frank_doc[506])\n)\n\n['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', 'ancestors', 'check_flag', 'children', 'cluster', 'conjuncts', 'dep', 'dep_', 'doc', 'ent_id', 'ent_id_', 'ent_iob', 'ent_iob_', 'ent_kb_id', 'ent_kb_id_', 'ent_type', 'ent_type_', 'get_extension', 'has_dep', 'has_extension', 'has_head', 'has_morph', 'has_vector', 'head', 'i', 'idx', 'iob_strings', 'is_alpha', 'is_ancestor', 'is_ascii', 'is_bracket', 'is_currency', 'is_digit', 'is_left_punct', 'is_lower', 'is_oov', 'is_punct', 'is_quote', 'is_right_punct', 'is_sent_end', 'is_sent_start', 'is_space', 'is_stop', 'is_title', 'is_upper', 'lang', 'lang_', 'left_edge', 'lefts', 'lemma', 'lemma_', 'lex', 'lex_id', 'like_email', 'like_num', 'like_url', 'lower', 'lower_', 'morph', 'n_lefts', 'n_rights', 'nbor', 'norm', 'norm_', 'orth', 'orth_', 'pos', 'pos_', 'prefix', 'prefix_', 'prob', 'rank', 'remove_extension', 'right_edge', 'rights', 'sent', 'sent_start', 'sentiment', 'set_extension', 'set_morph', 'shape', 'shape_', 'similarity', 'subtree', 'suffix', 'suffix_', 'tag', 'tag_', 'tensor', 'text', 'text_with_ws', 'vector', 'vector_norm', 'vocab', 'whitespace_']\n\n\nYou’ll see a lot more values and methods associated with the token than you normally would for a string. For example, frank_doc[506].text will give us the text of the token, and frank_doc[506].lemma_ will give us the lemma.\n\nprint(\n  f\"The word '{frank_doc[506].text}' is lemmatized as '{frank_doc[506].lemma_}'\"\n)\n\nThe word 'conjectures' is lemmatized as 'conjecture'\n\n\nOr we can get the guessed part of speech with frank_doc[506].pos_\n\nprint(\n  f\"The word '{frank_doc[506].text}' is given the part of speech '{frank_doc[506].pos_}'\"\n)\n\nThe word 'conjectures' is given the part of speech 'VERB'\n\n\nOr we can pull out the guessed morphological information:\n\nprint(\n  f\"spacy guesses '{frank_doc[506].text}' is '{frank_doc[506].morph}'\"\n)\n\nspacy guesses 'conjectures' is 'Number=Sing|Person=3|Tense=Pres|VerbForm=Fin'\n\n\n\n\n\nWe can use if statements to control how our code runs. An if statement checks to see if its logical comparison is true, and if it is, it executes its code.\n\n## This is not true, so it dosn't print\nif frank_doc[506].pos_ == \"NOUN\":\n  print(\"it's a verb!\")\n\n## This is true, so it prints\nif frank_doc[506].pos_ == \"VERB\":\n  print(\"it's a verb!\")\n\nit's a verb!\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nPrint the .text of every word whose .lemma_ is \"monster\"\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWith a for loop, create a list called five_letter which contains every 5 letter word from the book (a.k.a. .text is 5 characters long.)\n\n\n\n\n\n\n“Comprehensions” are a great shortcut around writing out a whole for loop. Let’s take the following list:\n\nrain_list = \"The rain in Spain stays mainly on the plain\".split(\" \")\nprint(rain_list)\n\n['The', 'rain', 'in', 'Spain', 'stays', 'mainly', 'on', 'the', 'plain']\n\n\nIf I wanted to capitalize all of those words, one way I could do it is with a for loop\n\nupper_rain = []\nfor word in rain_list:\n  upper_rain.append(word.upper())\n\nprint(upper_rain)\n\n['THE', 'RAIN', 'IN', 'SPAIN', 'STAYS', 'MAINLY', 'ON', 'THE', 'PLAIN']\n\n\nAlternatively, I could do it with a “list comprehension”:\n\nupper_rain2 = [word.upper() for word in rain_list]\n\nprint(upper_rain2)\n\n['THE', 'RAIN', 'IN', 'SPAIN', 'STAYS', 'MAINLY', 'ON', 'THE', 'PLAIN']\n\n\nList comprehensions keep the for word in rain_list part the same, but instead of needing to initialize a whole empty list, we wrap the whole thing inside [ ], which tells python we’re going to capture the results inside a list. The variable (& whatever we do to it) at the beginning of the command is what gets captured.\nWe can use if statements too.\n\nai_words = [word for word in rain_list if \"ai\" in word]\n\nprint(ai_words)\n\n['rain', 'Spain', 'mainly', 'plain']\n\n\nWe can even have nested for statements\n\nrain_list = \"The rain in Spain stays mainly on the plain\".split(\" \")\nletters = [letter\n            for word in rain_list\n              for letter in word]\nprint(letters)\n\n['T', 'h', 'e', 'r', 'a', 'i', 'n', 'i', 'n', 'S', 'p', 'a', 'i', 'n', 's', 't', 'a', 'y', 's', 'm', 'a', 'i', 'n', 'l', 'y', 'o', 'n', 't', 'h', 'e', 'p', 'l', 'a', 'i', 'n']\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWith a list comprehension, create a list called five_letter2 which contains every 5 letter word from the book (a.k.a. .text is 5 characters long.)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nBy whatever means necessary (but I recommend using a list comprehension), create a list containing all of the words with a VERB as .pos\n\n\n\n\n\n\nA set is another special python data structure that, among other things, will “uniquify” a list.\n\nbman_list = \"na na na na na na na na na na na na na na na na Batman\".split(\" \")\nbman_set = set(bman_list)\nprint(bman_set)\n\n{'na', 'Batman'}\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFind out how many total words there are in Frankenstein, excluding tokens with .pos of PUNCT and SPACE\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFind out how many total unique words (.text) there are in Frankenstein, excluding tokens with .pos of PUNCT and SPACE\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFind out how many total unique lemmas (.lemma_) there are in Frankenstein, excluding tokens with .pos of PUNCT and SPACE\n\n\n\n\n\n\nThere is a handy dandy function called Counter that we can import from the collections module like so\n\nfrom collections import Counter\n\nWhen we pass Counter() a list, it will return a dictionary of counts of items in that list.\n\nbman_list = \"na na na na na na na na na na na na na na na na Batman\".split(\" \")\nbman_count = Counter(bman_list)\nprint(bman_count)\n\nCounter({'na': 16, 'Batman': 1})\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nCreate a counter dictionary of all of the forms of “be” (.lemma == \"be\") in Frankenstein"
  },
  {
    "objectID": "python_sessions/01_session2.html",
    "href": "python_sessions/01_session2.html",
    "title": "Lists and Dictionaries",
    "section": "",
    "text": "In the previous module, we learned about assignment and the kinds of values we can have in python. Just a quick recap:"
  },
  {
    "objectID": "python_sessions/01_session2.html#lists",
    "href": "python_sessions/01_session2.html#lists",
    "title": "Lists and Dictionaries",
    "section": "Lists",
    "text": "Lists\nWe’ll rarely want to work with just one of string values. More often we’ll want to work with a collection of values. For this we’ll use lists. For example, here is a list of Mary Shelly’s published novels.\nnovels = [\"Frankenstein\",\n          \"Valperga\",\n          \"The Last Man\", \n          \"The Fortunes of Perkin Warbeck, A Romance\", \n          \"Lodore\", \n          \"Falkner. A Novel\"]\nAssignment works the same way as before. We can use any text, without commas, as a variable name, then assign a value to it with the = operator.\n\n💡 TASK 1\nAssign the names of people in your group (first and last) to a list called our_group.\n\nLists don’t just have to contain strings, but can actually contain any mixture of data types.\npublished_dates = [1818, 1823, 1826, 1830, 1835, 1837]\n\nLists can get complex\nYou can have lists that contain other lists.\nmy_signs = [\"Joe\", [\"Capricorn\", \"Gemini\", \"Capricorn\"]]\n\n💡 TASK 2\nEach person in the group create a list of lists, with your name as the first value, and your big three in a list as the second value. Assign this value to a variable named with your initials. (If you don’t know your big three or don’t want to share, just say “Capricorn” across the board.)\n\n\n\nMaking changes to lists\n\nAdding items\nThere are a few ways you can add additional values to lists. One way is with the + operator. For example, one of Mary Shelley’s books, Mathida was published posthumously.\nposthumous = [\"Mathilda\"]\nWe could add this to the list of novels like so.\nprint(novels + posthumous)\nOne thing you should note is that this doen’t change the list novels. If you run print(novels) now, it will have just the original books in it.\nTo change the actual list stored in novels, we need to use the .append() “method”. We’ll talk more about “methods” vs functions in later lessons\nnovels.append(\"Mathilda\")\nprint(novels)\nA thing to remember about this is .append() changes the variable novels “in place.” That is, without doing any additional assignment, or anything, you’ve changed its value.\n\n💡 TASK 3\nCreate a variable called group_sign which is a list of each person’s star sign list from Task 2.\n\n\n\nSorting\nTo sort a list, alphabetically for strings or numerically for numbers, you can use the .sort() method. Again, this changes the list in place.\n\n💡 TASK 4\nAlphabetically sort the titles of Mary Shelly’s novels.\n\n\n\n\nIndexing Lists (and other “iterables”)\nIn order to pull values out of a list, we need to “index” it. Here’s a really important thing to remember:\nIndexing Starts at 0!\nTo get the fist value out of a list we use the index value 0, and pass it in-between brackets, like so:\nprint(novels[0])\nIn the alphabetically ordered list, this should return Falkner, A Novel\n\n💡 TASK 5\nPrint the name of the second member in our_group.\n\nNumeric indexing works with any “iterable” in python. For example, if we assigned the first novel to a variable, and started indexing that, it would start printing individual letters.\nfirst_novel = novels[0]\nprint(first_novel[2])\nThis will have printed out the third letter of the first novel’s title.\n🚨We can do this better by stacking up indices.\nInstead of assigning the first valye to a variable, we can get the same result by just placing these indexing brackets one after another.\nprint(novels[0][2])\n\n💡 TASK 6\nPrint the second letter from the first value in the list our_group.\n\n\n💡 TASK 7\nPrint the moon sign of the third member of your group, as stored in our_signs from task 3.\n\n\n\nSlicing\nIf we want to get a range of value out of the list, we can use a “slice”. For example, to get the first three books out of the list novels, we can do\nprint(novels[0:3])\nNow… this should strike you as weird, because to get the third value from the list, you use novels[2]. Why does the slice end in 3?\nWe should think about the relationship between values and indices like this:\n\nRather than being set one on top of eachother, the indices come between the values. When you give just one numeric index, python returns the first value to the right. When you pass python a slice with a starting index and and ending index, it returns everything between those indices.\n\n\nReverse indexing.\nThere’s a quick way to get the final value from a list or iterable as well: index with -1.\n\n💡 TASK 8\nPrint the last letter from the last value in the list our_group\n\n\n💡 TASK 9\nPrint the rising sign of the first person as stored in our_signs"
  },
  {
    "objectID": "python_sessions/01_session2.html#dictionaries",
    "href": "python_sessions/01_session2.html#dictionaries",
    "title": "Lists and Dictionaries",
    "section": "Dictionaries",
    "text": "Dictionaries\nWhile lists can be highly complex, and can even capture the relationships between data, they can be a bit limiting. For example, here’s a list representing the relationship between the title and the year of publication of Mary Shelley’s first and last novels.\nnovel_dates = [[\"Frankenstein\", 1818],\n               [\"Mathilda\",     1959]]\nWhile this does the job, if I came along later, and all I knew was the title “Frankenstein” and wanted to quickly get the date, it would take a bit of work with these nested lists.\nWhat would speed up the job are “dictionaries”, which store key:value pairs.\nnovel_dict = {\"Frankenstein\"     : 1818,\n              \"Valperga\"         : 1823,\n              \"The Last Man\"     : 1826, \n              \"The Fortunes of Perkin Warbeck, A Romance\" : 1830,\n              \"Lodore\"           : 1835,\n              \"Falkner. A Novel\" : 1837}\nThese dictionaries are created with opening and closing curly brackets { }, and have a sequence of key : value pairs. The “key” is called the “key”, because instead of indexing dictionaries with numeric values, we index them with whatever the keys are.\nSo to get the publication date of Frankenstein, we do\nnovel_dict[\"Frankenstein\"]\nTo add a previously absent key : value pair to a dictionary, we pass the new key we want to use to [ ], and then assign the new value.\nnovel_dict[\"Mathilda\"] = 1959\n\n💡 TASK 10\nCreate a dictionary where the keys are the names (first and last) of your group members, and the values are your heights, in inches (as a numeric value).\n(Hint, if you’re 5’10, the math would be (5*12) + 10)\n\n\n💡 TASK 11\nPrint the height of the alphebetically first group member. (Don’t just type in their name, get it from python).\n\n\nChecking for keys\nTo check if a key is already in the dictionary, we can use the in operator.\n`python \"Mary: A Fiction\" in novel_dict # False"
  },
  {
    "objectID": "python_sessions/00_session1.html",
    "href": "python_sessions/00_session1.html",
    "title": "Starting Python",
    "section": "",
    "text": "Welcome to Python!\nIf you have never done programming before, all python is is a program that reads a text file, and executes commands contained in the text file."
  },
  {
    "objectID": "python_sessions/00_session1.html#its-a-text-file",
    "href": "python_sessions/00_session1.html#its-a-text-file",
    "title": "Starting Python",
    "section": "It’s a text file",
    "text": "It’s a text file\nGo ahead and open the text file fake.txt. It contains text that is all valid python code. If you pop over to the shell and run\npython3 fake.txt\nPython will happily run the code in fake.txt. However, I recommend we always save our python scripts with the .py file extension for a few reasons.\n\nEveryone does it, so it’ll match human expectations for people looking at your code.\nAny worthwhile code editor (replit included) will decide what the text in the file is supposed to be based on the file extension, and trys to help you accordingly with things like\n\nSyntax highlighting, making your code easier to read.\nAutocomplete assistance (e.g. if you type an open (, it’ll automatically insert the closing ).\nCode suggestions. Some text editors will try to clue you with the possible names of functions your trying to type if you just type in the first few letters, and then may even try to clue you with the names of arguments to the functions."
  },
  {
    "objectID": "python_sessions/00_session1.html#interacting-python",
    "href": "python_sessions/00_session1.html#interacting-python",
    "title": "Starting Python",
    "section": "Interacting Python",
    "text": "Interacting Python\nThere are are a bunch of different ways you can interact with python to get it to interpret your code.\n\nInteractive Session\nIf you go to the Shell and just run python3 without any other arguments, it will launch the python shell, which inteprets the text you type in as python code. If you copy-paste this code into the python shell, it should print hello world back at you.\nprint(\"hello world\")\nTo quit the python shell, run\nquit()\n\n\n“Notebooks” (not in replit)\nThere are a few “notebook” options out there which allow you to interleave text, like notes to yourself, or descriptions of the code, with python code chunks.\nYou can experiment with free online notebooks by setting up an account at Kaggle or Google Colab.\nFor your local set up, I’d recommend either configuring VS Code to run Jupyter notebooks, or use a Quarto document in RStudio (it’s not just for R!)\n\n\nScripts\nThe primary way we’ll be interacting with python inside of replit is with python “scripts,” which are just text documents of code python should run, line by line."
  },
  {
    "objectID": "python_sessions/00_session1.html#getting-started",
    "href": "python_sessions/00_session1.html#getting-started",
    "title": "Starting Python",
    "section": "Getting Started",
    "text": "Getting Started\nThe python script main.py already has some code in it.\nimport numpy as np\n\n# This is a comment\n# Python won't interpret or run a line starting with #\n\nages = np.array([40, 42, 25])\nyear = np.array([2016, 2001, 2019])\n\n💡 TASK 1\nBelow this line (it doesn’t matter how many new lines you add) enter this line of code crucially without any spaces or tabs preceding it.\nprint(\"hello world\")\nOnce you’ve done that, hit the big green run button, and hello world should print out in the console.\n\n\nHow to see what python is doing\nWe’re going to be using the print() function a lot. The primary way we’ll be interacting with python is via python scripts, and the only way to see what our code has done is to explicitly tell python to print the output."
  },
  {
    "objectID": "python_sessions/00_session1.html#values-variables-and-assignment",
    "href": "python_sessions/00_session1.html#values-variables-and-assignment",
    "title": "Starting Python",
    "section": "Values, Variables, and Assignment",
    "text": "Values, Variables, and Assignment\nBefore we get into what the stuff at the very top of the script means, we’re going to first cover the basics of values, variable, and assignment.\n\nValues\nThe main python value types are\n\nNumeric\nCharacter\nBoolean (True/False)\n\n\nNumeric Practice\n\n💡 TASK 2\nCalculate how many seconds and print the output by adding this line to your script.\n print(f\"There are {365 * 24 * 60} minutes in a year.\")\n\nWhat python has done is multiplied 365 (for the number of days) by 24 (for the number of hours in a day) by 60 (for the number of minutes in an hour) to produce the number of minutes in a day.\nPython can do any kind of arithmetic you ask of it. For example, we can caclute what percent of a whole day one minute is by adding this line of code to our script.\n\n💡 TASK 3\nCalculate how many seconds and print the output by adding this line to your script.\n print(f\"One minute is {(1/(24 * 60)) * 100}% of a day.\")\n\n\n💡 TASK 4\nChapter 1 of Frankenstein has 1,780 words, and 75 of them were the word “of”. Calculate what percent of words were “of” by adding this line of code to your script.\nprint(f\"{}% of words in Chapter 1 of Franenstein were 'of'\")\nFill in the correct mathematical formula inside the {}\n\n\n\n\nAssignment\nWe don’t usually want to just do some calculations and then just let the values disappear when we print them, though. We’ll usually want to save some values for future use. We can do that by “assigning” values to “variables.”\nThe assignment operator in python is =. For example we can assign the name of this class to a variable called this_class like so:\nthis_class = \"Lin517\"\n\n💡 TASK 5\nAssign the value \"Lin517\" to this_class\n\n\n💡 TASK 6\nPrint the variable this_class\n\n\nImportant Things to Note!\n\nThe variable this_class did not exist before we did assignment! If we had asked python to print this_class before we did the assignment, it would have given us an error.\nVariable names are case sensitive! If we tried to print This_class or this_Class or This_Class they would all return an error.\nYou can start variable names with any letter or underscore, but that’s all (no numbers at the start).\nAfter the first character, you can use any letter, number, or underscore.\nNo &, ., * or ? are allowed.\nAny text that isn’t enclosed inside \" \" will be interpreted as a variable name."
  },
  {
    "objectID": "python_sessions/00_session1.html#doing-things-with-variables.",
    "href": "python_sessions/00_session1.html#doing-things-with-variables.",
    "title": "Starting Python",
    "section": "Doing things with variables.",
    "text": "Doing things with variables.\nOnce you assign a value to a variable, it can stand in as if it was that variable. For example.\ndays_in_year    = 365\nhours_in_day    = 24\nminutes_in_hour = 60\n\nprint(f\"There are {days_in_year * hours_in_day * minutes_in_hour} minutes in a year\")\n\n💡 TASK 7\n\nAssign the current year to a variable called this_year.\nAssign one of your ages to a variable called my_age\nCalculate your year of birth by subtracting my_age from this_year\nPrint the result.\n\n\n\n💡 TASK 8\nCalculate how old you’ll be in 2040 and print the result.\n\nYou can overwrite the value you’ve assigned to any variable by just assigning a new value to it."
  },
  {
    "objectID": "python_sessions/00_session1.html#numbers",
    "href": "python_sessions/00_session1.html#numbers",
    "title": "Starting Python",
    "section": "Numbers",
    "text": "Numbers\nTechnically, there are two kinds of numbers in Python: integers (numbers without decimals places) and floats (numbers with decimal places). This used to be a bigger deal in python2, but python3 converts as necessary. We’ve already done some work with numbers above. The built in arithmetic in python that we can use on numbers is:\n\nx + y addition\nx - y subtraction\nx * y multiplication\nx / y division\nx ** y exponentiation (that is, xy)\nx % y modulus (this gives you the remainder of doing division)\nx // y floor division (this gives you the largest whole number that y can go into x)\n\n\n💡 TASK 9\nMary Shelly has written 1,780 words for Frankenstein Chapter 1, and her publisher has told her there is a strict word limit of 300 words per page. 1. Calculate how many full pages chapter 1 is going to be. Assign this value to the variable full_pages. 2. There’s going to be some words left over. Calculate how many words are going to go onto the overflow page. Assign this value to a variable called overflow."
  },
  {
    "objectID": "python_sessions/00_session1.html#strings",
    "href": "python_sessions/00_session1.html#strings",
    "title": "Starting Python",
    "section": "Strings",
    "text": "Strings\nWe’ve already been doing a lot with strings in these print() statements. But just to be explicit, everything that comes inside \" \" is interpreted as a string, even numbers. If you tried to do\n1 + \"1\"\nYou would get an error, because the first value is a number and the second value is a string.\nWe can use some math-looking-things on strings, though.\n\n💡 TASK 10\nDo the following assignments.\nroot    = \"Lingu\"\naffixes = \"istics\"\nNow, print what happens when you do root + affixes\n\n\n💡 TASK 11\nAgain, do the following assignments.\nframe = \"It's a \"\nword  = \"salad \"\nredup = word * 2\nNow, print the result of what happens when you do frame + redup"
  },
  {
    "objectID": "lectures/data_sparsity/data_sparsity.html",
    "href": "lectures/data_sparsity/data_sparsity.html",
    "title": "Data Sparsity",
    "section": "",
    "text": "Let’s say we’re biologists, working in a rain forest, and put out a bug net to survey the biodiversity of the forest. We catch 10 bugs, and each species is a different color:\n[\\(_1\\), \\(_2\\), \\(_3\\), \\(_4\\), \\(_5\\), \\(_6\\), \\(_7\\), \\(_8\\), \\(_9\\), \\(_{10}\\)]\nWe have 10 bugs in total, so we’ll say \\(N=10\\). This is our “token count.” We’ll use the \\(i\\) subscript to refer to each individual bug (or token).\nIf we made a table of each bug species, it would look like:\n\n\n\nspecies\nindex \\(j\\)\ncount\n\n\n\n\n\n1\n5\n\n\n\n2\n2\n\n\n\n3\n1\n\n\n\n4\n1\n\n\n\n5\n1\n\n\n\nLet’s use \\(M\\) to represent the total number of species, so \\(M=5\\) here. This is our type count, and we’ll the subscript \\(j\\) to represent the index of specific types.\nWe can mathematically represent the count of each species like so.\n\\[\nc_j = C(\\class{fa fa-bug}{}_j)\n\\]\nHere, the function \\(C()\\) takes a specific species representation \\(\\class{fa fa-bug}{}_j\\) as input, and returns the specific count \\(c_j\\) for how many times that species showed up in our net. So when \\(j = {\\color{#785EF0}{1}}\\), \\(\\color{#785EF0}{c_1}=5\\), and when \\(j = {\\color{#FFB000}{4}}\\), \\(\\color{#FFB000}{c_4}=1\\).\nHere’s a plot, with the species id \\(j\\) on the x-axis, and the number of times that species appeared in the net \\(c_j\\) on the y-axis.\n\n\n\n\n\n\n\nWhat is the probability that tomorrow, when we put the net out again, that the first bug we catch will be from species ? Usually in these cases, we’ll use past experience to predict the future. Today, of the \\(N=10\\) bugs we caught, \\(\\color{#785EF0}{c_1}=5\\) of them were species . We can represent this as a fraction like so:\n\\[\n\\frac{{\\color{#785EF0}{\\class{fa fa-bug}{}}}_1,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_2,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_3,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_4,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_5}\n{{\\color{#785EF0}{\\class{fa fa-bug}{}}}_1,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_2,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_3,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_4,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_5,\n      {\\color{#DC267F}{\\class{fa fa-bug}{}}}_6,\n      {\\color{#DC267F}{\\class{fa fa-bug}{}}}_7,\n      {\\color{#FE6100}{\\class{fa fa-bug}{}}}_8,\n      {\\color{#FFB000}{\\class{fa fa-bug}{}}}_9,\n      {\\color{#4C8C05}{\\class{fa fa-bug}{}}}_{10}}\n\\]\nOr, we can simplify it a little bit. The top part (the numerator) is equal to \\(\\color{#785EF0}{c_1}=5\\), and the bottom part (the denominator) is equal to the total number of bugs, \\(N\\). Simplifying then:\n\\[\n\\frac{\\color{#785EF0}{c_1}}{N} = \\frac{5}{10} = 0.5\n\\]\nWe’ll use this as our guesstimate of the probability that the very next bug we catch will be from species . Let’s use the function \\(\\hat{P}()\\) to mean “our method for guessing the probability”, and \\(\\hat{p}\\) to represent the guess we came to. We could express “our guess that the first bug we catch will be ” like so.\n\\[\n{\\color{#785EF0}{\\hat{p}_1}} = \\hat{P}({\\color{#785EF0}{\\class{fa fa-bug}{}}}) = \\frac{\\color{#785EF0}{c_1}}{N} = \\frac{5}{10} = 0.5\n\\]\nWe can then generalize our method to any bug like so:\n\\[\n\\hat{p}_j = \\hat{P}(\\class{fa fa-bug}{}_j) = \\frac{c_j}{N}\n\\]\n\n\n\nLet’s say we set out the net again, and the first bug we catch is actually . This is a new species of bug that wasn’t in the net the first time. Makes enough sense, the forest is very large. However, what probability would we have given catching this new species?\nWell, \\(\\color{#35F448}{c_6} = C({\\color{#35F448}{\\class{fa fa-bug}{}}}) = 0\\). So our estimate of the probability would have been \\({\\color{#35F448}{\\hat{p}_6}} = \\hat{P}({\\color{#35F448}{\\class{fa fa-bug}{}}}) = \\frac{\\color{#35F448}{c_6}}{N} = \\frac{0}{10} = 0\\).\nWell obviously, the probability that we would catch a bug from species  wasn’t 0, because events with 0 probability don’t happen, and we did catch the bug. Admittedly, \\(N=10\\) is a small sample to try and base a probability estimate on, so how large would we need the sample to be before we could make probabity estimates for all possible bug species, assuming we stick with the probability estimating function \\(\\hat{P}(\\class{fa fa-bug}{}_j) = \\frac{c_j}{N}\\)?"
  },
  {
    "objectID": "lectures/data_sparsity/data_sparsity.html#youd-need-fa-infinity",
    "href": "lectures/data_sparsity/data_sparsity.html#youd-need-fa-infinity",
    "title": "Data Sparsity",
    "section": "You’d need ",
    "text": "You’d need \nThis kind of data problem does arise for counting species, but this is really a tortured analogy for language data.1 For example, let’s take all of the words from Chapter 1 of Mary Shelly’s Frankenstein, downloaded from Project Gutenberg. I’ll count how often each word occurred, and assign it a rank, with 1 being given to the word that occurred the most.1 For me, I used this analogy to include colorful images of bugs in the lecture notes. For Good (1953), they had to use a tortured analogy since the methods for fixing probability estimates were still classified after being used to crack the Nazi Enigma Code in WWII.\n\n\n\n\n\n\n\n\n\nJust to draw the parallels between the two analogies:\n\n\n\n\n\n\n\n\nvariable\nin the analogy\nin Frankenstein Chapter 1\n\n\n\n\n\\(N\\)\nThe total number of bugs caught in the net. (\\(N=10\\))\nThe total number of words in the first chapter. (\\(N=1,780\\)).\n\n\n\\(x_i\\)\nAn individual bug. e.g. \\(_1\\)\nAn individual word token. In chapter 1, \\(x_1\\) = “i”\n\n\n\\(w_j\\)\nA bug species. \nA word type. The indices are frequency ordered, so for chapter 1 \\(w_1\\) = “of”\n\n\n\\(c_j\\)\nThe count of how many individuals there are of a species.\nThe count of how many tokens there are of a type.\n\n\n\nHere’s a table of the top 10 most frequent word types.\n\n\n\n\n\n\\(w_j\\)\n\\(c_j\\)\n\\(j\\)\n\n\n\n\nof\n75\n1\n\n\nthe\n75\n2\n\n\nand\n70\n3\n\n\nto\n61\n4\n\n\na\n52\n5\n\n\nher\n52\n6\n\n\nwas\n40\n7\n\n\nmy\n33\n8\n\n\nin\n32\n9\n\n\nhis\n29\n10\n\n\n\n\n\nIf we plot out all of the word types with the rank (\\(j\\)) on the x-axis and the count of each word type (\\(c_j\\)) on the y-axis, we get a pattern that if you’re not already familiar with it, you will be.\n\n\n\n\n\nThis is a “Zipfian Distribution” a.k.a. a “Pareto Distribution” a.k.a. a “Power law,” and it has a few features which make it ~problematic~ for all sorts of analyses.\nFor example, let’s come back to the issue of predicting the probability of the next word we’re going to see. Language Models are “string prediction models,” after all, and in order to get a prediction for a specific string, you need to have seen the string in the training data. Remember how our bug prediction method had no way of predicting that we’d see a  because it had never seen one before?\nThere are a lot of possible string types of “English” that we have not observed in Chapter 1 of Frankenstein. Good & Turing proposed that you could guesstimate that the probability of seeing a never before seen “species” was about equal to the proportion of “species” you’d only seen once. With just Chapter 1, that’s a pretty high probability that there are words you haven’t seen yet.\n\n\n\n\n\nseen once?\ntotal\nproportion\n\n\n\n\nno\n1216\n0.683\n\n\nyes\n564\n0.317\n\n\n\n\n\nSo, let’s increase our sample size. Here’s the same plot of rank by count for chapters 1 through 5.\n\n\n\n\n\n\n\n\n\n\nseen once?\ntotal\nproportion\n\n\n\n\nno\n9928\n0.858\n\n\nyes\n1649\n0.142\n\n\n\n\n\nWe increased the size of the whole corpus by a factor of 10, but we’ve still got a pretty high probability of encountering an unseen word.\nLet’s expand it out to the whole book now.\n\n\n\n\n\n\n\n\n\n\nseen once?\ntotal\nproportion\n\n\n\n\nno\n72122\n0.96\n\n\nyes\n3021\n0.04\n\n\n\n\n\n\n🎵 Ain’t no corpus large enough 🎵\nAs it turns out, there’s no corpus large enough to guarantee observing every possible word at least once, for a few reasons.\n\nThe infinite generative capacity of language! The set of all possible words is, in principle infinitely large.\nThese power law distributions will always have the a lot of tokens with a frequency of 1, and even just those tokens are going to have their probabilities poorly estimated.\n\nTo illustrate this, I downloaded the 1-grams of just words beginning with [Aa] from the Google Ngrams data set. This is an ngram dataset based on all of the books scanned by the Google Books project. It’s 4 columns wide, 86,618,505 rows long, and 1.8G large, and even then I think it’s a truncated version of the data set, because the fewest number of years any given word appears is exactly 40.\nIf we take just all of the words that start with [Aa] published in the year 2000, the most common frequency for a word to be is still just 1, even if it is a small proportion of all tokens.\n\n\n\nFrequencies of frequencies in words starting with [Aa] from the year 2000 in google ngrams \n\n\n\n\n\n\n\nword frequency\nnumber of types with frequency\nproportion of all tokens\n\n\n\n\n1\n205141\n4.77e-10\n\n\n2\n152142\n9.55e-10\n\n\n3\n107350\n1.43e-09\n\n\n4\n80215\n1.91e-09\n\n\n5\n60634\n2.39e-09\n\n\n6\n47862\n2.86e-09\n\n\n\n\n\n\n\nAn aside\nI’ll be plotting the rank vs the frequency with logarithmic axes from here on. Linear axes give equal visual space for every incremental change in the x and y values, while lograrithmic axes put more space between smaller numbers than larger numbers.\n\n\n\n\n\n\nrank by frequency on linear scales\n\n\n\n\n\n\n\nrank by frequency on logarithmic scales\n\n\n\n\n\n\n\nIt gets worse\nWe can maybe get very far with our data sparsity for how often we’ll see each individual word by increasing the size of our corpus size, but 1gram word counts are rarely as far as we’ll want to go.\nTo come back to our bugs example, let’s say that bug species  actually hunts bug species . If we just caught a  in our net, it’s a lot more likely that we’ll catch a  next, coming after the helpless  than it would be if we hadn’t just caught a . To know what exactly the probability catching  and then a  is, we’d need to count up every 2 bug sequence we’ve seen.\nBringing this back to words, 2 word sequences are called “bigrams” and 3 word sequences are called “trigrams,” and they are also distributed according to a Power Law, and each larger string of words has a worse data sparsity one than the one before. But each larger string of words means more context, which makes for better predictions."
  },
  {
    "objectID": "lectures/data_sparsity/data_sparsity.html#some-notes-on-power-laws",
    "href": "lectures/data_sparsity/data_sparsity.html#some-notes-on-power-laws",
    "title": "Data Sparsity",
    "section": "Some Notes on Power Laws",
    "text": "Some Notes on Power Laws\nThe power law distribution is pervasive in linguistic data, in almost every domain where we might count how often something happens or is observed. This is absolutely a fact that must be taken into account when we develop our theories or build our models. Some people also think it is an important fact to be explained about language, but I’m deeply skeptical.\nA lot of things follow power law distributions. The general property of these distributions is that the second most frequent thing will have a frequency about as half as the most frequent thing, the third most frequent thing will have a frequency about a third of the most frequent thing, etc. We could put that mathematically as:\n\\[\nc_j = \\frac{c_1}{j}\n\\]\nFor example, here’s the log-log plot of baby name rank by baby name frequency in the US between 1880 and 2017.22 Data from the babynames R package, which in turn got the data from the Social Security Administration.\n\n\n\n\n\nrank by frequency of baby names\n\n\n\n\nThe log-log plot isn’t perfectly straight (it’s common enough for data like this to have two “regimes”).\nHere’s the number of ratings each movie on IMDB has received.\n\n\n\n\n\nIf we break down the movies by their genre, we get the same kind of result.\n\n\n\n\n\nOther things that have been shown to exhibit power law distributions (Newman 2005; Jiang and Jia 2011) are\n\nUS city populations\nnumber of citations academic papers get\nwebsite traffic\nnumber of copies books sell\nearthquake magnitudes\n\nThese are all possibly examples of “preferential attachment”, but we can also create an example that doesn’t involve preferential attachment, and still wind up with a power-law. Let’s take the first 12 words from Frankenstein:\n\n\n\n\"to\"\"mrs\"\"saville\"\"england\"\"st\"\"petersburgh\"\"dec\"\"11th\"\"17\"\"you\"\"will\"\"rejoice\"\n\n\n\nNow, let’s paste them all together into one long string with spaces.\n\n\n\n\"to mrs saville england st petersburgh dec 11th 17 you will rejoice\"\n\n\nAnd now, let’s choose another arbitrary symbol to split up words besides \" \". I’ll go with e.\n\n\n\n\"to mrs savill\"\" \"\"ngland st p\"\"t\"\"rsburgh d\"\"c 11th 17 you will r\"\"joic\"\"\"\n\n\n\nThe results aren’t words. They’re hardly useful substrings. But, if we do this to the entire novel and plot out the rank and count of thes substrings like they were words, we still get a power law distribution.\n\n\n\n\n\nIn fact, if I take the top 4 most frequent letters, besides spaces, that occur in the text and use them as substring delimiters, the resulting substring distributions are all power-law distributed.\n\n\n\n\n\nThey even have other similar properties often associated with power law distributions in language. For example, it’s often been noted that more frequent words tend to be shorter. These weird substrings exhibit that pattern even more strongly than actual words do!\n\n\n\n\n\nThis is all to say, be cautious about explanations for power-law distributions that are"
  },
  {
    "objectID": "lectures/data_sparsity/data_sparsity.html#extra",
    "href": "lectures/data_sparsity/data_sparsity.html#extra",
    "title": "Data Sparsity",
    "section": "Extra",
    "text": "Extra\nTo work out just how accurate the Good-Turing estimate is, I did the following experiment.\nStarting from the beginning of the book, I coded each word \\(w_i\\) for whether or not it had already appeared in the book, 1 if yes, 0 if no. This is my best shot at writing that out in mathematical notation.\n\\[\na_i = \\left\\{\\begin{array}{ll}1,& x_i\\in x_{1:i-1}\\\\\n                             0,& x_1 \\notin x_{1:i-1}\\end{array}\\right\\}\n\\]\nThen for every position in the book, I made a table of counts of all the words up to that point in the book so far, and got the proportion of word tokens that had appeared only once. Again, here’s my best stab at writing that out mathematically.\n\\[\nc_{ji} = C(w_j), w_j \\in x_{i:i-1}\n\\]\n\\[\nr_i = \\sum_{j=1}\\left\\{\\begin{array}{ll}1,&c_{ji}=1\\\\0,& c_{ji} >1 \\end{array}\\right\\}\n\\]\n\\[\ng_i = \\frac{r_i}{i-1}\n\\]\n\nfrank_words$first_appearance <- NA\nfrank_words$first_appearance[1] <- 1\n\nfrank_words$gt_est <- NA\nfrank_words$gt_est[1] <- 1\nfor(i in 2:nrow(frank_words)){\n  i_minus <- i-1\n  prev_corp <- frank_words$word[1:i_minus]\n  this_word <- frank_words$word[i]\n  \n  frank_words$first_appearance[i] <- ifelse(this_word %in% prev_corp, 0, 1)\n  frank_words$gt_est[i] <- sum(table(prev_corp) == 1)/i_minus\n}\n\n\n\n\nThen, I plotted the Good-Turing estimate for every position as well as a non-linear logistic regression smooth."
  },
  {
    "objectID": "lectures/lem_stem/index.html#what-tokenizing-does-not-get-for-you",
    "href": "lectures/lem_stem/index.html#what-tokenizing-does-not-get-for-you",
    "title": "Lemmatizing and Stemming",
    "section": "What tokenizing does not get for you",
    "text": "What tokenizing does not get for you\nComing back to the example sentences from the first data processing lecture, properly tokenizing these sentences will only partly help us with our linguistic analysis.\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nfrom tabulate import tabulate\n\nphrase = \"\"\"The 2019 film CATS is a movie about cats. \nCats appear in every scene. \nA cat can always be seen\"\"\"\n\n# case folding\nphrase_lower = phrase.lower()\n\n# tokenization\ntokens = word_tokenize(phrase_lower)\n\n# counting\ntoken_count = Counter(tokens)\n\n# cat focus\ncat_list = [[k,token_count[k]] for k in token_count if \"cat\" in k]\n\nprint(tabulate(cat_list,\n               headers = [\"type\", \"count\"]))\n\n\n\ntype\ncount\n\n\n\n\ncats\n3\n\n\ncat\n1\n\n\n\nWe’ve still got the plural cats being counted as a separate word from cat, which for our weird use case, we don’t want. Our options here are to either “stem” or “lemmatize” our tokens."
  },
  {
    "objectID": "lectures/lem_stem/index.html#stemming",
    "href": "lectures/lem_stem/index.html#stemming",
    "title": "Lemmatizing and Stemming",
    "section": "Stemming",
    "text": "Stemming\nStemming is focused on cutting off morphemes and, to some degree, providing a consistent stem across all types that share a stem. So the outcomes aren’t always a recognizable word. The way it does this is all rule-based. For example, the first step of the Porter stemmer contains the following rewrite rules.\ni.   sses -> ss\nii.  ies -> i\niii. ss -> ss\niv.  s -> \nWhen a word comes into the first step, if its end matches any of the left hand sides, it will get re-written as the right hand side. If it could match multiple, the one it has the longest match with wins, so\n\n“passes” matches i. , so it gets rewritten as “pass”\n“pass” matches iii. and iv., but has the largest overlap with iii. so it gets rewritten as “pass”\n“parties” matches ii., so it gets rewritten as “parti”\n“pas” (as in “faux pas”) matches iv. so it gets rewritten as “pa”\n“cats” matches iv. so it gets rewritten as “cats”\n\nThis works basically correctly to the various /+z/ morphemes in English, but it over does it (“pas” should be left alone) and it produces some stems that don’t look like the actual root word (“parti” vs “party”).\nAfter this step, it contains a lot more hand crafted rules (e.g. ational - > ate).\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import SnowballStemmer\n\np_stemmer = PorterStemmer()\np_stemmed = [p_stemmer.stem(t) for t in tokens]\nfor t in p_stemmed:\n  print(f\"`{t}` |\", end = \" \")\nthe | 2019 | film | cat | is | a | movi | about | cat | . | cat | appear | in | everi | scene | . | a | cat | can | alway | be | seen |\ns_stemmer = SnowballStemmer(\"english\")\ns_stemmed = [s_stemmer.stem(t) for t in tokens]\nfor t in s_stemmed:\n  print(f\"`{t}` |\", end = \" \")\nthe | 2019 | film | cat | is | a | movi | about | cat | . | cat | appear | in | everi | scene | . | a | cat | can | alway | be | seen |\nJust to focus on how the stemmers operate over a specific paradigm:\ncry = [\"cry\", \"cries\", \"crying\", \"cried\", \"crier\"]\n\nprint(\n  tabulate(\n    [[c, s_stemmer.stem(c)] for c in cry],\n    headers=[\"token\", \"stem\"]\n  )\n)\n\n\n\ntoken\nstem\n\n\n\n\ncry\ncri\n\n\ncries\ncri\n\n\ncrying\ncri\n\n\ncried\ncri\n\n\ncrier\ncrier\n\n\n\nAlso, when something like inflectional morphology makes a change to the stem, it won’t get undone by the stemmer.\nrun = [\"run\", \"runs\", \"running\", \"ran\", \"runner\"]\n\nprint(\n  tabulate(\n    [[r, s_stemmer.stem(r)] for r in run],\n    headers=[\"token\", \"stem\"]\n  )\n)\n\n\n\ntoken\nstem\n\n\n\n\nrun\nrun\n\n\nruns\nrun\n\n\nrunning\nrun\n\n\nran\nran\n\n\nrunner\nrunner"
  },
  {
    "objectID": "lectures/lem_stem/index.html#lemmatizing",
    "href": "lectures/lem_stem/index.html#lemmatizing",
    "title": "Lemmatizing and Stemming",
    "section": "Lemmatizing",
    "text": "Lemmatizing\nLemmatizing involves a more complex morphological analysis of words, and as such requires language specific models to work.\n\nnltk lemmatizing\nnltk uses WordNet for its English lemmatizing. WordNet is a large database of lexical relations that have been hand annotated starting in the 1980s. Its outputs are always recognizable words.\n\nwnl = nltk.WordNetLemmatizer()\n\nprint(\n  tabulate(\n    [[c, wnl.lemmatize(c)] for c in cry],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n\n\n\ntoken\nlemma\n\n\n\n\ncry\ncry\n\n\ncries\ncry\n\n\ncrying\ncry\n\n\ncried\ncried\n\n\ncrier\ncrier\n\n\n\nprint(\n  tabulate(\n    [[r, wnl.lemmatize(r)] for r in run],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n\n\n\ntoken\nlemma\n\n\n\n\nrun\nrun\n\n\nruns\nrun\n\n\nrunning\nrunning\n\n\nran\nran\n\n\nrunner\nrunner\n\n\n\n\n\nspaCy lemmatizing\nspaCy has a number of models that do lemmatizing. They list WordNet along with a few other data sources for the model.\n\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\nlemmatizer = nlp.get_pipe(\"lemmatizer\")\n\ndoc = nlp(\" \".join(cry))\nprint(\n  tabulate(\n    [[c.text, c.lemma_] for c in doc],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n\n\n\ntoken\nlemma\n\n\n\n\ncry\ncry\n\n\ncries\ncry\n\n\ncrying\ncry\n\n\ncried\ncry\n\n\ncrier\ncrier\n\n\n\ndoc = nlp(\" \".join(run))\nprint(\n  tabulate(\n    [[r.text, r.lemma_] for r in doc],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n\n\n\ntoken\nlemma\n\n\n\n\nrun\nrun\n\n\nruns\nrun\n\n\nrunning\nrun\n\n\nran\nrun\n\n\nrunner\nrunner"
  },
  {
    "objectID": "lectures/lem_stem/index.html#the-use-of-lemmatizing-and-stemming",
    "href": "lectures/lem_stem/index.html#the-use-of-lemmatizing-and-stemming",
    "title": "Lemmatizing and Stemming",
    "section": "The use of lemmatizing and stemming",
    "text": "The use of lemmatizing and stemming\nFor a lot of the NLP tasks we’re going to be learning about, lemmatizing and stemming don’t factor in as part of the pre-processing pipeline. However, they’re useful tools to have handy when doing linguistic analyses. For example, for all of the importance of “word frequency” in linguistics literature, there’s often not much clarity about how the text was pre-processed to get these word frequencies."
  },
  {
    "objectID": "lectures/data_processing/addendum.html",
    "href": "lectures/data_processing/addendum.html",
    "title": "Addendum",
    "section": "",
    "text": "In the lecture notes before, I showed you how to do tokenizing in the python package nltk. But there’s another big NLP package out there called spaCy. Why did I focus on nltk? I think I can best explain that with this graph:"
  },
  {
    "objectID": "lectures/data_processing/addendum.html#tokenizing-with-spacy",
    "href": "lectures/data_processing/addendum.html#tokenizing-with-spacy",
    "title": "Addendum",
    "section": "Tokenizing with spaCy",
    "text": "Tokenizing with spaCy\n\n\n\n\n# bash\npython -m spacy download en_core_web_sm\n\n\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\nphrase2 = \"\"\"\nCATS had a budget of $100,000,000, most of which \nwent into the so-called 'digital fur technology'. \nIt's a little hard to believe, but it only made \n$75.5 million at the box office. #badmovie :-P\n\"\"\"\n\ndoc = nlp(phrase2.strip().replace(\"\\n\", \" \"))\n\nfor token in doc:\n  print(f\"| `{token.text}`\", end = \" \")\nCATS | had | a | budget | of | $ | 100,000,000 | , | most | of | which |  | went | into | the | so | - | called | ' | digital | fur | technology | ' | . |  | It | 's | a | little | hard | to | believe | , | but | it | only | made |  | $ | 75.5 | million | at | the | box | office | . | # | badmovie | :-P"
  },
  {
    "objectID": "lectures/data_processing/index.html",
    "href": "lectures/data_processing/index.html",
    "title": "Data Processing",
    "section": "",
    "text": "Before we even get to substantive issues of “text normalization” and “tokenization”, we need to also deal with basic data wrangling. For example, let’s say I wanted to download 4 works from Mary Shelly from Project Gutenberg and calculate what the most common 4 word sequences in her work are, I might quickly write some code like this.\n\n\n\n\n# python\n\n# urllib.request will download the books\nimport urllib.request\n\n# using a dictionary just to show the title of books here in the code.\nshelley_dict = {\"Tales and stories\": \"https://www.gutenberg.org/cache/epub/56665/pg56665.txt\",\n                \"Frankenstein\" : \"https://www.gutenberg.org/files/84/84-0.txt\",\n                \"The Last Man\" : \"https://www.gutenberg.org/cache/epub/18247/pg18247.txt\",\n                \"Mathilda\" : \"https://www.gutenberg.org/cache/epub/15238/pg15238.txt\"}\n\n# A collector list for all of the 4 word sequences\nall_grams4 = []\n\n# Loop over every url\nfor url in shelley_dict.values():\n  book_dat = urllib.request.urlopen(url)\n  \n  # this deals with the \n  # 1. character encoding\n  # 2. trailing whitespace\n  # 3. simplistic tokenization on spaces\n  book_lines = [line.decode(\"utf-8-sig\").strip().split(\" \") \n                for line in book_dat]\n  \n  # This flattens the list above into one long list of words\n  book_words = [word \n                for line in book_lines \n                  for word in line \n                    if len(word) > 0]\n  \n  # Collector list of 4grams from just this book\n  grams4 = []\n  \n  # loop over every index postion up to 4 words short of the end.\n  for i in range(len(book_words)-4):\n    \n    # glue together 4 word sequences with \"_\"\n    grams4.append(\"_\".join(book_words[i:(i+4)]))\n    \n  # Add this book's 4grams to all of the books' 4grams\n  all_grams4 += grams4\n\nThe list all_grams4 contains a list of every token of 4grams in these books. Let’s count them up and look at the top 10 most frequent 4 word phrases Mary Shelley used in her writing!\n\n\n\nTable 1: Top 10 4grams from Mary Shelley’s Work\n\n\n4gram\ncount\n\n\n\n\nProject_Gutenberg_Literary_Archive\n52\n\n\nthe_Project_Gutenberg_Literary\n44\n\n\nGutenberg_Literary_Archive_Foundation\n33\n\n\nthe_terms_of_this\n32\n\n\nProject_Gutenberg-tm_electronic_works\n31\n\n\nat_the_same_time\n25\n\n\nin_the_United_States\n24\n\n\nto_the_Project_Gutenberg\n24\n\n\n*_*_*_*\n22\n\n\nfor_the_sake_of\n21\n\n\n\n\n\nSo, either Mary Shelly was obsessed with the Project Gutenberg Literary Archive, and the terms of this and for the sake of, or something else is going on.\nAs it turns out, every plain text Project Gutenberg book has header information with a short version of the users’ rights and other metadata information, and then at the end has the entirety of the Project Gutenberg License, which is written in legal language.\nIn any corpus building project, decisions need to be made about how header, footer, and general boilerplate data like this will be treated. There are handy packages for python and R that make stripping out the legal language easy\n\npython: gutenbergpy\nR: gutenbergr\n\nOr, you might decide to leave it all in. It seems pretty clear this is the approach to the dataset they trained GPT-3 on, because if you prompt it with the first few lines of the Project Gutenberg license, it will continue it.\n\nFigure 1: The original Project Gutenberg License vs what GPT3 reproduces\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting aside the issue of headers and footers, we also need to deal with the fact that “markup” is everywhere. Even in the relatively plain text of Project Gutenberg books, they use underscores _ to indicate italics or emphasized text.\n\n# python\nunderscore_lines = [line \n                      for line in book_lines \n                        if any([\"_\" in word \n                                  for word in line])]\nfor i in range(4):\n  print(\" \".join(underscore_lines[i]))\n\nMathilda _is being published\nof_ Studies in Philology.\nnovelette _Mathilda_ together with the opening pages of its rough\ndraft, _The Fields of Fancy_. They are transcribed from the microfilm\n\n\nThis, again, is something we need to decide whether or not we want to include in our corpora. For these massive language models that focus on text generation, they may want the model to generate markup along with the text, so they might leave it in. Some text markup that’s intended to indicate prosodic patterns could be useful to leave in from a linguistic theory perspective.\nEither way, though, it is still a decision that needs to be made about the data."
  },
  {
    "objectID": "lectures/data_processing/index.html#text-normalization",
    "href": "lectures/data_processing/index.html#text-normalization",
    "title": "Data Processing",
    "section": "Text Normalization",
    "text": "Text Normalization\nI called the issues above “data wrangling”, since it’s mostly about identifying the content we want to be including in our modelling. But once you’ve done that, there are still questions about how we process data for analysis which fall under “text normalization”.\nConsider the following sentences\n\nThe 2019 film Cats is a movie about cats. Cats appear in every scene. A cat can always be seen.\n\nLet’s split this sentence up along whitespace1, and count how many times “cats” appears.\n\nimport re\nphrase = \"\"\"The 2019 film Cats is a movie about cats. \nCats appear in every scene. \nA cat can always be seen\"\"\"\n\nwords = re.split(\"\\s\", phrase)\ncat_c = Counter(words)\n\n\n\n\nTable 2: frequency of “cats”\n\n\n\n\n\n\nword\ncount\n\n\n\n\nCats\n2\n\n\ncats.\n1\n\n\ncat\n1\n\n\n\n\n\nA very important thing to keep in mind is that our language models will treat the words in these rows as three completely separate word types.2 That even includes the period . in the second row. Some typical steps involve\n\nseparating punctuation from words\n“case folding” or converting everything to lowercase.\n\n\nwords2 = re.split(r\"\\s\", phrase)\nwords2 = [re.sub(r\"\\W\", '', word) for word in words2]\nwords2 = [word.lower() for word in words2]\nwords2\n\n['the', '2019', 'film', 'cats', 'is', 'a', 'movie', 'about', 'cats', '', 'cats', 'appear', 'in', 'every', 'scene', '', 'a', 'cat', 'can', 'always', 'be', 'seen']\n\n\n\n\n\nTable 3: frequency of tokenized “cats”\n\n\nword\ncount\n\n\n\n\ncats\n3\n\n\ncat\n1\n\n\n\n\n\nWe’ve now got a slightly better set of counts. With the punctuation stripped and everything pushed to lowercase, there’s now just two word forms: cats and cat.\nOne downside, though, is we’ve also collapsed together the title Cats, which refers to either a Broadway musical or a 2019 film, and the word “cats” which refers to furry felines. Merging these two together could be sub-optimal for later tasks, like, say, sentiment analysis of movie reviews.\n\n‘Cats’ is both a horror and an endurance test, a dispatch from some neon-drenched netherworld where the ghastly is inextricable from the tedious. – LA Times"
  },
  {
    "objectID": "lectures/data_processing/index.html#tokenization-or-text-is-complex",
    "href": "lectures/data_processing/index.html#tokenization-or-text-is-complex",
    "title": "Data Processing",
    "section": "Tokenization (or, text is complex)",
    "text": "Tokenization (or, text is complex)\nSetting aside semantic issues, there are a lot of things that happen inside of text, especially if it is transcribed speech, that makes normalizing text and tokenizing it way more challenging than just splitting up on white space and stripping out punctuation, even just for English.\n\nPlaces to leave in punctuation\nSome examples given by Jurafsky & Martin for where you might want to leave in punctuation are:\n\nYou don’t want to eliminate punctuation from inside Ph.D, or m.p.h.. You also don’t want to eliminate it from some proper names, like ampersands in Procter & Gamble, Texas A&M, A&W, m&m's.\nYou’ll want to keep formatting in numerals, and not split them into separate words. These are all possible numeric formats cross culturally for the same quantity\n\n1,000.55\n1.000,55\n1 000,55\n\nCurrency symbols should probably be kept together with their numerals, and depending on the culture & denomination.\n\n$0.99\n99¢\n€0,99\n\nDates: There are so many different permutations on how dates can be formatted that I shouldn’t list them all here, but here are some.3\n\nyyyy-mm-dd 2022-09-12, yyyy/mm/dd 2022/09/12\nyyyy-m-dd 2022-9-12, yyyy/m/dd 2022/9/12\ndd-mm-yyyy 12-09-2022, dd/mm/yyyy 12/09/2022\ndd-m-yyyy 12-9-2022, dd/m/yyyy 12/9/2022\ndd-mm-yy 12-09-22, dd/mm/yy 12/09/2022\nmm-dd-yyyy 09-12-2022, mm/dd/yyyy 09/12/2022\nm-dd-yyyy 9-12-2022, m/dd/yyyy 9/12/2022\nmm-dd-yy 09-12-22, mm/dd/yy 09/12/22\nm-dd-yy 9-12-22, m/dd/yy 9/12/22\n\nEmoticons,4 where the token is entirely punctuation :), >.<.\n\n\n\nPlaces to split up words\nSometimes the tokens you get back from whitespace tokenization ought to be split up even further. One example might be hyphenated words, like hard-won.\n\nhard-won ➔ hard, won or hard, -, won.\n\nAnother example involves clitics, like n't or 're in English.\n\nisn't ➔ is, n't\ncan't ➔ ca, n't\nwhat're ➔ what, 're\n\n\n\nPlaces to glue words together\nYou might want to also glue together tokens from whitespace tokenization.\n\nNew, York, City ➔ New York City\nSuper, Smash, Brothers ➔ Super Smash Brothers.\n\n\n\nChallenges with speech and text\n\n: $1500\n\n: “one thousand five hundred dollars”\n: “fifteen hundred dollars”\n: “one and a half thousand dollars”\n: “one point five thousand dollars”"
  },
  {
    "objectID": "lectures/data_processing/index.html#tokenizers",
    "href": "lectures/data_processing/index.html#tokenizers",
    "title": "Data Processing",
    "section": "Tokenizers",
    "text": "Tokenizers\nThere seem to be broadly two kinds of tokenizers people use, depending on their goals.\n\nTokenizers that try to hew to linguistic structure, and can generate relatively large vocabulary sizes (number of tokens).\nTokenizers that try to keep the vocabulary size relatively small, to make neural network training possible.\n\n\nWord/language piece based tokenizers\nThere are a number of tokenizers available through the nltk (Natural Language Took Kit) (Bird, Klein, and Loper 2009) python package. They all have slightly different settings and outcomes. Here I’ll compare the PennTreeBank tokenizer, a simpler punctuation-based tokenizer, and a “casual” tokenizer.\n\nPennTreeBank\nThe PennTreeBank tokenizer is built up out of regular expressions (more on that soon). It\n\nseparates out punctuation and non-alphanumeric characters as their own tokens\nSeparates off contractions as their own tokens, using a fixed list\n\n\nfrom nltk.tokenize import sent_tokenize, TreebankWordTokenizer\n\nphrase2 = \"\"\"CATS had a budget of $100,000,000, most of which went into the so-called 'digital fur technology'.\nIt's a little hard to believe, but it only made $75.5 million at the box office. \n#badmovie :-P\n\"\"\"\n\nsentences = sent_tokenize(phrase2)\ntokens = [TreebankWordTokenizer().tokenize(s) for s in sentences]\n\n\n\n\nCATS\nhad\na\nbudget\nof\n$\n\n\n100,000,000\n,\nmost\nof\nwhich\nwent\n\n\ninto\nthe\nso-called\n'digital\nfur\ntechnology\n\n\n'\n.\nIt\n's\na\nlittle\n\n\nhard\nto\nbelieve\n,\nbut\nit\n\n\nonly\nmade\n$\n75.5\nmillion\nat\n\n\nthe\nbox\noffice\n.\n#\nbadmovie\n\n\n:\n-P\n\n\n\n\n\n\n\n\n\nSimple whitespace + punctuation tokenizer\nSplits strings based on whitespace & non-alphanum\n\nfrom nltk.tokenize import wordpunct_tokenize\ntokens = [wordpunct_tokenize(s) for s in sentences]\n\n\n\n\nCATS\nhad\na\nbudget\nof\n$\n\n\n100\n,\n000\n,\n000\n,\n\n\nmost\nof\nwhich\nwent\ninto\nthe\n\n\nso\n-\ncalled\n'\ndigital\nfur\n\n\ntechnology\n'.\nIt\n'\ns\na\n\n\nlittle\nhard\nto\nbelieve\n,\nbut\n\n\nit\nonly\nmade\n$\n75\n.\n\n\n5\nmillion\nat\nthe\nbox\noffice\n\n\n.\n#\nbadmovie\n:-\nP\n\n\n\n\n\n\nTweet Tokenizer\nIntended to be more apt for tokenizing tweets.\n\nfrom nltk.tokenize import TweetTokenizer\ntokens = [TweetTokenizer().tokenize(s) for s in sentences]\n\n\n\n\nCATS\nhad\na\nbudget\nof\n$\n\n\n100,000\n,\n000\n,\nmost\nof\n\n\nwhich\nwent\ninto\nthe\nso-called\n'\n\n\ndigital\nfur\ntechnology\n'\n.\nIt's\n\n\na\nlittle\nhard\nto\nbelieve\n,\n\n\nbut\nit\nonly\nmade\n$\n75.5\n\n\nmillion\nat\nthe\nbox\noffice\n.\n\n\n#badmovie\n:-P\n\n\n\n\n\n\n\n\n\n\nFixed Vocab Tokenizing\nThe downside of tokenizers like the three above is that you can’t pre-specify how many types you will get out. That is, you can’t pre-specify your vocabulary size. That isn’t ideal for neural-network based models, which need to use matrices of finite and pre-specified size. So there are also tokenizers that keep a fixed cap on the vocabulary size, even if they result in tokens that aren’t really linguistically meaningful.\n\nByte Pair Encoding\nByte Pair Encoding is one approach to tokenizing where we can pre-specify the maximum vocabulary size either by setting a maximum vocabulary hyperparameter, or by setting a maximum iteration hyperparameter.\nLet’s start out with a fake training of a byte pair encoder with the simple vocabulary “cats can’t canter”. We kick things off treating every character as a token, plus a specialized start-of-word symbol, which I’m representing with _.\n\n\n\nTokens\n\n\n_ c a t s\n_ c a n ' t\n_ c a n t e r\n\n\n\n\nTypes\n\n\n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", '_'}\n\n\n\n\n\n\nThis is, in principle, the smallest and simplest tokenization we could do for any input text. While the total number of words is infinite, the total number of characters or symbols we use to create those words is finite.\nThe next step is to count up all of the pairs (or bigrams) of tokens in the training data. In this case, both (_, c) and (c, a) appear equally commonly, so I make a decision and say (_, c) is the one we’ll process first. We’ll paste them together, call them a new type, and replace all (_, c) sequences with _c.\n\n\n\ntokens\n\n\n_c a t s\n_c a n ' t\n_c a n t e r\n\n\n\n\ntypes\n\n\n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", ' ',\n`_c`}\n\n\n\nRepeating the process, the most frequently occurring bigram is now (_c, a), so we’ll add _ca as a new type, and replace all (_c, a) sequences with _ca.\n\n\n\ntokens\n\n\n_ca t s \n_ca n ' t _\n_ca n t e r _\n\n\n\n\ntypes\n\n\n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", ' ', \n'_c', '_ca'}\n\n\n\nFinally, the last most frequent sequence is (_ca, n), so we’ll add _can to the vocabulary, and collapse (_ca, n) sequences.\n\n\n\ntokens\n\n\n_ca t s\n_can ' t\n_can t e r\n\n\n\n\ntypes\n\n\n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", ' ', \n'_c', '_ca', '_can'}\n\n\n\nWe’ll stop at that point, but we could either continue for a fixed number of iterations, or until our type, or vocabulary size reaches a fixed number.\n\n\nTraining\nThe python library sentencepiece has a method for training a BPE tokenizer. We’ll run it on Frankenstein with a vocabulary limited to about ~1,000 items to see what we’ll get.\n\nimport gutenbergpy.textget\nfrom gutenbergpy.gutenbergcache import GutenbergCacheSettings\nimport sentencepiece as spm\n\nGutenbergCacheSettings.set(TextFilesCacheFolder = \"gen/texts/\")\n\nraw_book = gutenbergpy.textget.get_text_by_id(84)\n\nclean_book  = gutenbergpy.textget.strip_headers(raw_book)\nwith open(\"gen/texts/frank.txt\", 'wb') as file:\n  x = file.write(clean_book)\n  file.close()\n\n\nspm.SentencePieceTrainer.train(input = \"gen/texts/frank.txt\", \n                               model_prefix = \"gen/m\",\n                               vocab_size = 1000, \n                               model_type = \"bpe\")\n\nThe code above trained the tokenizer and saved it as a model. Here we’ll load it and run it on a sample paragraph to look at the output.\n\nsp = spm.SentencePieceProcessor(model_file='gen/m.model')\n\n\npara = \"\"\"\nYou will rejoice to hear that no disaster has accompanied the\ncommencement of an enterprise which you have regarded with such evil\nforebodings. I arrived here yesterday, and my first task is to assure\nmy dear sister of my welfare and increasing confidence in the success\nof my undertaking\n\"\"\"\n\nprint(sp.encode_as_pieces(para))\n\n['▁You', '▁will', '▁re', 'j', 'o', 'ice', '▁to', '▁he', 'ar', '▁that', '▁no', '▁dis', 'as', 'ter', '▁has', '▁acc', 'om', 'p', 'an', 'ied', '▁the', '▁comm', 'ence', 'ment', '▁of', '▁an', '▁enter', 'pr', 'ise', '▁which', '▁you', '▁have', '▁reg', 'ard', 'ed', '▁with', '▁such', '▁ev', 'il', '▁f', 'ore', 'b', 'od', 'ings', '.', '▁I', '▁arri', 'ved', '▁he', 're', '▁y', 'es', 'ter', 'd', 'ay', ',', '▁and', '▁my', '▁first', '▁t', 'as', 'k', '▁is', '▁to', '▁ass', 'ure', '▁my', '▁dear', '▁s', 'is', 'ter', '▁of', '▁my', '▁we', 'lf', 'are', '▁and', '▁inc', 're', 'as', 'ing', '▁conf', 'id', 'ence', '▁in', '▁the', '▁su', 'cc', 'ess', '▁of', '▁my', '▁under', 't', 'aking']\n\n\n\n\nThe Benefit?\nWhy tokenize things this way? On the one hand, we wind up with tokens that don’t align with any particular linguistic structure. On the other, we’ve got a predictable vocabulary size, and a dramatically less serious data sparsity problem.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmethod\nonce\ntotal\nproportion once\n\n\n\n\nbpe\n9\n1015\n0.009\n\n\nptb\n3533\n7710\n0.458"
  },
  {
    "objectID": "lectures/ngram/01-ngram-eval.html#evaluating-ngram-models",
    "href": "lectures/ngram/01-ngram-eval.html#evaluating-ngram-models",
    "title": "ngrams - Perplexity",
    "section": "Evaluating NGram Models",
    "text": "Evaluating NGram Models\nNGram Models are often described in terms of their perplexity, which is a technical term from Information Theory. Rather than just dump the formula in here, let’s walk through it, since these information theoretic notions kind of keep coming up."
  },
  {
    "objectID": "lectures/ngram/01-ngram-eval.html#information---in-bits",
    "href": "lectures/ngram/01-ngram-eval.html#information---in-bits",
    "title": "ngrams - Perplexity",
    "section": "Information - In Bits",
    "text": "Information - In Bits\n\nFrom bits to probability - a light switch analogy\n\nOne light switch\nLet’s say we have a house with one room, and that one room has one light switch, and that light switch could be on  or off . Here’s a table of all of the possible lighting states of the house:\n\nLighting configurations, 1 switch\n\n\nLighting Config\nSwitch 1\n\n\n\n\non\n\n\n\noff\n\n\n\n\nWith just 1 switch, we have two possible lighting configurations. If each lighting configuration was equally possible, then the probability of seeing either lighting configuration is 0.5. In information theory terms, our switch is a “bit” (just like the computer bit), and you need 1 bit to represent a 50% probability.\n\\[\n1\\text{bit} = 2~\\text{states} = \\frac{1}{2} \\text{probability} = 0.5\n\\]\n\n\nTwo light switches\nNow, if our house had two rooms, (a living room and a kitchen), and each room had a switch, we can also workout how many different lighting configurations there are for the whole house.\n\nLighting configurations, 2 switches\n\n\n\n\n\n\n\nlighting configurations\nLiving Room Switch\nKitchen Switch\n\n\n\n\nliving room on, kitchen on\n\n\n\n\nliving room on, kitchen off\n\n\n\n\nliving room off, kitchen on\n\n\n\n\nliving room off, kitchen off\n\n\n\n\n\nWith 2 switches, we can describe 4 different lighting configurations in the house. And again, if every lighting configuration was equally likely, that means there is a \\(\\frac{1}{4}=0.25\\) probability we are describing with these two bits.\n\\[\n2\\text{bits} = 4~\\text{states}=\\frac{1}{4}=0.25\n\\]\n\n\nThree Light switches\nLet’s add 1 more room to the house (and then we’ll stop) that also has a switch. Here’s the table of house lighting configurations.\n\n\n\n\n\n\n\n\n\nlighting configuration\nLiving Room Switch\nKitchen Switch\nBedroom Switch\n\n\n\n\nliving room on, kitchen on, bedroom on\n\n\n\n\n\nliving room on, kitchen on, bedroom off\n\n\n\n\n\nliving room on, kitchen off, bedroom on\n\n\n\n\n\nliving room on, kitchen off, bedroom off\n\n\n\n\n\nliving room off, kitchen on, bedroom on\n\n\n\n\n\nliving room off, kitchen on, bedroom off\n\n\n\n\n\nliving room off, kitchen off, bedroom on\n\n\n\n\n\nliving room off, kitchen off, bedroom off\n\n\n\n\n\n\nWith 3 switches, we can describe 8 different house light configurations\n\\[\n3\\text{bits} = 8~\\text{states} = \\frac{1}{8} = 0.125\n\\]\n\n\nN Light switches\nThere is a general formula for figuring out how many states can be described by N bits, and therefore the probability of events they can represent.\n\\[\nN~\\text{bits} = 2^N ~\\text{states} = \\frac{1}{2^N}~\\text{probability}\n\\]\nThe number 2 got in there because that’s how many different options there are for each switch (on  or off )."
  },
  {
    "objectID": "lectures/ngram/01-ngram-eval.html#from-probability-to-bits-a.k.a.-surprisal",
    "href": "lectures/ngram/01-ngram-eval.html#from-probability-to-bits-a.k.a.-surprisal",
    "title": "ngrams - Perplexity",
    "section": "From probability to bits (a.k.a. “surprisal”)",
    "text": "From probability to bits (a.k.a. “surprisal”)\nOk, what if we didn’t know how many bits, or switches we had, but we knew the probability of something, and we want to know how many bits we need to represent that probability. For example, maybe we estimated the unigram probability of “the” in the novel Frankenstein.\n\n\n\n\n\n\nR\n\nfrank %>%\n  count(word) %>%\n  mutate(prob = n/sum(n)) %>%\n  filter(word == \"the\")\n\n\n# A tibble: 1 × 3\n  word      n   prob\n  <chr> <int>  <dbl>\n1 the    4194 0.0558\n\n\nWe’re in a classic math word problem: Solve for N\n\\[\n0.056 = \\frac{1}{2^N}\n\\]\nI’ve put the math steps to work this out in the collapsed Details block below, but to get N here, we need to take the negative \\(\\log_2\\) of the probability\n\n\\[\n\\frac{1}{2^N} = 2^{-N}\n\\]\n\\[\n\\log_2(2^{-N}) = -N\n\\]\n\\[\n-\\log_2(2^{-N}) = N\n\\]\n\n\\[\nN = -\\log_2(0.056) = 4.16\n\\]\nWe’ve obviously moved away from the concrete analogy of light switches, since it’s impossible to have 4 and 0.16 switches in your house. But this is a measure of how much information the probability takes up. It’s also often called surprisal as a technical term.\n\nWhy “Surprisal”\nImagine I came up to you and said:\n\nThe sun rose this morning.\n\nThat’s not especially informative or surprising, since the sun rises every morning. The sun rising in the morning is a very high probability event,1 so it’s not surprising it happens, and in the information theory world, we don’t need very many bits for it.\nOn the other hand, if someone came up to me and said:\n\nThe sun failed to rise this morning.\n\nThat is surprising! It’s also very informative. Thank you for telling me! I wasn’t expecting that! The smaller the probability of an event, the more surprising and informative it is if it happens, the larger the surprisal value is.\nHere’s a plot showing the relationship between the unigram probability of a word in Frankenstein, and its surprisal in bits."
  },
  {
    "objectID": "lectures/ngram/01-ngram-eval.html#expected-surprisal-a.k.a-entropy",
    "href": "lectures/ngram/01-ngram-eval.html#expected-surprisal-a.k.a-entropy",
    "title": "ngrams - Perplexity",
    "section": "Expected Surprisal, (a.k.a Entropy)",
    "text": "Expected Surprisal, (a.k.a Entropy)\nSo, we can calculate the probability of individual words in a book like Frankenstein, and from that probability, we can calculate each word’s surprisal.\nThe next thing to ask is what is the expected surprisal in a book like Frankenstein? As we’re reading the book, very common words will happen very often, and less common words will happen less often. What is our overall experience of reading the book like, in terms of surprisal? Here’s a table of some words that have a wide range of frequencies in the book.\n\n\n\nR\n\nfrank %>%\n  count(word) %>%\n  arrange(desc(n)) %>%\n  mutate(total = sum(n),\n         prob = n/sum(n),\n         surprisal = -log2(prob)) %>%\n  filter(word %in% c(\"the\", \"monster\", \"snow\", \"russia\"))\n\n\n# A tibble: 4 × 5\n  word        n total      prob surprisal\n  <chr>   <int> <int>     <dbl>     <dbl>\n1 the      4194 75143 0.0558         4.16\n2 monster    31 75143 0.000413      11.2 \n3 snow       16 75143 0.000213      12.2 \n4 russia      2 75143 0.0000266     15.2 \n\n\nWe could try just taking the average of the surprisal column to get the “average surprisal”, but that’s not quite right in terms of capturing the expected surprisal. Yes, words like snow and russia have a large surprisals, so they should drag our estimate upwards, but they don’t, by definition, happen all that often, so they shouldn’t drag it up too much.\nWhat we do instead is multiply the surprisal value of each word by its probability, and sum it up! This will capture our experience of the having a small surprisal and happening often, and words like snow having a large surprisal, but happening less often.\nThis “expected surprisal” is called entropy, and is often represented by \\(H(X)\\)\n\\[\n\\begin{array}{ll}\n\\text{surprisal:} & {s(x_i)=-\\log_2(p(x_i))}\\\\\n\\text{entropy:} & H(X) = \\sum_{i=1}^np(x_i)s(x_i)\n\\end{array}\n\\]\n\n\n\nR\n\nfrank %>%\n  count(word) %>%\n  arrange(desc(n)) %>%\n  mutate(prob = n/sum(n),\n         surprisal = -log2(prob)) %>%\n  summarise(entropy = sum(prob * surprisal))\n\n\n# A tibble: 1 × 1\n  entropy\n    <dbl>\n1    9.30\n\n\nSo, on average, while reading Frankenstein, (and only paying attention to unigram distribution), we have an expected surprisal (a.k.a, entropy) of \\(\\approx\\) 9.3 bits."
  },
  {
    "objectID": "lectures/ngram/01-ngram-eval.html#from-bits-back-to-states-a.k.a.-perplexity",
    "href": "lectures/ngram/01-ngram-eval.html#from-bits-back-to-states-a.k.a.-perplexity",
    "title": "ngrams - Perplexity",
    "section": "From bits back to states (a.k.a. Perplexity)",
    "text": "From bits back to states (a.k.a. Perplexity)\nNow there are just over 7,000 unique word types in Frankenstein.2\n\n\n\nR\n\nfrank %>%\n  count(word) %>%\n  nrow()\n\n\n[1] 7020\n\n\nBut because not every word is equally likely to show up, the expected surprisal, or entropy of the book is 9.3 bits. Just above, we saw that we can calculate the number of unique states we can encode with N bits with this formula:\n\\[\nN~\\text{bits} = 2^N~\\text{states}\n\\]\nIf we do this with the entropy value we got in bits, that would be\n\\[\nH(\\text{Frankenstein}) \\approx 9.3~\\text{bits}\n\\]\n\\[\n9.3\\text{bits} = 2^{9.3}~\\text{states} \\approx 630~\\text{states}\n\\]This is the estimated “perplexity” of the unigram model. Here’s how to think about it. If we already know the probability of every word that appears in the book, and we use that probability distribution to guess each next word in the book, it’s going to be as successful as trying to guess which next state is coming up out of 630 equally probable states."
  },
  {
    "objectID": "lectures/ngram/01-ngram-eval.html#perplexity-of-ngram-models",
    "href": "lectures/ngram/01-ngram-eval.html#perplexity-of-ngram-models",
    "title": "ngrams - Perplexity",
    "section": "Perplexity of ngram models",
    "text": "Perplexity of ngram models\nIn the first lecture on ngram models, we built a boring bigram model that looked like this.\n\n\n\n\n\n   \n\na\n\n a   \n\nbook\n\n book   \n\na->book\n\n  0.5   \n\ndog\n\n dog   \n\na->dog\n\n  0.5   \n\nEND\n\n END   \n\nbook->END\n\n  1   \n\ndog->END\n\n  1   \n\nI\n\n I   \n\nread\n\n read   \n\nI->read\n\n  0.25   \n\nsaw\n\n saw   \n\nI->saw\n\n  0.75   \n\nread->a\n\n  1   \n\nsaw->a\n\n  0.75   \n\nthe\n\n the   \n\nsaw->the\n\n  0.25   \n\nthe->dog\n\n  1   \n\nSTART\n\n START   \n\nSTART->I\n\n  0.8   \n\nWe\n\n We   \n\nSTART->We\n\n  0.2   \n\nWe->saw\n\n  1  \n\n\n\n\n\nAnd, we worked out that we could estimate the probability a new sentence like this:\n\nP(We | <START>) \\(\\times\\) P(saw | We) \\(\\times\\) P(the | saw) \\(\\times\\) P(dog | saw) \\(\\times\\) P(<END> | dog)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwords\na\nbook\ndog\nEND\nI\nread\nsaw\nthe\nWe\n\n\n\n\na\n0\n0.5\n0.5\n0\n0\n0\n0\n0\n0\n\n\nbook\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\ndog\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nI\n0\n0\n0\n0\n0\n0.25\n0.75\n0\n0\n\n\nread\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw\n0.75\n0\n0\n0\n0\n0\n0\n0.25\n0\n\n\nSTART\n0\n0\n0\n0\n0.8\n0\n0\n0\n0.2\n\n\nthe\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nWe\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n\n\\[\nP(X) = 0.2 \\times 1 \\times 1\\times 0.25 \\times 1 = 0.05\n\\]\nWith this probability, we can figure out how many total bits we need to encode this probability (a.k.a. the surprisal)\n\\[\ns(X) = -\\log_2(p(X)) = -\\log_2(0.05) = 4.32\n\\]\nNow, where evaluating ngram models diverge a bit from what we were doing before is we now figure out what the surprisial is per word, so we get bits per word (including the sentence ending tag).\n\\[\nspw(X)=\\frac{-\\log_2(p(X))}{N} = \\frac{-\\log_2(0.05)}{5} =0.86\n\\] To get the perplexity of this sentence, given the bigram model, we follow the formula for getting the number of states given a number of bits.\n\\[\npp(X) = 2^{spw(X)} = 2^{0.86} = 1.82\n\\]\n\nComparing models\nIf model A assigns higher probabilities to the sentences in a test set than model B, then model A will have a smaller perplexity.\nAnother way to think about the perplexity if ngram models, as Jurafsky & Martin point out, is that it’s the “weighted average branching factor”. Looking back at the bigram diagram above, it’s like saying “Any time you are at a node, you are, on average, choosing between 1.82 possible branches to go down.”"
  },
  {
    "objectID": "lectures/ngram/index.html",
    "href": "lectures/ngram/index.html",
    "title": "ngram Language Models",
    "section": "",
    "text": "I’m going to split up the “ngram model” materials into explaining how they work in principle, vs the how we have to make engineering decisions to make them work in reality."
  },
  {
    "objectID": "lectures/ngram/index.html#language-prediction",
    "href": "lectures/ngram/index.html#language-prediction",
    "title": "ngram Language Models",
    "section": "Language Prediction",
    "text": "Language Prediction\nWhen we are perceiving language, we are constantly and in real-time making predictions about what we are about to hear next. While we’re going to be talking about this in terms of predicting the next word, It’s been shown that we do this even partway through a word (Allopenna, Magnuson, and Tanenhaus 1998).\nSo, let’s say I spoke this much of a sentence to you:\n\nI could tell he was angry from the tone of his___\n\nAnd then a sudden noise obscured the final word, and you only caught part of it. Which of the following three words was I probably trying to say?\n\nboys\nchoice\nvoice\n\nYour ability to guess which word it was is based on your i) experience with English turns of phrase and ii) the information in the context.\nOne goal of Language Models is to assign probabilities across the vocabulary for what the next word will be, and hopefully assign higher probabilities to the “correct” answer than the “incorrect” answer. Applications for this kind of prediction range from speech-to-text (which could suffer from a very similar circumstance as the fictional one above) to autocomplete or spellcheck."
  },
  {
    "objectID": "lectures/ngram/index.html#using-context-ngrams",
    "href": "lectures/ngram/index.html#using-context-ngrams",
    "title": "ngram Language Models",
    "section": "Using context (ngrams)",
    "text": "Using context (ngrams)\nIn the example sentence above, one way we could go about trying to predict which word is most likely is to count up how many times the phrase “I could tell he was angry from the tone of his___” is finished by the candidate words. Here’s a table of google hits for the three possible phrases, as well as all hits for just the context phrase.\n\n\n\n“I could tell he was angry from the tone of his”\ncount\n\n\n\n\nboys\n0\n\n\nchoice\n0\n\n\nvoice\n3\n\n\n“I could tell he was angry from the tone of his”\n3\n\n\n\nWe’re going to start diving into mathematical formulas now (fortunately the numbers are easy right now).\nTo represent the count of a word or string of words in a corpus. We’ll use \\(C(\\text{word})\\). So given the table above we have\n\\[\n\\displaylines{C(\\text{I could tell he was angry from the tone of his})=3\\\\\nC(\\text{I could tell he was angry from the tone of his boys})=0\\\\\nC(\\text{I could tell he was angry from the tone of his choice})=0\\\\\nC(\\text{I could tell he was angry from the tone of his voice})=3}\n\\]\nTo describe the probability that the next word is “choice” given that we’ve already heard “I could tell he was angry from the tone of his”, we’ll use the notation \\(P(\\text{choice} | \\text{I could tell he was angry from the tone of his})\\). To calculate that probability, we’ll divide the total count of the whole phrase by the count of the preceding context.\n\\[\nP(\\text{choice} | \\text{I could tell he was angry from the tone of his}) = \\frac{C(\\text{I could tell he was angry by the tone of his choice})}{C(\\text{I could tell he was angry by the tone of his})} = \\frac{0}{3} = 0\n\\]\nIn fact, we can estimate the probability of an entire sentence with the Probability Chain Rule. The probability of a sequence of events like \\(P(X_1X_2X_3)\\) can be estimated by multiplying out their conditional probabilities like so:\n\\[\nP(X_1X_2X_3) = P(X_1)P(X_2|X_1)P(X_3|X_1X_2)\n\\]\nOr, to use a phrase as an example:1\n\\[\nP(\\text{du hast mich gefragt})=P(\\text{du})P(\\text{hast}|\\text{du})P(\\text{mich}|\\text{du hast})P(\\text{gefragt}|\\text{du hast mich})\n\\]\n\nData Sparsity rears its head\nThe problem with data sparsity rears its head, though. As we can already see in the table above, long phrases, although possible, might not appear in any corpus, giving us a very unreliable probability estimate.\nInstead of using the whole history, we can use a smaller context in a more strictly defined window. So, instead of looking at the whole sentence, what if we looked at counts of just “of his” from the example sentence.\n\n\n\n“of his”\ncount (in millions)\n\n\n\n\nboys\n2.2\n\n\nchoice\n14.2\n\n\nvoice\n44.5\n\n\n“of his”\n2,400.0\n\n\n\n\\[\n\\displaylines{\nP(\\text{boys} | \\text{of his}) = \\frac{C(\\text{of his boys)}}{C(\\text{of his})}=\\frac{2.2}{2400} = 0.0009\\\\\nP({\\text{choice}|\\text{of his}})= \\frac{C(\\text{of his choice)}}{C(\\text{of his})}=\\frac{14.2}{2400} = 0.005\\\\\nP({\\text{voice}|\\text{of his}})= \\frac{C(\\text{of his voice)}}{C(\\text{of his})}=\\frac{44.5}{2400} = 0.018}\n\\]\nThe continuation “voice” here is still relatively low probability, but has the highest probability of our candidate set.\nThis is the basic approach of an ngram model. Instead of using all available words to calculate the probability of the next word, we’ll approximate it with a smaller window. The example in the table above is a “trigram” model.\n\n\n\n\n\n\n“gram” names\n\n\n\n\nunigram:\n\nCounting up every individual (1) word, and try to estimate the probability of word in isolation.\n\nbigram:\n\nCount up every sequence of two words, and try to estimate the probability of a word given just one word before it,\n\ntrigram\n\nCount up every sequence of three words, and try to estimate the probability of a word given just the two words before it.\n\n\n“Trigrams” are the last n-gram with a special name. The rest are just called “4-gram” or “5-gram”.\n\n\n\n\n\n\n\nBuilding up a bigram model\nLet’s look at what happens as we gradually build up a bigram model we’ll start with one sentence.\nI saw the dog\n\n\n\n\n\n\n\n\n   \n\ndog\n\n dog   \n\nEND\n\n END   \n\ndog->END\n\n  1   \n\nI\n\n I   \n\nsaw\n\n saw   \n\nI->saw\n\n  1   \n\nthe\n\n the   \n\nsaw->the\n\n  1   \n\nthe->dog\n\n  1   \n\nSTART\n\n START   \n\nSTART->I\n\n  1  \n\n\n\n\n\nI saw the dog\nWe saw a dog\n\n\n\n\n\n\n\n\n   \n\na\n\n a   \n\ndog\n\n dog   \n\na->dog\n\n  1   \n\nEND\n\n END   \n\ndog->END\n\n  1   \n\nI\n\n I   \n\nsaw\n\n saw   \n\nI->saw\n\n  1   \n\nsaw->a\n\n  0.5   \n\nthe\n\n the   \n\nsaw->the\n\n  0.5   \n\nthe->dog\n\n  1   \n\nSTART\n\n START   \n\nSTART->I\n\n  0.5   \n\nWe\n\n We   \n\nSTART->We\n\n  0.5   \n\nWe->saw\n\n  1  \n\n\n\n\n\nI saw the dog\nWe saw a dog\nI read a book\n\n\n\n\n\n\n\n\n   \n\na\n\n a   \n\nbook\n\n book   \n\na->book\n\n  0.5   \n\ndog\n\n dog   \n\na->dog\n\n  0.5   \n\nEND\n\n END   \n\nbook->END\n\n  1   \n\ndog->END\n\n  1   \n\nI\n\n I   \n\nread\n\n read   \n\nI->read\n\n  0.5   \n\nsaw\n\n saw   \n\nI->saw\n\n  0.5   \n\nread->a\n\n  1   \n\nsaw->a\n\n  0.5   \n\nthe\n\n the   \n\nsaw->the\n\n  0.5   \n\nthe->dog\n\n  1   \n\nSTART\n\n START   \n\nSTART->I\n\n  0.67   \n\nWe\n\n We   \n\nSTART->We\n\n  0.33   \n\nWe->saw\n\n  1  \n\n\n\n\n\nI saw the dog\nWe saw a dog\nI read a book\nI saw a book\nI saw a dog\n\n\n\n\nFigure 1: Before and after update.\n\n\n\n\n\n\n\n   \n\na\n\n a   \n\nbook\n\n book   \n\na->book\n\n  0.5   \n\ndog\n\n dog   \n\na->dog\n\n  0.5   \n\nEND\n\n END   \n\nbook->END\n\n  1   \n\ndog->END\n\n  1   \n\nI\n\n I   \n\nread\n\n read   \n\nI->read\n\n  0.5   \n\nsaw\n\n saw   \n\nI->saw\n\n  0.5   \n\nread->a\n\n  1   \n\nsaw->a\n\n  0.5   \n\nthe\n\n the   \n\nsaw->the\n\n  0.5   \n\nthe->dog\n\n  1   \n\nSTART\n\n START   \n\nSTART->I\n\n  0.67   \n\nWe\n\n We   \n\nSTART->We\n\n  0.33   \n\nWe->saw\n\n  1  \n\n\n\n\n\n\n\n\n\n\n   \n\na\n\n a   \n\nbook\n\n book   \n\na->book\n\n  0.5   \n\ndog\n\n dog   \n\na->dog\n\n  0.5   \n\nEND\n\n END   \n\nbook->END\n\n  1   \n\ndog->END\n\n  1   \n\nI\n\n I   \n\nread\n\n read   \n\nI->read\n\n  0.25   \n\nsaw\n\n saw   \n\nI->saw\n\n  0.75   \n\nread->a\n\n  1   \n\nsaw->a\n\n  0.75   \n\nthe\n\n the   \n\nsaw->the\n\n  0.25   \n\nthe->dog\n\n  1   \n\nSTART\n\n START   \n\nSTART->I\n\n  0.8   \n\nWe\n\n We   \n\nSTART->We\n\n  0.2   \n\nWe->saw\n\n  1"
  },
  {
    "objectID": "lectures/ngram/index.html#the-probability-of-a-sentence",
    "href": "lectures/ngram/index.html#the-probability-of-a-sentence",
    "title": "ngram Language Models",
    "section": "The probability of a sentence",
    "text": "The probability of a sentence\nAnother way to visualize the final state diagram from above is with a matrix, with the “from” words along the rows and the “to” words along the columns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwords\na\nbook\ndog\nEND\nI\nread\nsaw\nthe\nWe\n\n\n\n\na\n0\n0.5\n0.5\n0\n0\n0\n0\n0\n0\n\n\nbook\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\ndog\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nI\n0\n0\n0\n0\n0\n0.25\n0.75\n0\n0\n\n\nread\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw\n0.75\n0\n0\n0\n0\n0\n0\n0.25\n0\n\n\nSTART\n0\n0\n0\n0\n0.8\n0\n0\n0\n0.2\n\n\nthe\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nWe\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\nThere is a non-zero number for every arrow in the state diagram. Every 0 value in the table represents a possible bigram that wasn’t observed (so, no arrow in the diagram).\nGiven these bigram probabilities we estimated from the corpus and our assumption that we can approximate the probability of whole sentences with smaller ngram probabilities, we can estimate the probability of a new sentence like so:\n\nWe saw the dog.\n\n\nP(We | <START>) \\(\\times\\) P(saw | We) \\(\\times\\) P(the | saw) \\(\\times\\) P(dog | saw) \\(\\times\\) P(<END> | dog)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwords\na\nbook\ndog\nEND\nI\nread\nsaw\nthe\nWe\n\n\n\n\na\n0\n0.5\n0.5\n0\n0\n0\n0\n0\n0\n\n\nbook\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\ndog\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nI\n0\n0\n0\n0\n0\n0.25\n0.75\n0\n0\n\n\nread\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw\n0.75\n0\n0\n0\n0\n0\n0\n0.25\n0\n\n\nSTART\n0\n0\n0\n0\n0.8\n0\n0\n0\n0.2\n\n\nthe\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nWe\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n\nWe can re-write the probability formula above like so:\n\\[\nP(s) = \\prod_{i=1}^n P(w_i|w_{i-1})\n\\]\nWe can also plug in the probabilities of these bigrams into the formula to get our estimated probility of the sentence.\n\\[\nP(s) = 0.2 \\times 1 \\times 1\\times 0.25 \\times 1 = 0.05\n\\]\n\n\n\n\n\n\nLog probabilities\n\n\n\nOnce you start multiplying probabilities, you’re going to get smaller and smaller numbers.\n\n\n\n\n\nthis\n\nequals\n\n\n\n\n0.5\n=\n0.500\n\n\n0.5 × 0.5\n=\n0.250\n\n\n0.5 × 0.5 × 0.5\n=\n0.125\n\n\n0.5 × 0.5 × 0.5 × 0.5\n=\n0.062\n\n\n0.5 × 0.5 × 0.5 × 0.5 × 0.5\n=\n0.031\n\n\n\n\n\nEven one very small probability (which you’ll get sometimes) can start sending the overall estimate into infintesimally small numbers close to 0, which computers may not be able to represent.\nSo, it’s also common to see the log-probability (a.k.a. the log-likelihood, in this case) being calculated instead. The way logarithms work, you add together values that you would have multiplied in the probability space.\n\\[\n\\log(P(\\text{We saw the dog}))=\\log(P(\\text{We | <START>})) +  \\log(P(\\text{saw | We}))+\\dots\n\\]\n\\[\n\\log(P(s)) = \\sum_{i=1}^n \\log(P(w_i|w_{i-1}))\n\\]\n\\[\n\\log(P(s)) = -1.609438 + 0 + 0 + -1.386294 + 0 = -2.995732\n\\]\n\n\n\nLarger ngrams\nLanguage models that take a larger window of adjacent words (3, or 4 grams) work in the same way, and are more “accurate” but are harder to visualize.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprev\na\nbook\ndog\nEND\nread\nsaw\nthe\n\n\n\n\na_a\n0\n0\n0\n0\n0\n0\n0\n\n\na_book\n0\n0\n0\n1\n0\n0\n0\n\n\na_dog\n0\n0\n0\n1\n0\n0\n0\n\n\na_I\n0\n0\n0\n0\n0\n0\n0\n\n\na_read\n0\n0\n0\n0\n0\n0\n0\n\n\na_saw\n0\n0\n0\n0\n0\n0\n0\n\n\na_the\n0\n0\n0\n0\n0\n0\n0\n\n\na_We\n0\n0\n0\n0\n0\n0\n0\n\n\nI_a\n0\n0\n0\n0\n0\n0\n0\n\n\nI_book\n0\n0\n0\n0\n0\n0\n0\n\n\nI_dog\n0\n0\n0\n0\n0\n0\n0\n\n\nI_I\n0\n0\n0\n0\n0\n0\n0\n\n\nI_read\n1\n0\n0\n0\n0\n0\n0\n\n\nI_saw\n0.666666666666667\n0\n0\n0\n0\n0\n0.333333333333333\n\n\nI_the\n0\n0\n0\n0\n0\n0\n0\n\n\nI_We\n0\n0\n0\n0\n0\n0\n0\n\n\nread_a\n0\n1\n0\n0\n0\n0\n0\n\n\nread_book\n0\n0\n0\n0\n0\n0\n0\n\n\nread_dog\n0\n0\n0\n0\n0\n0\n0\n\n\nread_I\n0\n0\n0\n0\n0\n0\n0\n\n\nread_read\n0\n0\n0\n0\n0\n0\n0\n\n\nread_saw\n0\n0\n0\n0\n0\n0\n0\n\n\nread_the\n0\n0\n0\n0\n0\n0\n0\n\n\nread_We\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw_a\n0\n0.333333333333333\n0.666666666666667\n0\n0\n0\n0\n\n\nsaw_book\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw_dog\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw_I\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw_read\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw_saw\n0\n0\n0\n0\n0\n0\n0\n\n\nsaw_the\n0\n0\n1\n0\n0\n0\n0\n\n\nsaw_We\n0\n0\n0\n0\n0\n0\n0\n\n\nSTART_a\n0\n0\n0\n0\n0\n0\n0\n\n\nSTART_book\n0\n0\n0\n0\n0\n0\n0\n\n\nSTART_dog\n0\n0\n0\n0\n0\n0\n0\n\n\nSTART_I\n0\n0\n0\n0\n0.25\n0.75\n0\n\n\nSTART_read\n0\n0\n0\n0\n0\n0\n0\n\n\nSTART_saw\n0\n0\n0\n0\n0\n0\n0\n\n\nSTART_the\n0\n0\n0\n0\n0\n0\n0\n\n\nSTART_We\n0\n0\n0\n0\n0\n1\n0\n\n\nthe_a\n0\n0\n0\n0\n0\n0\n0\n\n\nthe_book\n0\n0\n0\n0\n0\n0\n0\n\n\nthe_dog\n0\n0\n0\n1\n0\n0\n0\n\n\nthe_I\n0\n0\n0\n0\n0\n0\n0\n\n\nthe_read\n0\n0\n0\n0\n0\n0\n0\n\n\nthe_saw\n0\n0\n0\n0\n0\n0\n0\n\n\nthe_the\n0\n0\n0\n0\n0\n0\n0\n\n\nthe_We\n0\n0\n0\n0\n0\n0\n0\n\n\nWe_a\n0\n0\n0\n0\n0\n0\n0\n\n\nWe_book\n0\n0\n0\n0\n0\n0\n0\n\n\nWe_dog\n0\n0\n0\n0\n0\n0\n0\n\n\nWe_I\n0\n0\n0\n0\n0\n0\n0\n\n\nWe_read\n0\n0\n0\n0\n0\n0\n0\n\n\nWe_saw\n1\n0\n0\n0\n0\n0\n0\n\n\nWe_the\n0\n0\n0\n0\n0\n0\n0\n\n\nWe_We\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "lectures/ngram/index.html#generating-text",
    "href": "lectures/ngram/index.html#generating-text",
    "title": "ngram Language Models",
    "section": "Generating text",
    "text": "Generating text\nOnce we’ve estimated all of these transition probabilities, we can turn them around to generate text, if we want. Let’s take the final bigram “model” we had from before:\n\n\n\n\n\n   \n\na\n\n a   \n\nbook\n\n book   \n\na->book\n\n  0.5   \n\ndog\n\n dog   \n\na->dog\n\n  0.5   \n\nEND\n\n END   \n\nbook->END\n\n  1   \n\ndog->END\n\n  1   \n\nI\n\n I   \n\nread\n\n read   \n\nI->read\n\n  0.25   \n\nsaw\n\n saw   \n\nI->saw\n\n  0.75   \n\nread->a\n\n  1   \n\nsaw->a\n\n  0.75   \n\nthe\n\n the   \n\nsaw->the\n\n  0.25   \n\nthe->dog\n\n  1   \n\nSTART\n\n START   \n\nSTART->I\n\n  0.8   \n\nWe\n\n We   \n\nSTART->We\n\n  0.2   \n\nWe->saw\n\n  1  \n\n\n\n\n\nIf we start at <START> and roll some dice, there’s a 80% chance we’ll move to I and a 20% chance we’ll move to We.\n\n<START>\n\n = 26\nWe\n\n = 67\nsaw\n\n = 67\na\n\n = 67\ndog\n\n = 23\n<END>\n\n\n\n\n\n\nOur bigram model on our boring corpus generates boring results. But here’s the output of a tigram model estimated over Frankenstein.\n\n\n\n\n\nDetails\n\nThe version of the data I’m working with here hasn’t been sentence-ized (so no <START> or <BEGIN> tags), and has also had punctuation stripped out (see function here). So after estimating the trigram probabilities, seed the generator by sampling over all trigrams, then progress by sampling over the distribution of \\(P(w_i|w_{i-2}w_{i-1})\\).\n\n\n\nTrigram R Code\nfrank %>%\n  select(word) %>%\n  mutate(nextw1 = lead(word),\n         nextw2 = lead(word, n = 2)) %>%\n  group_by(word, nextw1) %>%\n  mutate(n_tot = n()) %>%\n  group_by(word, nextw1, nextw2) %>%\n  mutate(n = n(),\n         p = n/n_tot) %>%\n  summarise(p = mean(p),\n            n = n())%>%\n  ungroup()->trigram\n\n\n\ngenerator R Code\n#set.seed(517)\ngenerate_frankenstein <- function(trigram, n = 100){\n  trigram %>%\n    mutate(all_p = n/sum(n)) %>%\n    sample_n(size = 1, weight = all_p)->init_df\n  \n  init_vec <- c(init_df$word, init_df$nextw1, init_df$nextw2)\n  for(i in seq(n)){\n    hist = rev(rev(init_vec)[1:2])\n    trigram %>%\n      filter(word == hist[1],\n             nextw1 == hist[2]) %>%\n      sample_n(size = 1, weight = p) %>%\n      pull(nextw2) -> new_w\n      init_vec <- c(init_vec, new_w)\n  }\n  return(str_c(init_vec, collapse = \" \"))\n}\n\ncat(paste0(\"> \", generate_frankenstein(trigram, n = 100)))\n\n\nthis misery until he was but a high wind quickly dried the earth a dæmon whose delight was in a cloud of wonder and admiration sometimes i found it was a narrow minded trader and saw idleness and ruin in the neighbourhood of man a servant a condition which in his better days being even now you tremble you are too considerate to make so great that not only useless but draw down treble misery on his persecutor his tale i perhaps may be wafted to a want of sleep i often took his tools the use of which the servant presently brought"
  },
  {
    "objectID": "lectures/ngram/index.html#sparsity-again",
    "href": "lectures/ngram/index.html#sparsity-again",
    "title": "ngram Language Models",
    "section": "Sparsity, again",
    "text": "Sparsity, again\n\n\n\n\n\n\n\n\n\nThe rectangle represents a matrix, with the y-axis representing “from” words and the x-axis representing “to” words in Frankenstein. There could be a point in any location in the rectangle, representing a time that word \\(w_n\\) followed word \\(w_{n-1}\\). Each point represents a cell in that matrix where any data was observed."
  },
  {
    "objectID": "lectures/evaluation/index.html",
    "href": "lectures/evaluation/index.html",
    "title": "Evaluating models",
    "section": "",
    "text": "Language Models (including ngram models) are focused on “string prediction”, meaning we need to evaluate them like we would any predictive model. In both statistics and machine learning, there are some conventionalized approaches to this task that we can discuss in general."
  },
  {
    "objectID": "lectures/evaluation/index.html#training-vs-testing",
    "href": "lectures/evaluation/index.html#training-vs-testing",
    "title": "Evaluating models",
    "section": "Training vs testing",
    "text": "Training vs testing\nThe best way to evaluate a prediction model is to see how good its predictions are on some data that you already know what the predictions should be. The workflow, then, is\n\n“Train” the model on a training data set.\n“Test” the model on a test data set.\n\nNow, it’s rarely the case that people collect and curate a large training dataset to train a model, then go out and collect and curate a whole nother test dataset to test the model’s predictions. Instead, what they do is take their original dataset and split it into two pieces: “train” and “test”. Usually, “train” contains most of the data (80% to 90%), while “test” is a smaller held-out dataset (10% to 20% of the data).\n\nWhy not train and test on the whole dataset?\nWhen the model is “learning” how to make its predictions, the values it learns to make those predictions with will always be dependent on the training data. For example, compare the mini-bigram models below, one based on Frankenstein and the other based on Count of Monte Cristo.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitle\nword\nnext word\nn\n\n\n\n\nFrankenstein; Or, The Modern Prometheus\nthe\nsame\n62\n\n\nFrankenstein; Or, The Modern Prometheus\nthe\nmost\n56\n\n\nFrankenstein; Or, The Modern Prometheus\nthe\ncottage\n41\n\n\nFrankenstein; Or, The Modern Prometheus\nthe\nsun\n39\n\n\nFrankenstein; Or, The Modern Prometheus\nthe\nfirst\n35\n\n\n\n\n\n\n\n\n\n\ntitle\nword\nnext word\nn\n\n\n\n\nThe Count of Monte Cristo, Illustrated\nthe\ncount\n1107\n\n\nThe Count of Monte Cristo, Illustrated\nthe\nyoung\n530\n\n\nThe Count of Monte Cristo, Illustrated\nthe\nsame\n442\n\n\nThe Count of Monte Cristo, Illustrated\nthe\ndoor\n410\n\n\nThe Count of Monte Cristo, Illustrated\nthe\nfirst\n295\n\n\n\n\n\n\n\nIf we used the bigram model trained on Frankenstein to predict the most likely word to come after “the” on Frankenstein itself, we would do pretty well! But if we tried to use it to predict the most likely word to come after “the” in The Count of Monte Cristo, we’d do a lot worse! The highest frequency the w bigram in The Count of Monte Cristo (the count) doesn’t even appear in Frankenstein.\n\n\nBias vs Variance\nThis issue of the predictions of a model being too particularized to training data is related to the concept of the “Bias-Variance Tradeoff”.\n\nHigh Bias, Low Variance: One way to think of a high-Bias model is that it “over simplifies” the relationships in the data, but it also means that it will generalize to new data sets in a way that’s comparable to the training set.\n\nhigh-Bias model of morphology: All past tense is formed by adding -ed. This will produce some errors for verbs like run, and go, but will generalize well to new verbs, like yeet.\n\nLow Bias, High Variance: A high-Variance model captures much more detail to the relationships in the data, but that means it might be “overfit” on the training data.\n\nhigh-Variance model of morphology: You just have to memorize every present ➡️ past mapping verb by verb. This won’t any errors on verbs we already know like wait, run or go, but won’t generalize well to new verbs, like yeet.\n\n\n\n\nMeasuring error (or Loss)\nInevitably, the model we’re fitting will make predictions that are wrong, and then we need some way to quantify, or measure, how wrong the predictions were. The specific measure we use is going to depend a lot on the kind of prediction task we have, but some you might have already seen (or will see) are\n\nMean Squared Error, or MSE\nCross Entropy\nF(1) score\nAccuracy\nBLEU score\n\nOften the numbers these scores put out aren’t meaningful in and of themselves, but rather are used to compare models. But when you see a “score” or a “loss function” mentioned, understand it, generally, to be a measure of the difference between the values we expected in the test set, vs the values the model predicted."
  },
  {
    "objectID": "lectures/evaluation/index.html#a-linear-model-example",
    "href": "lectures/evaluation/index.html#a-linear-model-example",
    "title": "Evaluating models",
    "section": "A Linear Model example",
    "text": "A Linear Model example\nAs a brief example, let’s look at a data set of body measurements of penguins.\n\n\n\nArtwork by @allison_horst\n\n\n\nhead(palmerpenguins::penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\nnrow(palmerpenguins::penguins)\n\n[1] 344\n\n\nWe’re going to look at two models that try to predict the bill length of these penguins, one trying to predict it based on body mass, and the other based on bill depth.\n\nFigure 1: Body measurements of penguins\n\n\n\n\n\n\n(a) Bill length predicted by body mass\n\n\n\n\n\n\n\n(b) Bill length predicted by bill depth\n\n\n\n\n\n\nFirst, well split the data into train and test sets. I’m going to choose a random 80% of the data to be the training set, and use remaining 20% to be the test set.\n\nset.seed(517)\n\npenguins_id <- penguins %>% \n                  drop_na() %>%\n                  mutate(row_id = 1:n())\ntrain <- penguins_id %>%\n          slice_sample(prop = 0.8)\ntest <- penguins_id %>%\n          filter(!row_id %in% train$row_id)\nnrow(train)\n\n[1] 266\n\nnrow(test)\n\n[1] 67\n\n\nNow, I’ll fit two linear models with the training set. The linear model is saying “the predicted bill length for a given predictor (body mass or bill depth) is whatever value is on the line.”\n\nFigure 2: Models trained on train.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese are both “High Bias” models. Hardly any of the points are exactly on the lines.\nNow, let’s see how well these models perform on the held out test data.\\\n\n\n\n\nFigure 3: Model performance on the test set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s kind of hard to see from just the figures which model performed better on the test set. To evaluate the error, or loss, of these models on the test set, I’ll use the Mean Squared error, which is calculated like so:\n\nFor every data point, subtract the predicted value from the actual value, to get the difference between the prediction and reality.\nMultiply each difference by itself, a.k.a. square it.1\nGet the mean of these squared errors.\n\nIn math terms, let’s say\n\n\\(y_i\\) is the actual value of some data point.\n\\(\\hat{y}_i\\) is the predicted value\n\\(N\\) is the total number of data points.\n\nThen the mean squared error is\n\\[\n\\text{MSE} = \\frac{\\sum_{i=1}^N(y_i-\\hat{y}_i)^2}{N}\n\\]\nI’ll show you the actual code I’m using to get the MSE, in case it demystifies the mathematical formula.\n\ndiff_body        = test$bill_length_mm - test$body_pred\nsquare_diff_body = diff_body^2\nmse_body         = mean(square_diff_body)\nprint(mse_body)\n\n[1] 16.72451\n\ndiff_bill        = test$bill_length_mm - test$bill_pred\nsquare_diff_bill = diff_bill^2\nmse_bill         = mean(square_diff_bill)\nprint(mse_bill)\n\n[1] 26.83769\n\n\nIt looks like the model predicting bill length using body mass has a smaller MSE than the model predicting bill length using bill depth. So if we had to choose between these two models, we’d go with the body mass model."
  },
  {
    "objectID": "lectures/evaluation/index.html#precision-recall-and-f-measure",
    "href": "lectures/evaluation/index.html#precision-recall-and-f-measure",
    "title": "Evaluating models",
    "section": "Precision, Recall, and F-Measure",
    "text": "Precision, Recall, and F-Measure\nFor NLP tasks, we’re not usually trying to predict a continuous measure, but rather trying to categorize words, sentences, or documents. In these kinds of tasks, when we evalulate a model, we’ll often want to use measures known as “Precision”, “Recall” and “F-measure”.\nTo illustrate these measures, let’s say we’ve built a robot to pick raspberries, and we want it to pick all of the ripe berries (if we leave any behind, they’ll rot!) and none of the unripe berries (if we pick them before they’re ripe, we’ve missed out on future berries!). Let’s present the robot with this following scene that has 9 berries in the foreground. 5 of them are ripe and 4 of them are unripe.\n\n\n\n\n\n\nScenario 1: It picks all of the berries\nLet’s say the robot picked all of the berries.\n\n\n\nPicked\nUnpicked\n\n\n\n\nripe berry\n\n\n\nripe berry\n\n\n\nripe berry\n\n\n\nripe berry\n\n\n\nripe berry\n\n\n\nunripe berry\n\n\n\nunripe berry\n\n\n\nunripe berry\n\n\n\nunripe berry\n\n\n\n\nThis strategy has high “Recall”. There were a total of 5 ripe berries, and the robot picked all 5 of them.\n\\[\n\\text{recall} = \\frac{\\text{ripe picked berries}}{\\text{ripe berries}} = \\frac{5}{5} = 1\n\\] But, it has low “precision”. Of all the berries it picked, a lot of them were unripe.\n\\[\n\\text{precision} = \\frac{\\text{ripe picked berries}}{\\text{picked berries}} = \\frac{5}{9} = 0.55\n\\]\nThe “F-measure” or “F1 score” is a way to combine the precision and recall score into one overall performance score.\n\\[\nF_1 = 2\\frac{\\text{precision}\\times \\text{recall}}{\\text{precision} + \\text{recall}}=2\\frac{0.55\\times 1}{0.55 + 1}=2\\frac{0.55}{1.55} = 0.71\n\\]\n\n\nScenario 2: The robot picks only 1 ripe raspberry\n\n\n\nPicked\nUnpicked\n\n\n\n\nripe berry\n\n\n\n\nripe berry\n\n\n\nripe berry\n\n\n\nripe berry\n\n\n\nripe berry\n\n\n\nunripe berry\n\n\n\nunripe berry\n\n\n\nunripe berry\n\n\n\nunripe berry\n\n\n\nThis strategy has a very low recall. There are 5 ripe berries, and it has only picked 1 of them\n\\[\n\\text{recall} = \\frac{\\text{ripe picked berries}}{\\text{ripe berries}} = \\frac{1}{5} = 0.2\n\\] But, it has an extremely high precision. Of all of the berries it picked, it only picked ripe ones!\n\\[\n\\text{precision} = \\frac{\\text{ripe picked berries}}{\\text{picked berries}} = \\frac{1}{1} = 1\n\\] The very low precision winds up dragging down the overall F-measure. \\[\nF_1 = 2\\frac{\\text{precision}\\times \\text{recall}}{\\text{precision} + \\text{recall}}=2\\frac{1 \\times 0.2}{1 + 0.2}=2\\frac{0.2}{1.2} = 0.33\n\\]\n\n\nScenario 3: The robot alternates between picking and not picking\n\n\n\nPicked\nUnpicked\n\n\n\n\nripe berry\n\n\n\n\nripe berry\n\n\nripe berry\n\n\n\n\nripe berry\n\n\nripe berry\n\n\n\n\nunripe berry\n\n\nunripe berry\n\n\n\n\nunripe berry\n\n\nunripe berry\n\n\n\n\n\\[\n\\text{recall} = \\frac{\\text{ripe picked berries}}{\\text{ripe berries}} = \\frac{3}{5} = 0.6\n\\]\n\\[\n\\text{precision} = \\frac{\\text{ripe picked berries}}{\\text{picked berries}} = \\frac{3}{5} = 0.6\n\\]\n\\[\nF_1 = 2\\frac{\\text{precision}\\times \\text{recall}}{\\text{precision} + \\text{recall}}=2\\frac{0.6 \\times 0.6}{0.6 + 0.6}=2\\frac{0.36}{1.2} = 0.6\n\\]\n\n\nA non-berry example\nOne kind of NLP task is “Named Entity Recognition” (NER), or detecting and identifying the kind of named entities in a text. Here’s an example of spaCy doing that with a the sentence\n\nDr. Josef Fruehwald is teaching Lin517 at the University of Kentucky in the Fall 2022 semester.\n\n# python\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n\ntext = \"Dr. Josef Fruehwald is teaching Lin517 at the University of Kentucky in the Fall 2022 semester.\"\n\ndoc = nlp(text)\nprint(displacy.render(doc, style = 'ent'))\n\nDr.   Josef Fruehwald PERSON  is teaching  Lin517 GPE  at  the University of Kentucky ORG  in the Fall  2022 DATE  semester.\n\nWhen looking at the original sentence, we could think of each word, or span of words, as a berry, and the named entities as the ripe berries. The precision and recall here would be\n\\[\n\\text{precision}=\\frac{\\text{named entitites identified}}{\\text{identified words}}\n\\]\n\\[\n\\text{recall}=\\frac{\\text{named entities identified}}{\\text{named entities}}\n\\]\nIn this example, additional adjustments might need to be made for whether the span of the named entities is correct. 2022 is correctly a date, but maybe Fall should be included in its span. Also, it has identified Lin517 as a GeoPolitical Entity (GPE)."
  },
  {
    "objectID": "lectures/evaluation/index.html#edit-distance",
    "href": "lectures/evaluation/index.html#edit-distance",
    "title": "Evaluating models",
    "section": "Edit Distance",
    "text": "Edit Distance"
  },
  {
    "objectID": "lectures/what_is_nlp/index.html#nlp-in-computational-linguistics",
    "href": "lectures/what_is_nlp/index.html#nlp-in-computational-linguistics",
    "title": "What is NLP?(for this course)",
    "section": "NLP \\(\\in\\) Computational Linguistics",
    "text": "NLP \\(\\in\\) Computational Linguistics\nIn set notation, \\(\\in\\) means “is an element of”. That is, there’s a large set of things called “Computational Linguistics”, and NLP is a part of that larger set.\n“Computational Linguistics” covers a very broad range of topics. Natural Language Processing is currently an area of research and application that receives a lot of attention & money, but Computational Linguistics is a much broader umbrella term. The Association for Computational Linguistics defines it as\n\nComputational linguistics is the scientific study of language from a computational perspective. Computational linguists are interested in providing computational models of various kinds of linguistic phenomena. These models may be “knowledge-based” (“hand-crafted”) or “data-driven” (“statistical” or “empirical”). Work in computational linguistics is in some cases motivated from a scientific perspective in that one is trying to provide a computational explanation for a particular linguistic or psycholinguistic phenomenon; and in other cases the motivation may be more purely technological in that one wants to provide a working component of a speech or natural language system. Indeed, the work of computational linguists is incorporated into many working systems today, including speech recognition systems, text-to-speech synthesizers, automated voice response systems, web search engines, text editors, language instruction materials, to name just a few."
  },
  {
    "objectID": "lectures/what_is_nlp/index.html#some-examples-of-x-x-in-textcltextandx-ne-textnlp",
    "href": "lectures/what_is_nlp/index.html#some-examples-of-x-x-in-textcltextandx-ne-textnlp",
    "title": "What is NLP?(for this course)",
    "section": "Some examples of \\(\\{x | x \\in \\text{CL}~\\text{and}~x \\ne \\text{NLP}\\}\\)",
    "text": "Some examples of \\(\\{x | x \\in \\text{CL}~\\text{and}~x \\ne \\text{NLP}\\}\\)\nThis is set notation that means “the set of things, such that each thing is in Computational Linguistics, and the thing is not Natural Language Processing\n\nFormalizing Theory\nOne use of computational linguistics is to formalize linguistic theories into a computational framework. This might seem weird, since a lot of linguistic theory already looks very formal. But giving mathy looking definitions in a verbal description of a theory is a very different thing from implementing that theory in code that will run.\nSome examples are\n\nMinimalist Parsers (Berwick and Stabler 2019) implementing parsers for Minimalist Syntax\nThe Gradual Learning Algorithm (Boersma and Hayes 2001) implementing constraint re-ranking in Optimality Theory\nThe Tolerance Principle (Charles. Yang 2016), formalizing how learners might acquire rules that have exceptions.\n\nThe interesting thing with formalizing verbal theories, computationally, is that things that might seem like big differences in the verbal theories could turn out to be computationally identical, and some things that might not seem like a big difference can turn out to be massively different computationally.\n\n\nConceptual Experiments\nYou can use general computational principles to flesh out what you would expect to happen given under specific theories, or to use specific computational implementations of linguistic theory to explore their consequences.\nHere’s a little example from dialectology. We have two proposed principles:\n\nGarde’s Principle: Mergers are irreversible by linguistic means\n\nonce a community gets merger, like the cot/caught merger, it cannot get back the distinction\n\nHerzog’s Corollary: Mergers expand at the expense of distinctions.\n\nonce a community develops a merger, like the cot/caught merger, it will inevitably spread geographically to other communities\n\n\nWe can translate these two principles into a “finite state automaton” below.\n\n\n\n\n\n\n\nfinite_state_machine\n\n  \n\nd\n\n ɔ/ɑ distinction   \n\nd->d\n\n  0.90   \n\nm\n\n ɔ/ɑ merger   \n\nd->m\n\n  0.10   \n\nm->d\n\n  0.01   \n\nm->m\n\n  0.99   \n\ninit\n\n   \n\ninit->d\n\n   \n\n\n\n\n\nA verbal translation of this diagram would be\n\nWe start out in a state of distinguishing between /ɔ/ and /ɑ/. With each step in time (“generation”), we probably keep distinguishing between /ɔ/ and /ɑ/ with a 0.9 probability, but there’s some chance we become a merged community. Once we become a merged community, we are overwhelmingly likely to remain a merged community with a 0.99 probability. But there is a very little probability that we might go back to being merged at 0.01 probability.\n\nUnder these circumstances, are we inevitably going to become a merged community? How long until we reach the maximum probability of becoming a merged community? We can answer these questions with a conceptual experiment, converting the description and diagram above into a transition probability matrix, and then just doing a bunch of matrix multiplications.\n\n# python\nimport numpy as np\n\nd_change = np.array([0.90, 0.10])\nm_change = np.array([0.01, 0.99])\n\nchange_mat = np.row_stack((d_change, m_change))\nprint(change_mat)\n\n[[0.9  0.1 ]\n [0.01 0.99]]\n\n\n\n# python\ninitial_state = np.array((1,0))\nn_generations = 100\ncollector = [initial_state]\n\ncurrent_state = initial_state\nfor i in range(n_generations):\n  new_state = current_state @ change_mat\n  collector.append(new_state)\n  current_state = new_state\n  \nresults_mat = np.row_stack(collector)\n\n\n\n\n\n\nLooks like with the probabilities set up this way, we’re not guaranteed to become a merged community. The probability is very high (about 0.91), but not for certain. We might say, seeing this, that unless the Garde’s Principle is absolute (it’s impossible to undo a merger by any means) then Herzog’s Corollary won’t necessarily hold.\nOther examples of conceptual experiments are\n\nC. D. Yang (2000) used a model of variable grammar learning to see if he could predict which grammars (e.g. V2 vs no-V2) would win over time.\nSneller, Fruehwald, and Yang (2019) used the tolerance principle to see if a specific phonological change in Philadelphia could plausibly develop on its own, or if it had to be due to dialect contact.\nLinzen and Baroni (2021) used RNNs (a kind of neural network) to see if “garden path” sentences (e.g. “The horse raced past the barn fell.”1) were difficult just because the word at the pivot point was especially unlikely.\n\n\n\nAgent Based Modelling\nThis doesn’t always fall under the rubric of “computational linguistics,” but agent-based modelling involves programming virtual “agents” that then “interact” with each other. Part of what you program into the simulation is rules for how agents interact with each other, and what information they exchange or adopt when they do. It’s often used to model the effect of social network structure.\n\nDe Boer (2001) models vowel system acquisition and development over time.\nStanford and Kenny (2013) explore models of geographic spread of variation.\nKauhanen (2017) explores whether any linguistic variant needs to have an advantage over another in order to become the dominant form.\n\n\n\nBuilding and using computational tools and data\nOf course, there is a massive amount of effort that goes into constructing linguistic corpora, and developing computational tools to analyze those corpora."
  },
  {
    "objectID": "lectures/what_is_nlp/index.html#nlp",
    "href": "lectures/what_is_nlp/index.html#nlp",
    "title": "What is NLP?(for this course)",
    "section": "NLP",
    "text": "NLP\nFor this class, we’ll be mostly focusing on the “Language Modeling” component of NLP, and we’ll be following the definition of “Language Model” from Bender and Koller (2020) as a model trained to predict what string or word is most likely in the context of other words. For example, from the following sentence, can you guess the missing word?\n\nI could tell he was mad from the tone of his [____]\n\n\nUsing the predictions\nLanguage model predictions are really useful for many applications. For example, let’s say you built an autocaptioning system that took audio and processed it into a transcription. You might have a situation where the following sentence gets transcribed.\n\nPoker and blackjack are both  games people play at casinos.\n\nThe digital signal, , in this sentence is consistent with two possible words here\n\ncar\ncard\n\nUs humans here know that in the context of “poker”, “blackjack”, “games” and “casinos”, the more likely word is “card”, not “car.” But a simple model that’s just processing acoustics doesn’t know that. So to improve your captioning, you’d probably want to incorporate a language model that takes the context into account and boosts the probability of “card”.\nThis is just one example, but there are many other kinds of string prediction tasks, such as:\n\nGiven a string in language A, predict the string in language B (a.k.a. machine translation).\nGiven a whole paragraph, predict a summary of the paragraph (summarization).\nGiven a question, predict an answer (question answering).\nGiven a prompt, continue the text in the same style (text generation).\n\n\n\nUsing the representations\nIn the process of training models to do text generation, they develop internal representations of strings of text that can be useful for other purposes. For example, a common NLP task is “sentiment analysis,” that could be used to analyze, say, reviews of products online.2\nOne really very simplistic approach would be to get a dictionary of words that have been scored for their “positivity” and “negativity.” Then, every one of those words or a tweet or what ever has one of those words in it, you add its score as a total sentiment score.\n\n# R\nlibrary(tidytext)\nset.seed(101)\nget_sentiments(\"afinn\") %>%\n  sample_n(10) %>%\n  kable()\n\n\n\n\nword\nvalue\n\n\n\n\nlobby\n-2\n\n\nstricken\n-2\n\n\nloser\n-3\n\n\njealous\n-2\n\n\nbreakthrough\n3\n\n\ninability\n-2\n\n\nharshest\n-2\n\n\nranter\n-3\n\n\ncried\n-2\n\n\nwarfare\n-2\n\n\n\n\n\nHere’s an example with a notable tweet.\n\n# R\ntweet <- \"IF THE ZOO BANS ME FOR HOLLERING AT THE ANIMALS I WILL FACE GOD AND WALK BACKWARDS INTO HELL\"\ntweet_df <- tibble(word = tweet %>%\n                     tolower() %>%\n                     str_split(\" \") %>%\n                     simplify()) %>%\n  left_join(get_sentiments(\"afinn\")) %>%\n  replace_na(replace = list(value = 0))\n\n\nfull tweetsum\n\n\n\n# R\ntweet_df\n\n# A tibble: 19 × 2\n   word      value\n   <chr>     <dbl>\n 1 if            0\n 2 the           0\n 3 zoo           0\n 4 bans          0\n 5 me            0\n 6 for           0\n 7 hollering     0\n 8 at            0\n 9 the           0\n10 animals       0\n11 i             0\n12 will          0\n13 face          0\n14 god           1\n15 and           0\n16 walk          0\n17 backwards     0\n18 into          0\n19 hell         -4\n\n\n\n\n\n# R\ntweet_df %>%\n  summarise(sentiment = sum(value))\n\n# A tibble: 1 × 1\n  sentiment\n      <dbl>\n1        -3\n\n\n\n\n\nHowever, this is a kind of lackluster approach to sentiment analysis nowadays. Many language models now now, as a by product of their string prediction training, have more complex representations of words than just a score between -5 and 5, and have representations of whole strings that can be used (so it won’t give the same score to “good” and “not good”).\n\n# python\nfrom transformers import pipeline\n\n# warning, this will download approx\n# 1.3G of data.\nsentiment_analysis = pipeline(\"sentiment-analysis\",\n                              model=\"siebert/sentiment-roberta-large-english\")\n\n\n# python\nprint(sentiment_analysis(\"IF THE ZOO BANS ME FOR HOLLERING AT THE ANIMALS I WILL FACE GOD AND WALK BACKWARDS INTO HELL\"))\nprint(sentiment_analysis(\"This ain't bad!\"))\n\n[{'label': 'NEGATIVE', 'score': 0.9990140199661255}]\n[{'label': 'POSITIVE', 'score': 0.9944340586662292}]"
  }
]